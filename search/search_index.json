{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CAGPJax","text":"<p>Computation-Aware Gaussian Processes (CaGPs) for JAX</p> <p>CAGPJax provides efficient Gaussian processes by leveraging structured kernel approximations and sparse matrix operations, built on JAX and GPJax.</p> <p>For \\(n\\) data-points, the computational cost of exact Gaussian processes scales as \\(\\mathcal{O}(n^3)\\) due to matrix inversions, while the memory requirements scale as \\(\\mathcal{O}(n^2)\\). CaGPs project the data to a \\(k(\\ll n)\\)-dimensional subspace to perform inference, reducing the computational cost to \\(\\mathcal{O}(n^2k)\\) and the memory requirements to \\(\\mathcal{O}(nk)\\). Using sparse projections further reduces the computational cost to \\(\\mathcal{O}(n^2)\\).</p> <p>Compared to other apprximate GP inference approaches such as inducing point methods, the prediction uncertainty of CaGPs accounts for the additional uncertainty due to only observing a subspace of the data.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install git+https://github.com/sethaxen/CAGPJax.git\n</code></pre>"},{"location":"#citation","title":"Citation","text":"<p>There's not yet a citation for this package. If using the code, please cite</p> <pre><code>@inproceedings{wenger2022itergp,\n  title         = {Posterior and Computational Uncertainty in {G}aussian Processes},\n  author        = {Wenger, Jonathan and Pleiss, Geoff and Pf\\\"{o}rtner, Marvin and Hennig, Philipp and Cunningham, John P},\n  year          = 2022,\n  booktitle     = {Advances in Neural Information Processing Systems},\n  publisher     = {Curran Associates, Inc.},\n  volume        = 35,\n  pages         = {10876--10890},\n  url           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/4683beb6bab325650db13afd05d1a14a-Paper-Conference.pdf},\n  editor        = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},\n  eprint        = {2205.15449},\n  archiveprefix = {arXiv},\n  primaryclass  = {cs.LG}\n}\n@inproceedings{wenger2024cagp,\n  title         = {Computation-Aware {G}aussian Processes: Model Selection And Linear-Time Inference},\n  author        = {Wenger, Jonathan and Wu, Kaiwen and Hennig, Philipp and Gardner, Jacob R. and Pleiss, Geoff and Cunningham, John P.},\n  year          = 2024,\n  booktitle     = {Advances in Neural Information Processing Systems},\n  publisher     = {Curran Associates, Inc.},\n  volume        = 37,\n  pages         = {31316--31349},\n  url           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/379ea6eb0faad176b570c2e26d58ff2b-Paper-Conference.pdf},\n  editor        = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},\n  eprint        = {2411.01036},\n  archiveprefix = {arXiv},\n  primaryclass  = {cs.LG}\n}\n</code></pre>"},{"location":"#api-reference","title":"API Reference","text":"<p>See the Reference section for detailed API documentation.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>cagpjax<ul> <li>linalg<ul> <li>congruence</li> <li>eigh</li> <li>lower_cholesky</li> <li>orthogonalize</li> <li>utils</li> </ul> </li> <li>models<ul> <li>base</li> <li>cagp</li> </ul> </li> <li>operators<ul> <li>annotations</li> <li>block_diagonal_sparse</li> <li>diag_like</li> </ul> </li> <li>policies<ul> <li>base</li> <li>block_sparse</li> <li>lanczos</li> <li>orthogonalization</li> <li>pseudoinput</li> </ul> </li> <li>solvers<ul> <li>base</li> <li>cholesky</li> <li>pseudoinverse</li> </ul> </li> <li>typing</li> </ul> </li> </ul>"},{"location":"reference/cagpjax/","title":"cagpjax","text":""},{"location":"reference/cagpjax/#cagpjax","title":"<code>cagpjax</code>","text":"<p>Computation-Aware Gaussian Processes for GPJax.</p>"},{"location":"reference/cagpjax/#cagpjax.BlockDiagonalSparse","title":"<code>BlockDiagonalSparse</code>","text":"<p>               Bases: <code>LinearOperator</code></p> <p>Block-diagonal sparse linear operator.</p> <p>This operator represents a block-diagonal matrix structure where the blocks are contiguous, and each contains a column vector, so that exactly one value is non-zero in each row.</p> <p>Parameters:</p> Name Type Description Default <code>nz_values</code> <code>Float[Array, N]</code> <p>Non-zero values to be distributed across diagonal blocks.</p> required <code>n_blocks</code> <code>int</code> <p>Number of diagonal blocks in the matrix.</p> required"},{"location":"reference/cagpjax/#cagpjax.BlockDiagonalSparse--examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; from cagpjax.operators import BlockDiagonalSparse\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create a 3x6 block-diagonal matrix with 3 blocks\n&gt;&gt;&gt; nz_values = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n&gt;&gt;&gt; op = BlockDiagonalSparse(nz_values, n_blocks=3)\n&gt;&gt;&gt; print(op.shape)\n(6, 3)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Apply to identity matrices\n&gt;&gt;&gt; op @ jnp.eye(3)\nArray([[1., 0., 0.],\n       [2., 0., 0.],\n       [0., 3., 0.],\n       [0., 4., 0.],\n       [0., 0., 5.],\n       [0., 0., 6.]], dtype=float32)\n</code></pre> Source code in <code>src/cagpjax/operators/block_diagonal_sparse.py</code> <pre><code>class BlockDiagonalSparse(LinearOperator):\n    \"\"\"Block-diagonal sparse linear operator.\n\n    This operator represents a block-diagonal matrix structure where the blocks are contiguous, and\n    each contains a column vector, so that exactly one value is non-zero in each row.\n\n    Args:\n        nz_values: Non-zero values to be distributed across diagonal blocks.\n        n_blocks: Number of diagonal blocks in the matrix.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; import jax.numpy as jnp\n    &gt;&gt;&gt; from cagpjax.operators import BlockDiagonalSparse\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Create a 3x6 block-diagonal matrix with 3 blocks\n    &gt;&gt;&gt; nz_values = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n    &gt;&gt;&gt; op = BlockDiagonalSparse(nz_values, n_blocks=3)\n    &gt;&gt;&gt; print(op.shape)\n    (6, 3)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Apply to identity matrices\n    &gt;&gt;&gt; op @ jnp.eye(3)\n    Array([[1., 0., 0.],\n           [2., 0., 0.],\n           [0., 3., 0.],\n           [0., 4., 0.],\n           [0., 0., 5.],\n           [0., 0., 6.]], dtype=float32)\n    ```\n    \"\"\"\n\n    def __init__(self, nz_values: Float[Array, \"N\"], n_blocks: int):\n        n = nz_values.shape[0]\n        super().__init__(nz_values.dtype, (n, n_blocks), annotations={ScaledOrthogonal})\n        self.nz_values = nz_values\n\n    def _matmat(self, X: Float[Array, \"K M\"]) -&gt; Float[Array, \"N M\"]:\n        n, n_blocks = self.shape\n        block_size = n // n_blocks\n        n_blocks_main = n_blocks if n % n_blocks == 0 else n_blocks - 1\n        n_main = n_blocks_main * block_size\n        m = X.shape[1]\n\n        # block-wise multiplication for main blocks\n        blocks_main = self.nz_values[:n_main].reshape(n_blocks_main, block_size)\n        X_main = X[:n_blocks_main, :]\n        res_main = (blocks_main[..., None] * X_main[:, None, :]).reshape(n_main, m)\n\n        # handle overhang if any\n        if n &gt; n_main:\n            n_overhang = n - n_main\n            X_overhang = X[n_blocks_main, :]\n            block_overhang = self.nz_values[n_main:]\n            res_overhang = jnp.outer(block_overhang, X_overhang).reshape(n_overhang, m)\n            res = jnp.concatenate([res_main, res_overhang], axis=0)\n        else:\n            res = res_main\n\n        return res\n\n    def _rmatmat(self, X: Float[Array, \"M N\"]) -&gt; Float[Array, \"M K\"]:\n        # figure out size of main blocks\n        n, n_blocks = self.shape\n        block_size = n // n_blocks\n        n_blocks_main = n_blocks if n % n_blocks == 0 else n_blocks - 1\n        n_main = n_blocks_main * block_size\n        m = X.shape[0]\n\n        # block-wise multiplication for main blocks\n        blocks_main = self.nz_values[:n_main].reshape(n_blocks_main, block_size)\n        X_main = X[:, :n_main].reshape(m, n_blocks_main, block_size)\n        res_main = jnp.einsum(\"ik,jik-&gt;ji\", blocks_main, X_main)\n\n        # handle overhang if any\n        if n &gt; n_main:\n            n_overhang = n - n_main\n            X_overhang = X[:, n_main:].reshape(m, n_overhang)\n            block_overhang = self.nz_values[n_main:]\n            res_overhang = (X_overhang @ block_overhang)[:, None]\n            res = jnp.concatenate([res_main, res_overhang], axis=1)\n        else:\n            res = res_main\n\n        return res\n</code></pre>"},{"location":"reference/cagpjax/#cagpjax.BlockSparsePolicy","title":"<code>BlockSparsePolicy</code>","text":"<p>               Bases: <code>AbstractBatchLinearSolverPolicy</code></p> <p>Block-sparse linear solver policy.</p> <p>This policy uses a fixed block-diagonal sparse structure to define independent learnable actions. The matrix has the following structure:</p> \\[ S = \\begin{bmatrix}     s_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\     0 &amp; s_2 &amp; \\cdots &amp; 0 \\\\     \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\     0 &amp; 0 &amp; \\cdots &amp; s_{\\text{n_actions}} \\end{bmatrix} \\] <p>These are stacked and stored as a single trainable parameter <code>nz_values</code>.</p> Source code in <code>src/cagpjax/policies/block_sparse.py</code> <pre><code>class BlockSparsePolicy(AbstractBatchLinearSolverPolicy):\n    r\"\"\"Block-sparse linear solver policy.\n\n    This policy uses a fixed block-diagonal sparse structure to define\n    independent learnable actions. The matrix has the following structure:\n\n    $$\n    S = \\begin{bmatrix}\n        s_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\\n        0 &amp; s_2 &amp; \\cdots &amp; 0 \\\\\n        \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\\n        0 &amp; 0 &amp; \\cdots &amp; s_{\\text{n_actions}}\n    \\end{bmatrix}\n    $$\n\n    These are stacked and stored as a single trainable parameter ``nz_values``.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_actions: int,\n        n: int | None = None,\n        nz_values: Float[Array, \"N\"] | nnx.Variable[Float[Array, \"N\"]] | None = None,\n        key: PRNGKeyArray | None = None,\n        **kwargs,\n    ):\n        \"\"\"Initialize the block sparse policy.\n\n        Args:\n            n_actions: Number of actions to use.\n            n: Number of rows and columns of the full operator. Must be provided if ``nz_values`` is\n                not provided.\n            nz_values: Non-zero values of the block-diagonal sparse matrix (shape ``(n,)``). If not\n                provided, random actions are sampled using the key if provided.\n            key: Random key for sampling actions if ``nz_values`` is not provided.\n            **kwargs: Additional keyword arguments for ``jax.random.normal`` (e.g. ``dtype``)\n        \"\"\"\n        if nz_values is None:\n            if n is None:\n                raise ValueError(\"n must be provided if nz_values is not provided\")\n            if key is None:\n                key = jax.random.PRNGKey(0)\n            block_size = n // n_actions\n            nz_values = jax.random.normal(key, (n,), **kwargs)\n            nz_values /= jnp.sqrt(block_size)\n        elif n is not None:\n            warnings.warn(\"n is ignored because nz_values is provided\")\n\n        if not isinstance(nz_values, nnx.Variable):\n            nz_values = Real(nz_values)\n\n        self.nz_values: nnx.Variable[Float[Array, \"N\"]] = nz_values\n        self._n_actions: int = n_actions\n\n    @property\n    @override\n    def n_actions(self) -&gt; int:\n        \"\"\"Number of actions to be used.\"\"\"\n        return self._n_actions\n\n    @override\n    def to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n        \"\"\"Convert to block diagonal sparse action operators.\n\n        Args:\n            A: Linear operator (unused).\n\n        Returns:\n            BlockDiagonalSparse: Sparse action structure representing the blocks.\n        \"\"\"\n        return BlockDiagonalSparse(self.nz_values.value, self.n_actions)\n</code></pre>"},{"location":"reference/cagpjax/#cagpjax.BlockSparsePolicy.n_actions","title":"<code>n_actions</code>  <code>property</code>","text":"<p>Number of actions to be used.</p>"},{"location":"reference/cagpjax/#cagpjax.BlockSparsePolicy.__init__","title":"<code>__init__(n_actions, n=None, nz_values=None, key=None, **kwargs)</code>","text":"<p>Initialize the block sparse policy.</p> <p>Parameters:</p> Name Type Description Default <code>n_actions</code> <code>int</code> <p>Number of actions to use.</p> required <code>n</code> <code>int | None</code> <p>Number of rows and columns of the full operator. Must be provided if <code>nz_values</code> is not provided.</p> <code>None</code> <code>nz_values</code> <code>Float[Array, N] | Variable[Float[Array, N]] | None</code> <p>Non-zero values of the block-diagonal sparse matrix (shape <code>(n,)</code>). If not provided, random actions are sampled using the key if provided.</p> <code>None</code> <code>key</code> <code>PRNGKeyArray | None</code> <p>Random key for sampling actions if <code>nz_values</code> is not provided.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for <code>jax.random.normal</code> (e.g. <code>dtype</code>)</p> <code>{}</code> Source code in <code>src/cagpjax/policies/block_sparse.py</code> <pre><code>def __init__(\n    self,\n    n_actions: int,\n    n: int | None = None,\n    nz_values: Float[Array, \"N\"] | nnx.Variable[Float[Array, \"N\"]] | None = None,\n    key: PRNGKeyArray | None = None,\n    **kwargs,\n):\n    \"\"\"Initialize the block sparse policy.\n\n    Args:\n        n_actions: Number of actions to use.\n        n: Number of rows and columns of the full operator. Must be provided if ``nz_values`` is\n            not provided.\n        nz_values: Non-zero values of the block-diagonal sparse matrix (shape ``(n,)``). If not\n            provided, random actions are sampled using the key if provided.\n        key: Random key for sampling actions if ``nz_values`` is not provided.\n        **kwargs: Additional keyword arguments for ``jax.random.normal`` (e.g. ``dtype``)\n    \"\"\"\n    if nz_values is None:\n        if n is None:\n            raise ValueError(\"n must be provided if nz_values is not provided\")\n        if key is None:\n            key = jax.random.PRNGKey(0)\n        block_size = n // n_actions\n        nz_values = jax.random.normal(key, (n,), **kwargs)\n        nz_values /= jnp.sqrt(block_size)\n    elif n is not None:\n        warnings.warn(\"n is ignored because nz_values is provided\")\n\n    if not isinstance(nz_values, nnx.Variable):\n        nz_values = Real(nz_values)\n\n    self.nz_values: nnx.Variable[Float[Array, \"N\"]] = nz_values\n    self._n_actions: int = n_actions\n</code></pre>"},{"location":"reference/cagpjax/#cagpjax.BlockSparsePolicy.to_actions","title":"<code>to_actions(A)</code>","text":"<p>Convert to block diagonal sparse action operators.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>LinearOperator</code> <p>Linear operator (unused).</p> required <p>Returns:</p> Name Type Description <code>BlockDiagonalSparse</code> <code>LinearOperator</code> <p>Sparse action structure representing the blocks.</p> Source code in <code>src/cagpjax/policies/block_sparse.py</code> <pre><code>@override\ndef to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n    \"\"\"Convert to block diagonal sparse action operators.\n\n    Args:\n        A: Linear operator (unused).\n\n    Returns:\n        BlockDiagonalSparse: Sparse action structure representing the blocks.\n    \"\"\"\n    return BlockDiagonalSparse(self.nz_values.value, self.n_actions)\n</code></pre>"},{"location":"reference/cagpjax/#cagpjax.ComputationAwareGP","title":"<code>ComputationAwareGP</code>","text":"<p>               Bases: <code>AbstractComputationAwareGP</code></p> <p>Computation-aware Gaussian Process model.</p> <p>This model implements scalable GP inference by using batch linear solver policies to project the kernel and data to a lower-dimensional subspace, while accounting for the extra uncertainty imposed by observing only this subspace.</p> <p>Attributes:</p> Name Type Description <code>posterior</code> <code>ConjugatePosterior</code> <p>The original (exact) posterior.</p> <code>policy</code> <code>AbstractBatchLinearSolverPolicy</code> <p>The batch linear solver policy.</p> <code>solver_method</code> <code>AbstractLinearSolverMethod</code> <p>The linear solver method to use for solving linear systems with positive semi-definite operators.</p> Notes <ul> <li>Only single-output models are currently supported.</li> </ul> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>class ComputationAwareGP(AbstractComputationAwareGP):\n    \"\"\"Computation-aware Gaussian Process model.\n\n    This model implements scalable GP inference by using batch linear solver\n    policies to project the kernel and data to a lower-dimensional subspace, while\n    accounting for the extra uncertainty imposed by observing only this subspace.\n\n    Attributes:\n        posterior: The original (exact) posterior.\n        policy: The batch linear solver policy.\n        solver_method: The linear solver method to use for solving linear systems\n            with positive semi-definite operators.\n\n    Notes:\n        - Only single-output models are currently supported.\n    \"\"\"\n\n    posterior: ConjugatePosterior\n    policy: AbstractBatchLinearSolverPolicy\n    solver_method: AbstractLinearSolverMethod\n\n    def __init__(\n        self,\n        posterior: ConjugatePosterior,\n        policy: AbstractBatchLinearSolverPolicy,\n        solver_method: AbstractLinearSolverMethod = Cholesky(1e-6),\n    ):\n        \"\"\"Initialize the Computation-Aware GP model.\n\n        Args:\n            posterior: GPJax conjugate posterior.\n            policy: The batch linear solver policy that defines the subspace into\n                which the data is projected.\n            solver_method: The linear solver method to use for solving linear systems with\n                positive semi-definite operators.\n        \"\"\"\n        super().__init__(posterior)\n        self.policy = policy\n        self.solver_method = solver_method\n        self._posterior_params: _ProjectedPosteriorParameters | None = None\n\n    @property\n    def is_conditioned(self) -&gt; bool:\n        \"\"\"Whether the model has been conditioned on training data.\"\"\"\n        return self._posterior_params is not None\n\n    def condition(self, train_data: Dataset) -&gt; None:\n        \"\"\"Compute and store the projected quantities of the conditioned GP posterior.\n\n        Args:\n            train_data: The training data used to fit the GP.\n        \"\"\"\n        # Ensure we have supervised training data\n        if train_data.X is None or train_data.y is None:\n            raise ValueError(\"Training data must be supervised.\")\n\n        # Unpack training data\n        x = jnp.atleast_2d(train_data.X)\n        y = jnp.atleast_1d(train_data.y).squeeze()\n\n        # Unpack prior and likelihood\n        prior = self.posterior.prior\n        likelihood = self.posterior.likelihood\n\n        # Mean and covariance of prior-predictive distribution\n        mean_prior = prior.mean_function(x).squeeze()\n        # Work around GPJax promoting dtype of mean to float64 (See JaxGaussianProcesses/GPJax#523)\n        if isinstance(prior.mean_function, Constant):\n            mean_prior = mean_prior.astype(prior.mean_function.constant.value.dtype)\n        cov_xx = prior.kernel.gram(x)\n        obs_cov = diag_like(cov_xx, likelihood.obs_stddev.value**2)\n        cov_prior = cov_xx + obs_cov\n\n        # Project quantities to subspace\n        actions = self.policy.to_actions(cov_prior)\n        obs_cov_proj = congruence_transform(actions, obs_cov)\n        cov_prior_proj = congruence_transform(actions, cov_prior)\n        cov_prior_proj_solver = self.solver_method(cov_prior_proj)\n\n        residual_proj = actions.T @ (y - mean_prior)\n        repr_weights_proj = cov_prior_proj_solver.solve(residual_proj)\n\n        self._posterior_params = _ProjectedPosteriorParameters(\n            x=x,\n            actions=actions,\n            obs_cov_proj=obs_cov_proj,\n            cov_prior_proj_solver=cov_prior_proj_solver,\n            residual_proj=residual_proj,\n            repr_weights_proj=repr_weights_proj,\n        )\n\n    @override\n    def predict(\n        self, test_inputs: Float[Array, \"N D\"] | None = None\n    ) -&gt; GaussianDistribution:\n        \"\"\"Compute the predictive distribution of the GP at the test inputs.\n\n        ``condition`` must be called before this method can be used.\n\n        Args:\n            test_inputs: The test inputs at which to make predictions. If not provided,\n                predictions are made at the training inputs.\n\n        Returns:\n            GaussianDistribution: The predictive distribution of the GP at the\n                test inputs.\n        \"\"\"\n        if not self.is_conditioned:\n            raise ValueError(\"Model is not yet conditioned. Call ``condition`` first.\")\n\n        # help out pyright\n        assert self._posterior_params is not None\n\n        # Unpack posterior parameters\n        x = self._posterior_params.x\n        actions = self._posterior_params.actions\n        cov_prior_proj_solver = self._posterior_params.cov_prior_proj_solver\n        repr_weights_proj = self._posterior_params.repr_weights_proj\n\n        # Predictions at test points\n        z = test_inputs if test_inputs is not None else x\n        prior = self.posterior.prior\n        mean_z = prior.mean_function(z).squeeze()\n        # Work around GPJax promoting dtype of mean to float64 (See JaxGaussianProcesses/GPJax#523)\n        if isinstance(prior.mean_function, Constant):\n            mean_z = mean_z.astype(prior.mean_function.constant.value.dtype)\n        cov_zz = prior.kernel.gram(z)\n        cov_zx = cov_zz if test_inputs is None else prior.kernel.cross_covariance(z, x)\n        cov_zx_proj = cov_zx @ actions\n\n        # Posterior predictive distribution\n        mean_pred = jnp.atleast_1d(mean_z + cov_zx_proj @ repr_weights_proj)\n        cov_pred = cov_zz - cov_prior_proj_solver.inv_congruence_transform(\n            cov_zx_proj.T\n        )\n        cov_pred = cola.PSD(cov_pred + diag_like(cov_pred, self.posterior.jitter))\n\n        return GaussianDistribution(mean_pred, cov_pred)\n\n    def prior_kl(self) -&gt; ScalarFloat:\n        r\"\"\"Compute KL divergence between CaGP posterior and GP prior..\n\n        Calculates $\\mathrm{KL}[q(f) || p(f)]$, where $q(f)$ is the CaGP\n        posterior approximation and $p(f)$ is the GP prior.\n\n        ``condition`` must be called before this method can be used.\n\n        Returns:\n            KL divergence value (scalar).\n        \"\"\"\n        if not self.is_conditioned:\n            raise ValueError(\"Model is not yet conditioned. Call ``condition`` first.\")\n\n        # help out pyright\n        assert self._posterior_params is not None\n\n        # Unpack posterior parameters\n        obs_cov_proj = self._posterior_params.obs_cov_proj\n        cov_prior_proj_solver = self._posterior_params.cov_prior_proj_solver\n        residual_proj = self._posterior_params.residual_proj\n        repr_weights_proj = self._posterior_params.repr_weights_proj\n\n        obs_cov_proj_solver = self.solver_method(obs_cov_proj)\n\n        kl = (\n            _kl_divergence_from_solvers(\n                residual_proj,\n                obs_cov_proj_solver,\n                jnp.zeros_like(residual_proj),\n                cov_prior_proj_solver,\n            )\n            - 0.5 * congruence_transform(repr_weights_proj.T, obs_cov_proj).squeeze()\n        )\n\n        return kl\n</code></pre>"},{"location":"reference/cagpjax/#cagpjax.ComputationAwareGP.is_conditioned","title":"<code>is_conditioned</code>  <code>property</code>","text":"<p>Whether the model has been conditioned on training data.</p>"},{"location":"reference/cagpjax/#cagpjax.ComputationAwareGP.__init__","title":"<code>__init__(posterior, policy, solver_method=Cholesky(1e-06))</code>","text":"<p>Initialize the Computation-Aware GP model.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>ConjugatePosterior</code> <p>GPJax conjugate posterior.</p> required <code>policy</code> <code>AbstractBatchLinearSolverPolicy</code> <p>The batch linear solver policy that defines the subspace into which the data is projected.</p> required <code>solver_method</code> <code>AbstractLinearSolverMethod</code> <p>The linear solver method to use for solving linear systems with positive semi-definite operators.</p> <code>Cholesky(1e-06)</code> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def __init__(\n    self,\n    posterior: ConjugatePosterior,\n    policy: AbstractBatchLinearSolverPolicy,\n    solver_method: AbstractLinearSolverMethod = Cholesky(1e-6),\n):\n    \"\"\"Initialize the Computation-Aware GP model.\n\n    Args:\n        posterior: GPJax conjugate posterior.\n        policy: The batch linear solver policy that defines the subspace into\n            which the data is projected.\n        solver_method: The linear solver method to use for solving linear systems with\n            positive semi-definite operators.\n    \"\"\"\n    super().__init__(posterior)\n    self.policy = policy\n    self.solver_method = solver_method\n    self._posterior_params: _ProjectedPosteriorParameters | None = None\n</code></pre>"},{"location":"reference/cagpjax/#cagpjax.ComputationAwareGP.condition","title":"<code>condition(train_data)</code>","text":"<p>Compute and store the projected quantities of the conditioned GP posterior.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>Dataset</code> <p>The training data used to fit the GP.</p> required Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def condition(self, train_data: Dataset) -&gt; None:\n    \"\"\"Compute and store the projected quantities of the conditioned GP posterior.\n\n    Args:\n        train_data: The training data used to fit the GP.\n    \"\"\"\n    # Ensure we have supervised training data\n    if train_data.X is None or train_data.y is None:\n        raise ValueError(\"Training data must be supervised.\")\n\n    # Unpack training data\n    x = jnp.atleast_2d(train_data.X)\n    y = jnp.atleast_1d(train_data.y).squeeze()\n\n    # Unpack prior and likelihood\n    prior = self.posterior.prior\n    likelihood = self.posterior.likelihood\n\n    # Mean and covariance of prior-predictive distribution\n    mean_prior = prior.mean_function(x).squeeze()\n    # Work around GPJax promoting dtype of mean to float64 (See JaxGaussianProcesses/GPJax#523)\n    if isinstance(prior.mean_function, Constant):\n        mean_prior = mean_prior.astype(prior.mean_function.constant.value.dtype)\n    cov_xx = prior.kernel.gram(x)\n    obs_cov = diag_like(cov_xx, likelihood.obs_stddev.value**2)\n    cov_prior = cov_xx + obs_cov\n\n    # Project quantities to subspace\n    actions = self.policy.to_actions(cov_prior)\n    obs_cov_proj = congruence_transform(actions, obs_cov)\n    cov_prior_proj = congruence_transform(actions, cov_prior)\n    cov_prior_proj_solver = self.solver_method(cov_prior_proj)\n\n    residual_proj = actions.T @ (y - mean_prior)\n    repr_weights_proj = cov_prior_proj_solver.solve(residual_proj)\n\n    self._posterior_params = _ProjectedPosteriorParameters(\n        x=x,\n        actions=actions,\n        obs_cov_proj=obs_cov_proj,\n        cov_prior_proj_solver=cov_prior_proj_solver,\n        residual_proj=residual_proj,\n        repr_weights_proj=repr_weights_proj,\n    )\n</code></pre>"},{"location":"reference/cagpjax/#cagpjax.ComputationAwareGP.predict","title":"<code>predict(test_inputs=None)</code>","text":"<p>Compute the predictive distribution of the GP at the test inputs.</p> <p><code>condition</code> must be called before this method can be used.</p> <p>Parameters:</p> Name Type Description Default <code>test_inputs</code> <code>Float[Array, 'N D'] | None</code> <p>The test inputs at which to make predictions. If not provided, predictions are made at the training inputs.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GaussianDistribution</code> <code>GaussianDistribution</code> <p>The predictive distribution of the GP at the test inputs.</p> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>@override\ndef predict(\n    self, test_inputs: Float[Array, \"N D\"] | None = None\n) -&gt; GaussianDistribution:\n    \"\"\"Compute the predictive distribution of the GP at the test inputs.\n\n    ``condition`` must be called before this method can be used.\n\n    Args:\n        test_inputs: The test inputs at which to make predictions. If not provided,\n            predictions are made at the training inputs.\n\n    Returns:\n        GaussianDistribution: The predictive distribution of the GP at the\n            test inputs.\n    \"\"\"\n    if not self.is_conditioned:\n        raise ValueError(\"Model is not yet conditioned. Call ``condition`` first.\")\n\n    # help out pyright\n    assert self._posterior_params is not None\n\n    # Unpack posterior parameters\n    x = self._posterior_params.x\n    actions = self._posterior_params.actions\n    cov_prior_proj_solver = self._posterior_params.cov_prior_proj_solver\n    repr_weights_proj = self._posterior_params.repr_weights_proj\n\n    # Predictions at test points\n    z = test_inputs if test_inputs is not None else x\n    prior = self.posterior.prior\n    mean_z = prior.mean_function(z).squeeze()\n    # Work around GPJax promoting dtype of mean to float64 (See JaxGaussianProcesses/GPJax#523)\n    if isinstance(prior.mean_function, Constant):\n        mean_z = mean_z.astype(prior.mean_function.constant.value.dtype)\n    cov_zz = prior.kernel.gram(z)\n    cov_zx = cov_zz if test_inputs is None else prior.kernel.cross_covariance(z, x)\n    cov_zx_proj = cov_zx @ actions\n\n    # Posterior predictive distribution\n    mean_pred = jnp.atleast_1d(mean_z + cov_zx_proj @ repr_weights_proj)\n    cov_pred = cov_zz - cov_prior_proj_solver.inv_congruence_transform(\n        cov_zx_proj.T\n    )\n    cov_pred = cola.PSD(cov_pred + diag_like(cov_pred, self.posterior.jitter))\n\n    return GaussianDistribution(mean_pred, cov_pred)\n</code></pre>"},{"location":"reference/cagpjax/#cagpjax.ComputationAwareGP.prior_kl","title":"<code>prior_kl()</code>","text":"<p>Compute KL divergence between CaGP posterior and GP prior..</p> <p>Calculates \\(\\mathrm{KL}[q(f) || p(f)]\\), where \\(q(f)\\) is the CaGP posterior approximation and \\(p(f)\\) is the GP prior.</p> <p><code>condition</code> must be called before this method can be used.</p> <p>Returns:</p> Type Description <code>ScalarFloat</code> <p>KL divergence value (scalar).</p> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def prior_kl(self) -&gt; ScalarFloat:\n    r\"\"\"Compute KL divergence between CaGP posterior and GP prior..\n\n    Calculates $\\mathrm{KL}[q(f) || p(f)]$, where $q(f)$ is the CaGP\n    posterior approximation and $p(f)$ is the GP prior.\n\n    ``condition`` must be called before this method can be used.\n\n    Returns:\n        KL divergence value (scalar).\n    \"\"\"\n    if not self.is_conditioned:\n        raise ValueError(\"Model is not yet conditioned. Call ``condition`` first.\")\n\n    # help out pyright\n    assert self._posterior_params is not None\n\n    # Unpack posterior parameters\n    obs_cov_proj = self._posterior_params.obs_cov_proj\n    cov_prior_proj_solver = self._posterior_params.cov_prior_proj_solver\n    residual_proj = self._posterior_params.residual_proj\n    repr_weights_proj = self._posterior_params.repr_weights_proj\n\n    obs_cov_proj_solver = self.solver_method(obs_cov_proj)\n\n    kl = (\n        _kl_divergence_from_solvers(\n            residual_proj,\n            obs_cov_proj_solver,\n            jnp.zeros_like(residual_proj),\n            cov_prior_proj_solver,\n        )\n        - 0.5 * congruence_transform(repr_weights_proj.T, obs_cov_proj).squeeze()\n    )\n\n    return kl\n</code></pre>"},{"location":"reference/cagpjax/#cagpjax.LanczosPolicy","title":"<code>LanczosPolicy</code>","text":"<p>               Bases: <code>AbstractBatchLinearSolverPolicy</code></p> <p>Lanczos-based policy for eigenvalue decomposition approximation.</p> <p>This policy uses the Lanczos algorithm to compute the top <code>n_actions</code> eigenvectors of the linear operator \\(A\\).</p> <p>Attributes:</p> Name Type Description <code>n_actions</code> <code>int</code> <p>Number of Lanczos vectors/actions to compute.</p> <code>key</code> <code>PRNGKeyArray | None</code> <p>Random key for reproducible Lanczos iterations.</p> Source code in <code>src/cagpjax/policies/lanczos.py</code> <pre><code>class LanczosPolicy(AbstractBatchLinearSolverPolicy):\n    \"\"\"Lanczos-based policy for eigenvalue decomposition approximation.\n\n    This policy uses the Lanczos algorithm to compute the top ``n_actions`` eigenvectors\n    of the linear operator $A$.\n\n    Attributes:\n        n_actions: Number of Lanczos vectors/actions to compute.\n        key: Random key for reproducible Lanczos iterations.\n    \"\"\"\n\n    key: PRNGKeyArray | None\n    grad_rtol: float | None\n\n    def __init__(\n        self,\n        n_actions: int | None,\n        key: PRNGKeyArray | None = None,\n        grad_rtol: float | None = 0.0,\n    ):\n        \"\"\"Initialize the Lanczos policy.\n\n        Args:\n            n_actions: Number of Lanczos vectors to compute.\n            key: Random key for initialization.\n            grad_rtol: Specifies the cutoff for similar eigenvalues, used to improve\n                gradient computation for (almost-)degenerate matrices.\n                If not provided, the default is 0.0.\n                If None or negative, all eigenvalues are treated as distinct.\n                (see [`cagpjax.linalg.eigh`][] for more details)\n        \"\"\"\n        self._n_actions: int = n_actions\n        self.key = key\n        self.grad_rtol = grad_rtol\n\n    @property\n    @override\n    def n_actions(self) -&gt; int:\n        return self._n_actions\n\n    @override\n    def to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n        \"\"\"Compute action matrix.\n\n        Args:\n            A: Symmetric linear operator representing the linear system.\n\n        Returns:\n            Linear operator containing the Lanczos vectors as columns.\n        \"\"\"\n        vecs = eigh(\n            A, alg=Lanczos(self.n_actions, key=self.key), grad_rtol=self.grad_rtol\n        ).eigenvectors\n        return vecs\n</code></pre>"},{"location":"reference/cagpjax/#cagpjax.LanczosPolicy.__init__","title":"<code>__init__(n_actions, key=None, grad_rtol=0.0)</code>","text":"<p>Initialize the Lanczos policy.</p> <p>Parameters:</p> Name Type Description Default <code>n_actions</code> <code>int | None</code> <p>Number of Lanczos vectors to compute.</p> required <code>key</code> <code>PRNGKeyArray | None</code> <p>Random key for initialization.</p> <code>None</code> <code>grad_rtol</code> <code>float | None</code> <p>Specifies the cutoff for similar eigenvalues, used to improve gradient computation for (almost-)degenerate matrices. If not provided, the default is 0.0. If None or negative, all eigenvalues are treated as distinct. (see <code>cagpjax.linalg.eigh</code> for more details)</p> <code>0.0</code> Source code in <code>src/cagpjax/policies/lanczos.py</code> <pre><code>def __init__(\n    self,\n    n_actions: int | None,\n    key: PRNGKeyArray | None = None,\n    grad_rtol: float | None = 0.0,\n):\n    \"\"\"Initialize the Lanczos policy.\n\n    Args:\n        n_actions: Number of Lanczos vectors to compute.\n        key: Random key for initialization.\n        grad_rtol: Specifies the cutoff for similar eigenvalues, used to improve\n            gradient computation for (almost-)degenerate matrices.\n            If not provided, the default is 0.0.\n            If None or negative, all eigenvalues are treated as distinct.\n            (see [`cagpjax.linalg.eigh`][] for more details)\n    \"\"\"\n    self._n_actions: int = n_actions\n    self.key = key\n    self.grad_rtol = grad_rtol\n</code></pre>"},{"location":"reference/cagpjax/#cagpjax.LanczosPolicy.to_actions","title":"<code>to_actions(A)</code>","text":"<p>Compute action matrix.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>LinearOperator</code> <p>Symmetric linear operator representing the linear system.</p> required <p>Returns:</p> Type Description <code>LinearOperator</code> <p>Linear operator containing the Lanczos vectors as columns.</p> Source code in <code>src/cagpjax/policies/lanczos.py</code> <pre><code>@override\ndef to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n    \"\"\"Compute action matrix.\n\n    Args:\n        A: Symmetric linear operator representing the linear system.\n\n    Returns:\n        Linear operator containing the Lanczos vectors as columns.\n    \"\"\"\n    vecs = eigh(\n        A, alg=Lanczos(self.n_actions, key=self.key), grad_rtol=self.grad_rtol\n    ).eigenvectors\n    return vecs\n</code></pre>"},{"location":"reference/cagpjax/typing/","title":"typing","text":""},{"location":"reference/cagpjax/typing/#cagpjax.typing","title":"<code>cagpjax.typing</code>","text":""},{"location":"reference/cagpjax/linalg/","title":"linalg","text":""},{"location":"reference/cagpjax/linalg/#cagpjax.linalg","title":"<code>cagpjax.linalg</code>","text":"<p>Linear algebra functions.</p>"},{"location":"reference/cagpjax/linalg/#cagpjax.linalg.Eigh","title":"<code>Eigh</code>","text":"<p>               Bases: <code>Algorithm</code></p> <p>Eigh algorithm for eigenvalue decomposition.</p> Source code in <code>src/cagpjax/linalg/eigh.py</code> <pre><code>class Eigh(cola.linalg.Algorithm):\n    \"\"\"\n    Eigh algorithm for eigenvalue decomposition.\n    \"\"\"\n</code></pre>"},{"location":"reference/cagpjax/linalg/#cagpjax.linalg.Lanczos","title":"<code>Lanczos</code>","text":"<p>               Bases: <code>Algorithm</code></p> <p>Lanczos algorithm for approximate partial eigenvalue decomposition.</p> <p>Parameters:</p> Name Type Description Default <code>max_iters</code> <code>int | None</code> <p>Maximum number of iterations (number of eigenvalues/vectors to compute). If <code>None</code>, all eigenvalues/eigenvectors are computed.</p> <code>None</code> <code>v0</code> <code>Float[Array, N] | None</code> <p>Initial vector. If <code>None</code>, a random vector is generated using <code>key</code>.</p> <code>None</code> <code>key</code> <code>PRNGKeyArray | None</code> <p>Random key for generating a random initial vector if <code>v0</code> is not provided.</p> <code>None</code> Source code in <code>src/cagpjax/linalg/eigh.py</code> <pre><code>class Lanczos(cola.linalg.Algorithm):\n    \"\"\"Lanczos algorithm for approximate partial eigenvalue decomposition.\n\n    Args:\n        max_iters: Maximum number of iterations (number of eigenvalues/vectors to compute).\n            If `None`, all eigenvalues/eigenvectors are computed.\n        v0: Initial vector. If `None`, a random vector is generated using `key`.\n        key: Random key for generating a random initial vector if `v0` is\n            not provided.\n    \"\"\"\n\n    max_iters: int | None\n    v0: Float[Array, \"N\"] | None\n    key: PRNGKeyArray | None\n\n    def __init__(\n        self,\n        max_iters: int | None = None,\n        /,\n        *,\n        v0: Float[Array, \"N\"] | None = None,\n        key: PRNGKeyArray | None = None,\n    ):\n        self.max_iters = max_iters\n        self.v0 = v0\n        self.key = key\n</code></pre>"},{"location":"reference/cagpjax/linalg/#cagpjax.linalg.OrthogonalizationMethod","title":"<code>OrthogonalizationMethod</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Methods for orthogonalizing a matrix.</p> Source code in <code>src/cagpjax/linalg/orthogonalize.py</code> <pre><code>class OrthogonalizationMethod(Enum):\n    \"\"\"Methods for orthogonalizing a matrix.\"\"\"\n\n    QR = \"qr\"\n    \"\"\"Householder QR decomposition\"\"\"\n    CGS = \"cgs\"\n    \"\"\"Classical Gram\u2013Schmidt orthogonalization\"\"\"\n    MGS = \"mgs\"\n    \"\"\"Modified Gram\u2013Schmidt orthogonalization\"\"\"\n</code></pre>"},{"location":"reference/cagpjax/linalg/#cagpjax.linalg.OrthogonalizationMethod.CGS","title":"<code>CGS = 'cgs'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Classical Gram\u2013Schmidt orthogonalization</p>"},{"location":"reference/cagpjax/linalg/#cagpjax.linalg.OrthogonalizationMethod.MGS","title":"<code>MGS = 'mgs'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Modified Gram\u2013Schmidt orthogonalization</p>"},{"location":"reference/cagpjax/linalg/#cagpjax.linalg.OrthogonalizationMethod.QR","title":"<code>QR = 'qr'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Householder QR decomposition</p>"},{"location":"reference/cagpjax/linalg/#cagpjax.linalg.congruence_transform","title":"<code>congruence_transform(A, B)</code>","text":"<pre><code>congruence_transform(A: Any, B: Any) -&gt; Any\n</code></pre><pre><code>congruence_transform(A: Diagonal, B: Diagonal) -&gt; Diagonal\n</code></pre><pre><code>congruence_transform(A: BlockDiagonalSparse, B: Diagonal | ScalarMul) -&gt; Diagonal\n</code></pre> <p>Congruence transformation <code>A.T @ B @ A</code>.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Any</code> <p>Linear operator or array to be applied.</p> required <code>B</code> <code>Any</code> <p>Square linear operator or array to be transformed.</p> required Source code in <code>src/cagpjax/linalg/congruence.py</code> <pre><code>@cola.dispatch\ndef congruence_transform(A: Any, B: Any) -&gt; Any:\n    \"\"\"Congruence transformation ``A.T @ B @ A``.\n\n    Args:\n        A: Linear operator or array to be applied.\n        B: Square linear operator or array to be transformed.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/linalg/congruence/","title":"congruence","text":""},{"location":"reference/cagpjax/linalg/congruence/#cagpjax.linalg.congruence","title":"<code>cagpjax.linalg.congruence</code>","text":"<p>Congruence transformations for linear operators.</p>"},{"location":"reference/cagpjax/linalg/congruence/#cagpjax.linalg.congruence.congruence_transform","title":"<code>congruence_transform(A, B)</code>","text":"<pre><code>congruence_transform(A: Any, B: Any) -&gt; Any\n</code></pre><pre><code>congruence_transform(A: Diagonal, B: Diagonal) -&gt; Diagonal\n</code></pre><pre><code>congruence_transform(A: BlockDiagonalSparse, B: Diagonal | ScalarMul) -&gt; Diagonal\n</code></pre> <p>Congruence transformation <code>A.T @ B @ A</code>.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Any</code> <p>Linear operator or array to be applied.</p> required <code>B</code> <code>Any</code> <p>Square linear operator or array to be transformed.</p> required Source code in <code>src/cagpjax/linalg/congruence.py</code> <pre><code>@cola.dispatch\ndef congruence_transform(A: Any, B: Any) -&gt; Any:\n    \"\"\"Congruence transformation ``A.T @ B @ A``.\n\n    Args:\n        A: Linear operator or array to be applied.\n        B: Square linear operator or array to be transformed.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/linalg/eigh/","title":"eigh","text":""},{"location":"reference/cagpjax/linalg/eigh/#cagpjax.linalg.eigh","title":"<code>cagpjax.linalg.eigh</code>","text":"<p>Hermitian eigenvalue decomposition.</p>"},{"location":"reference/cagpjax/linalg/eigh/#cagpjax.linalg.eigh.Eigh","title":"<code>Eigh</code>","text":"<p>               Bases: <code>Algorithm</code></p> <p>Eigh algorithm for eigenvalue decomposition.</p> Source code in <code>src/cagpjax/linalg/eigh.py</code> <pre><code>class Eigh(cola.linalg.Algorithm):\n    \"\"\"\n    Eigh algorithm for eigenvalue decomposition.\n    \"\"\"\n</code></pre>"},{"location":"reference/cagpjax/linalg/eigh/#cagpjax.linalg.eigh.EighResult","title":"<code>EighResult</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Result of Hermitian eigenvalue decomposition.</p> <p>Attributes:</p> Name Type Description <code>eigenvalues</code> <code>Float[Array, N]</code> <p>Eigenvalues of the operator.</p> <code>eigenvectors</code> <code>LinearOperator</code> <p>Eigenvectors of the operator.</p> Source code in <code>src/cagpjax/linalg/eigh.py</code> <pre><code>class EighResult(NamedTuple):\n    \"\"\"Result of Hermitian eigenvalue decomposition.\n\n    Attributes:\n        eigenvalues: Eigenvalues of the operator.\n        eigenvectors: Eigenvectors of the operator.\n    \"\"\"\n\n    eigenvalues: Float[Array, \"N\"]\n    eigenvectors: LinearOperator\n</code></pre>"},{"location":"reference/cagpjax/linalg/eigh/#cagpjax.linalg.eigh.Lanczos","title":"<code>Lanczos</code>","text":"<p>               Bases: <code>Algorithm</code></p> <p>Lanczos algorithm for approximate partial eigenvalue decomposition.</p> <p>Parameters:</p> Name Type Description Default <code>max_iters</code> <code>int | None</code> <p>Maximum number of iterations (number of eigenvalues/vectors to compute). If <code>None</code>, all eigenvalues/eigenvectors are computed.</p> <code>None</code> <code>v0</code> <code>Float[Array, N] | None</code> <p>Initial vector. If <code>None</code>, a random vector is generated using <code>key</code>.</p> <code>None</code> <code>key</code> <code>PRNGKeyArray | None</code> <p>Random key for generating a random initial vector if <code>v0</code> is not provided.</p> <code>None</code> Source code in <code>src/cagpjax/linalg/eigh.py</code> <pre><code>class Lanczos(cola.linalg.Algorithm):\n    \"\"\"Lanczos algorithm for approximate partial eigenvalue decomposition.\n\n    Args:\n        max_iters: Maximum number of iterations (number of eigenvalues/vectors to compute).\n            If `None`, all eigenvalues/eigenvectors are computed.\n        v0: Initial vector. If `None`, a random vector is generated using `key`.\n        key: Random key for generating a random initial vector if `v0` is\n            not provided.\n    \"\"\"\n\n    max_iters: int | None\n    v0: Float[Array, \"N\"] | None\n    key: PRNGKeyArray | None\n\n    def __init__(\n        self,\n        max_iters: int | None = None,\n        /,\n        *,\n        v0: Float[Array, \"N\"] | None = None,\n        key: PRNGKeyArray | None = None,\n    ):\n        self.max_iters = max_iters\n        self.v0 = v0\n        self.key = key\n</code></pre>"},{"location":"reference/cagpjax/linalg/eigh/#cagpjax.linalg.eigh.eigh","title":"<code>eigh(A, alg=Eigh(), grad_rtol=None)</code>","text":"<p>Compute the Hermitian eigenvalue decomposition of a linear operator.</p> <p>For some algorithms, the decomposition may be approximate or partial.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>LinearOperator</code> <p>Hermitian linear operator.</p> required <code>alg</code> <code>Algorithm</code> <p>Algorithm for eigenvalue decomposition.</p> <code>Eigh()</code> <code>grad_rtol</code> <code>float | None</code> <p>Specifies the cutoff for similar eigenvalues, used to improve gradient computation for (almost-)degenerate matrices. If not provided, the default is 0.0. If None or negative, all eigenvalues are treated as distinct.</p> <code>None</code> <p>Returns:</p> Type Description <code>EighResult</code> <p>A named tuple of <code>(eigenvalues, eigenvectors)</code> where <code>eigenvectors</code> is a (semi-)orthogonal <code>LinearOperator</code>.</p> Note <p>Degenerate matrices have repeated eigenvalues. The set of eigenvectors that correspond to the same eigenvalue is not unique but instead forms a subspace. <code>grad_rtol</code> only improves stability of gradient-computation if the function being differentiated depends only depends on these subspaces and not the specific eigenvectors themselves.</p> Source code in <code>src/cagpjax/linalg/eigh.py</code> <pre><code>def eigh(\n    A: LinearOperator,\n    alg: cola.linalg.Algorithm = Eigh(),\n    grad_rtol: float | None = None,\n) -&gt; EighResult:\n    \"\"\"Compute the Hermitian eigenvalue decomposition of a linear operator.\n\n    For some algorithms, the decomposition may be approximate or partial.\n\n    Args:\n        A: Hermitian linear operator.\n        alg: Algorithm for eigenvalue decomposition.\n        grad_rtol: Specifies the cutoff for similar eigenvalues, used to improve\n            gradient computation for (almost-)degenerate matrices.\n            If not provided, the default is 0.0.\n            If None or negative, all eigenvalues are treated as distinct.\n\n    Returns:\n        A named tuple of `(eigenvalues, eigenvectors)` where `eigenvectors` is a\n            (semi-)orthogonal `LinearOperator`.\n\n    Note:\n        Degenerate matrices have repeated eigenvalues.\n        The set of eigenvectors that correspond to the same eigenvalue is not unique\n        but instead forms a subspace.\n        `grad_rtol` only improves stability of gradient-computation if the function\n        being differentiated depends only depends on these subspaces and not the\n        specific eigenvectors themselves.\n    \"\"\"\n    if grad_rtol is None:\n        grad_rtol = -1.0\n    vals, vecs = _eigh(A, alg, grad_rtol)  # pyright: ignore[reportArgumentType]\n    if vecs.shape[-1] == A.shape[-1]:\n        vecs = cola.Unitary(vecs)\n    else:\n        vecs = cola.Stiefel(vecs)\n    return EighResult(vals, vecs)\n</code></pre>"},{"location":"reference/cagpjax/linalg/lower_cholesky/","title":"lower_cholesky","text":""},{"location":"reference/cagpjax/linalg/lower_cholesky/#cagpjax.linalg.lower_cholesky","title":"<code>cagpjax.linalg.lower_cholesky</code>","text":"<p>Lower Cholesky decomposition of positive semidefinite operators.</p>"},{"location":"reference/cagpjax/linalg/lower_cholesky/#cagpjax.linalg.lower_cholesky.lower_cholesky","title":"<code>lower_cholesky(A, jitter=None)</code>","text":"<p>Lower Cholesky decomposition of a positive semidefinite operator.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>LinearOperator</code> <p>Positive semidefinite operator</p> required <code>jitter</code> <code>ScalarFloat | None</code> <p>Positive jitter to add to the operator.</p> <code>None</code> <p>Returns:</p> Type Description <code>LinearOperator</code> <p>Lower Cholesky factor of A.</p> Source code in <code>src/cagpjax/linalg/lower_cholesky.py</code> <pre><code>def lower_cholesky(\n    A: LinearOperator, jitter: ScalarFloat | None = None\n) -&gt; LinearOperator:\n    \"\"\"Lower Cholesky decomposition of a positive semidefinite operator.\n\n    Args:\n        A: Positive semidefinite operator\n        jitter: Positive jitter to add to the operator.\n\n    Returns:\n        Lower Cholesky factor of A.\n    \"\"\"\n    if jitter is None:\n        return gpjax.lower_cholesky.lower_cholesky(cola.PSD(A))\n    return _lower_cholesky_jittered(A, jitter)\n</code></pre>"},{"location":"reference/cagpjax/linalg/orthogonalize/","title":"orthogonalize","text":""},{"location":"reference/cagpjax/linalg/orthogonalize/#cagpjax.linalg.orthogonalize","title":"<code>cagpjax.linalg.orthogonalize</code>","text":"<p>Orthogonalization methods.</p>"},{"location":"reference/cagpjax/linalg/orthogonalize/#cagpjax.linalg.orthogonalize.OrthogonalizationMethod","title":"<code>OrthogonalizationMethod</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Methods for orthogonalizing a matrix.</p> Source code in <code>src/cagpjax/linalg/orthogonalize.py</code> <pre><code>class OrthogonalizationMethod(Enum):\n    \"\"\"Methods for orthogonalizing a matrix.\"\"\"\n\n    QR = \"qr\"\n    \"\"\"Householder QR decomposition\"\"\"\n    CGS = \"cgs\"\n    \"\"\"Classical Gram\u2013Schmidt orthogonalization\"\"\"\n    MGS = \"mgs\"\n    \"\"\"Modified Gram\u2013Schmidt orthogonalization\"\"\"\n</code></pre>"},{"location":"reference/cagpjax/linalg/orthogonalize/#cagpjax.linalg.orthogonalize.OrthogonalizationMethod.CGS","title":"<code>CGS = 'cgs'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Classical Gram\u2013Schmidt orthogonalization</p>"},{"location":"reference/cagpjax/linalg/orthogonalize/#cagpjax.linalg.orthogonalize.OrthogonalizationMethod.MGS","title":"<code>MGS = 'mgs'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Modified Gram\u2013Schmidt orthogonalization</p>"},{"location":"reference/cagpjax/linalg/orthogonalize/#cagpjax.linalg.orthogonalize.OrthogonalizationMethod.QR","title":"<code>QR = 'qr'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Householder QR decomposition</p>"},{"location":"reference/cagpjax/linalg/orthogonalize/#cagpjax.linalg.orthogonalize.orthogonalize","title":"<code>orthogonalize(A, /, method=OrthogonalizationMethod.QR, n_reortho=0)</code>","text":"<p>Orthogonalize the operator using the specified method.</p> <p>The columns of the resulting matrix should span a (super-)space of the columns of the input matrix and be mutually orthogonal. For column-rank-deficient matrices, some methods (e.g. Gram-Schmidt variants) may include columns of norm 0.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Float[Array, 'm n'] | LinearOperator</code> <p>The operator to orthogonalize.</p> required <code>method</code> <code>OrthogonalizationMethod</code> <p>The method to use for orthogonalization.</p> <code>QR</code> <code>n_reortho</code> <code>int</code> <p>The number of times to re-orthogonalize each column. Reorthogonalizing once is generally sufficient to improve orthogonality for Gram-Schmidt variants (see e.g. 10.1007/s00211-005-0615-4).</p> <code>0</code> <p>Returns:</p> Type Description <code>Float[Array, 'm n'] | LinearOperator</code> <p>The orthogonalized operator. If the input is a LinearOperator, then so is the output.</p> Source code in <code>src/cagpjax/linalg/orthogonalize.py</code> <pre><code>def orthogonalize(\n    A: Float[Array, \"m n\"] | cola.ops.LinearOperator,\n    /,\n    method: OrthogonalizationMethod = OrthogonalizationMethod.QR,\n    n_reortho: int = 0,\n) -&gt; Float[Array, \"m n\"] | cola.ops.LinearOperator:\n    \"\"\"\n    Orthogonalize the operator using the specified method.\n\n    The columns of the resulting matrix should span a (super-)space of the columns of\n    the input matrix and be mutually orthogonal. For column-rank-deficient matrices,\n    some methods (e.g. Gram-Schmidt variants) may include columns of norm 0.\n\n    Args:\n        A: The operator to orthogonalize.\n        method: The method to use for orthogonalization.\n        n_reortho: The number of times to _re_-orthogonalize each column.\n            Reorthogonalizing once is generally sufficient to improve orthogonality\n            for Gram-Schmidt variants\n            (see e.g. [10.1007/s00211-005-0615-4](https://doi.org/10.1007/s00211-005-0615-4)).\n\n    Returns:\n        The orthogonalized operator. If the input is a LinearOperator, then so is the output.\n    \"\"\"\n    return _orthogonalize(A, method, n_reortho)\n</code></pre>"},{"location":"reference/cagpjax/linalg/utils/","title":"utils","text":""},{"location":"reference/cagpjax/linalg/utils/#cagpjax.linalg.utils","title":"<code>cagpjax.linalg.utils</code>","text":"<p>Linear algebra utilities.</p>"},{"location":"reference/cagpjax/models/","title":"models","text":""},{"location":"reference/cagpjax/models/#cagpjax.models","title":"<code>cagpjax.models</code>","text":"<p>Gaussian process models.</p>"},{"location":"reference/cagpjax/models/#cagpjax.models.AbstractComputationAwareGP","title":"<code>AbstractComputationAwareGP</code>","text":"<p>               Bases: <code>AbstractVariationalFamily</code>, <code>ABC</code></p> <p>Abstract base class for Computation-Aware Gaussian Processes.</p> <p>While CaGPs can be viewed as exact GPs on a data subspace, when the actions are learnable, they can also be interpreted as a variational family whose variational parameters are the parameters of the actions.</p> Source code in <code>src/cagpjax/models/base.py</code> <pre><code>class AbstractComputationAwareGP(AbstractVariationalFamily, abc.ABC):\n    \"\"\"Abstract base class for Computation-Aware Gaussian Processes.\n\n    While CaGPs can be viewed as exact GPs on a data subspace, when the actions\n    are learnable, they can also be interpreted as a variational family whose\n    variational parameters are the parameters of the actions.\n    \"\"\"\n\n    ...\n</code></pre>"},{"location":"reference/cagpjax/models/#cagpjax.models.ComputationAwareGP","title":"<code>ComputationAwareGP</code>","text":"<p>               Bases: <code>AbstractComputationAwareGP</code></p> <p>Computation-aware Gaussian Process model.</p> <p>This model implements scalable GP inference by using batch linear solver policies to project the kernel and data to a lower-dimensional subspace, while accounting for the extra uncertainty imposed by observing only this subspace.</p> <p>Attributes:</p> Name Type Description <code>posterior</code> <code>ConjugatePosterior</code> <p>The original (exact) posterior.</p> <code>policy</code> <code>AbstractBatchLinearSolverPolicy</code> <p>The batch linear solver policy.</p> <code>solver_method</code> <code>AbstractLinearSolverMethod</code> <p>The linear solver method to use for solving linear systems with positive semi-definite operators.</p> Notes <ul> <li>Only single-output models are currently supported.</li> </ul> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>class ComputationAwareGP(AbstractComputationAwareGP):\n    \"\"\"Computation-aware Gaussian Process model.\n\n    This model implements scalable GP inference by using batch linear solver\n    policies to project the kernel and data to a lower-dimensional subspace, while\n    accounting for the extra uncertainty imposed by observing only this subspace.\n\n    Attributes:\n        posterior: The original (exact) posterior.\n        policy: The batch linear solver policy.\n        solver_method: The linear solver method to use for solving linear systems\n            with positive semi-definite operators.\n\n    Notes:\n        - Only single-output models are currently supported.\n    \"\"\"\n\n    posterior: ConjugatePosterior\n    policy: AbstractBatchLinearSolverPolicy\n    solver_method: AbstractLinearSolverMethod\n\n    def __init__(\n        self,\n        posterior: ConjugatePosterior,\n        policy: AbstractBatchLinearSolverPolicy,\n        solver_method: AbstractLinearSolverMethod = Cholesky(1e-6),\n    ):\n        \"\"\"Initialize the Computation-Aware GP model.\n\n        Args:\n            posterior: GPJax conjugate posterior.\n            policy: The batch linear solver policy that defines the subspace into\n                which the data is projected.\n            solver_method: The linear solver method to use for solving linear systems with\n                positive semi-definite operators.\n        \"\"\"\n        super().__init__(posterior)\n        self.policy = policy\n        self.solver_method = solver_method\n        self._posterior_params: _ProjectedPosteriorParameters | None = None\n\n    @property\n    def is_conditioned(self) -&gt; bool:\n        \"\"\"Whether the model has been conditioned on training data.\"\"\"\n        return self._posterior_params is not None\n\n    def condition(self, train_data: Dataset) -&gt; None:\n        \"\"\"Compute and store the projected quantities of the conditioned GP posterior.\n\n        Args:\n            train_data: The training data used to fit the GP.\n        \"\"\"\n        # Ensure we have supervised training data\n        if train_data.X is None or train_data.y is None:\n            raise ValueError(\"Training data must be supervised.\")\n\n        # Unpack training data\n        x = jnp.atleast_2d(train_data.X)\n        y = jnp.atleast_1d(train_data.y).squeeze()\n\n        # Unpack prior and likelihood\n        prior = self.posterior.prior\n        likelihood = self.posterior.likelihood\n\n        # Mean and covariance of prior-predictive distribution\n        mean_prior = prior.mean_function(x).squeeze()\n        # Work around GPJax promoting dtype of mean to float64 (See JaxGaussianProcesses/GPJax#523)\n        if isinstance(prior.mean_function, Constant):\n            mean_prior = mean_prior.astype(prior.mean_function.constant.value.dtype)\n        cov_xx = prior.kernel.gram(x)\n        obs_cov = diag_like(cov_xx, likelihood.obs_stddev.value**2)\n        cov_prior = cov_xx + obs_cov\n\n        # Project quantities to subspace\n        actions = self.policy.to_actions(cov_prior)\n        obs_cov_proj = congruence_transform(actions, obs_cov)\n        cov_prior_proj = congruence_transform(actions, cov_prior)\n        cov_prior_proj_solver = self.solver_method(cov_prior_proj)\n\n        residual_proj = actions.T @ (y - mean_prior)\n        repr_weights_proj = cov_prior_proj_solver.solve(residual_proj)\n\n        self._posterior_params = _ProjectedPosteriorParameters(\n            x=x,\n            actions=actions,\n            obs_cov_proj=obs_cov_proj,\n            cov_prior_proj_solver=cov_prior_proj_solver,\n            residual_proj=residual_proj,\n            repr_weights_proj=repr_weights_proj,\n        )\n\n    @override\n    def predict(\n        self, test_inputs: Float[Array, \"N D\"] | None = None\n    ) -&gt; GaussianDistribution:\n        \"\"\"Compute the predictive distribution of the GP at the test inputs.\n\n        ``condition`` must be called before this method can be used.\n\n        Args:\n            test_inputs: The test inputs at which to make predictions. If not provided,\n                predictions are made at the training inputs.\n\n        Returns:\n            GaussianDistribution: The predictive distribution of the GP at the\n                test inputs.\n        \"\"\"\n        if not self.is_conditioned:\n            raise ValueError(\"Model is not yet conditioned. Call ``condition`` first.\")\n\n        # help out pyright\n        assert self._posterior_params is not None\n\n        # Unpack posterior parameters\n        x = self._posterior_params.x\n        actions = self._posterior_params.actions\n        cov_prior_proj_solver = self._posterior_params.cov_prior_proj_solver\n        repr_weights_proj = self._posterior_params.repr_weights_proj\n\n        # Predictions at test points\n        z = test_inputs if test_inputs is not None else x\n        prior = self.posterior.prior\n        mean_z = prior.mean_function(z).squeeze()\n        # Work around GPJax promoting dtype of mean to float64 (See JaxGaussianProcesses/GPJax#523)\n        if isinstance(prior.mean_function, Constant):\n            mean_z = mean_z.astype(prior.mean_function.constant.value.dtype)\n        cov_zz = prior.kernel.gram(z)\n        cov_zx = cov_zz if test_inputs is None else prior.kernel.cross_covariance(z, x)\n        cov_zx_proj = cov_zx @ actions\n\n        # Posterior predictive distribution\n        mean_pred = jnp.atleast_1d(mean_z + cov_zx_proj @ repr_weights_proj)\n        cov_pred = cov_zz - cov_prior_proj_solver.inv_congruence_transform(\n            cov_zx_proj.T\n        )\n        cov_pred = cola.PSD(cov_pred + diag_like(cov_pred, self.posterior.jitter))\n\n        return GaussianDistribution(mean_pred, cov_pred)\n\n    def prior_kl(self) -&gt; ScalarFloat:\n        r\"\"\"Compute KL divergence between CaGP posterior and GP prior..\n\n        Calculates $\\mathrm{KL}[q(f) || p(f)]$, where $q(f)$ is the CaGP\n        posterior approximation and $p(f)$ is the GP prior.\n\n        ``condition`` must be called before this method can be used.\n\n        Returns:\n            KL divergence value (scalar).\n        \"\"\"\n        if not self.is_conditioned:\n            raise ValueError(\"Model is not yet conditioned. Call ``condition`` first.\")\n\n        # help out pyright\n        assert self._posterior_params is not None\n\n        # Unpack posterior parameters\n        obs_cov_proj = self._posterior_params.obs_cov_proj\n        cov_prior_proj_solver = self._posterior_params.cov_prior_proj_solver\n        residual_proj = self._posterior_params.residual_proj\n        repr_weights_proj = self._posterior_params.repr_weights_proj\n\n        obs_cov_proj_solver = self.solver_method(obs_cov_proj)\n\n        kl = (\n            _kl_divergence_from_solvers(\n                residual_proj,\n                obs_cov_proj_solver,\n                jnp.zeros_like(residual_proj),\n                cov_prior_proj_solver,\n            )\n            - 0.5 * congruence_transform(repr_weights_proj.T, obs_cov_proj).squeeze()\n        )\n\n        return kl\n</code></pre>"},{"location":"reference/cagpjax/models/#cagpjax.models.ComputationAwareGP.is_conditioned","title":"<code>is_conditioned</code>  <code>property</code>","text":"<p>Whether the model has been conditioned on training data.</p>"},{"location":"reference/cagpjax/models/#cagpjax.models.ComputationAwareGP.__init__","title":"<code>__init__(posterior, policy, solver_method=Cholesky(1e-06))</code>","text":"<p>Initialize the Computation-Aware GP model.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>ConjugatePosterior</code> <p>GPJax conjugate posterior.</p> required <code>policy</code> <code>AbstractBatchLinearSolverPolicy</code> <p>The batch linear solver policy that defines the subspace into which the data is projected.</p> required <code>solver_method</code> <code>AbstractLinearSolverMethod</code> <p>The linear solver method to use for solving linear systems with positive semi-definite operators.</p> <code>Cholesky(1e-06)</code> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def __init__(\n    self,\n    posterior: ConjugatePosterior,\n    policy: AbstractBatchLinearSolverPolicy,\n    solver_method: AbstractLinearSolverMethod = Cholesky(1e-6),\n):\n    \"\"\"Initialize the Computation-Aware GP model.\n\n    Args:\n        posterior: GPJax conjugate posterior.\n        policy: The batch linear solver policy that defines the subspace into\n            which the data is projected.\n        solver_method: The linear solver method to use for solving linear systems with\n            positive semi-definite operators.\n    \"\"\"\n    super().__init__(posterior)\n    self.policy = policy\n    self.solver_method = solver_method\n    self._posterior_params: _ProjectedPosteriorParameters | None = None\n</code></pre>"},{"location":"reference/cagpjax/models/#cagpjax.models.ComputationAwareGP.condition","title":"<code>condition(train_data)</code>","text":"<p>Compute and store the projected quantities of the conditioned GP posterior.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>Dataset</code> <p>The training data used to fit the GP.</p> required Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def condition(self, train_data: Dataset) -&gt; None:\n    \"\"\"Compute and store the projected quantities of the conditioned GP posterior.\n\n    Args:\n        train_data: The training data used to fit the GP.\n    \"\"\"\n    # Ensure we have supervised training data\n    if train_data.X is None or train_data.y is None:\n        raise ValueError(\"Training data must be supervised.\")\n\n    # Unpack training data\n    x = jnp.atleast_2d(train_data.X)\n    y = jnp.atleast_1d(train_data.y).squeeze()\n\n    # Unpack prior and likelihood\n    prior = self.posterior.prior\n    likelihood = self.posterior.likelihood\n\n    # Mean and covariance of prior-predictive distribution\n    mean_prior = prior.mean_function(x).squeeze()\n    # Work around GPJax promoting dtype of mean to float64 (See JaxGaussianProcesses/GPJax#523)\n    if isinstance(prior.mean_function, Constant):\n        mean_prior = mean_prior.astype(prior.mean_function.constant.value.dtype)\n    cov_xx = prior.kernel.gram(x)\n    obs_cov = diag_like(cov_xx, likelihood.obs_stddev.value**2)\n    cov_prior = cov_xx + obs_cov\n\n    # Project quantities to subspace\n    actions = self.policy.to_actions(cov_prior)\n    obs_cov_proj = congruence_transform(actions, obs_cov)\n    cov_prior_proj = congruence_transform(actions, cov_prior)\n    cov_prior_proj_solver = self.solver_method(cov_prior_proj)\n\n    residual_proj = actions.T @ (y - mean_prior)\n    repr_weights_proj = cov_prior_proj_solver.solve(residual_proj)\n\n    self._posterior_params = _ProjectedPosteriorParameters(\n        x=x,\n        actions=actions,\n        obs_cov_proj=obs_cov_proj,\n        cov_prior_proj_solver=cov_prior_proj_solver,\n        residual_proj=residual_proj,\n        repr_weights_proj=repr_weights_proj,\n    )\n</code></pre>"},{"location":"reference/cagpjax/models/#cagpjax.models.ComputationAwareGP.predict","title":"<code>predict(test_inputs=None)</code>","text":"<p>Compute the predictive distribution of the GP at the test inputs.</p> <p><code>condition</code> must be called before this method can be used.</p> <p>Parameters:</p> Name Type Description Default <code>test_inputs</code> <code>Float[Array, 'N D'] | None</code> <p>The test inputs at which to make predictions. If not provided, predictions are made at the training inputs.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GaussianDistribution</code> <code>GaussianDistribution</code> <p>The predictive distribution of the GP at the test inputs.</p> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>@override\ndef predict(\n    self, test_inputs: Float[Array, \"N D\"] | None = None\n) -&gt; GaussianDistribution:\n    \"\"\"Compute the predictive distribution of the GP at the test inputs.\n\n    ``condition`` must be called before this method can be used.\n\n    Args:\n        test_inputs: The test inputs at which to make predictions. If not provided,\n            predictions are made at the training inputs.\n\n    Returns:\n        GaussianDistribution: The predictive distribution of the GP at the\n            test inputs.\n    \"\"\"\n    if not self.is_conditioned:\n        raise ValueError(\"Model is not yet conditioned. Call ``condition`` first.\")\n\n    # help out pyright\n    assert self._posterior_params is not None\n\n    # Unpack posterior parameters\n    x = self._posterior_params.x\n    actions = self._posterior_params.actions\n    cov_prior_proj_solver = self._posterior_params.cov_prior_proj_solver\n    repr_weights_proj = self._posterior_params.repr_weights_proj\n\n    # Predictions at test points\n    z = test_inputs if test_inputs is not None else x\n    prior = self.posterior.prior\n    mean_z = prior.mean_function(z).squeeze()\n    # Work around GPJax promoting dtype of mean to float64 (See JaxGaussianProcesses/GPJax#523)\n    if isinstance(prior.mean_function, Constant):\n        mean_z = mean_z.astype(prior.mean_function.constant.value.dtype)\n    cov_zz = prior.kernel.gram(z)\n    cov_zx = cov_zz if test_inputs is None else prior.kernel.cross_covariance(z, x)\n    cov_zx_proj = cov_zx @ actions\n\n    # Posterior predictive distribution\n    mean_pred = jnp.atleast_1d(mean_z + cov_zx_proj @ repr_weights_proj)\n    cov_pred = cov_zz - cov_prior_proj_solver.inv_congruence_transform(\n        cov_zx_proj.T\n    )\n    cov_pred = cola.PSD(cov_pred + diag_like(cov_pred, self.posterior.jitter))\n\n    return GaussianDistribution(mean_pred, cov_pred)\n</code></pre>"},{"location":"reference/cagpjax/models/#cagpjax.models.ComputationAwareGP.prior_kl","title":"<code>prior_kl()</code>","text":"<p>Compute KL divergence between CaGP posterior and GP prior..</p> <p>Calculates \\(\\mathrm{KL}[q(f) || p(f)]\\), where \\(q(f)\\) is the CaGP posterior approximation and \\(p(f)\\) is the GP prior.</p> <p><code>condition</code> must be called before this method can be used.</p> <p>Returns:</p> Type Description <code>ScalarFloat</code> <p>KL divergence value (scalar).</p> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def prior_kl(self) -&gt; ScalarFloat:\n    r\"\"\"Compute KL divergence between CaGP posterior and GP prior..\n\n    Calculates $\\mathrm{KL}[q(f) || p(f)]$, where $q(f)$ is the CaGP\n    posterior approximation and $p(f)$ is the GP prior.\n\n    ``condition`` must be called before this method can be used.\n\n    Returns:\n        KL divergence value (scalar).\n    \"\"\"\n    if not self.is_conditioned:\n        raise ValueError(\"Model is not yet conditioned. Call ``condition`` first.\")\n\n    # help out pyright\n    assert self._posterior_params is not None\n\n    # Unpack posterior parameters\n    obs_cov_proj = self._posterior_params.obs_cov_proj\n    cov_prior_proj_solver = self._posterior_params.cov_prior_proj_solver\n    residual_proj = self._posterior_params.residual_proj\n    repr_weights_proj = self._posterior_params.repr_weights_proj\n\n    obs_cov_proj_solver = self.solver_method(obs_cov_proj)\n\n    kl = (\n        _kl_divergence_from_solvers(\n            residual_proj,\n            obs_cov_proj_solver,\n            jnp.zeros_like(residual_proj),\n            cov_prior_proj_solver,\n        )\n        - 0.5 * congruence_transform(repr_weights_proj.T, obs_cov_proj).squeeze()\n    )\n\n    return kl\n</code></pre>"},{"location":"reference/cagpjax/models/base/","title":"base","text":""},{"location":"reference/cagpjax/models/base/#cagpjax.models.base","title":"<code>cagpjax.models.base</code>","text":"<p>Abstract base classes for models.</p>"},{"location":"reference/cagpjax/models/base/#cagpjax.models.base.AbstractComputationAwareGP","title":"<code>AbstractComputationAwareGP</code>","text":"<p>               Bases: <code>AbstractVariationalFamily</code>, <code>ABC</code></p> <p>Abstract base class for Computation-Aware Gaussian Processes.</p> <p>While CaGPs can be viewed as exact GPs on a data subspace, when the actions are learnable, they can also be interpreted as a variational family whose variational parameters are the parameters of the actions.</p> Source code in <code>src/cagpjax/models/base.py</code> <pre><code>class AbstractComputationAwareGP(AbstractVariationalFamily, abc.ABC):\n    \"\"\"Abstract base class for Computation-Aware Gaussian Processes.\n\n    While CaGPs can be viewed as exact GPs on a data subspace, when the actions\n    are learnable, they can also be interpreted as a variational family whose\n    variational parameters are the parameters of the actions.\n    \"\"\"\n\n    ...\n</code></pre>"},{"location":"reference/cagpjax/models/cagp/","title":"cagp","text":""},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp","title":"<code>cagpjax.models.cagp</code>","text":"<p>Computation-aware Gaussian Process models.</p>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGP","title":"<code>ComputationAwareGP</code>","text":"<p>               Bases: <code>AbstractComputationAwareGP</code></p> <p>Computation-aware Gaussian Process model.</p> <p>This model implements scalable GP inference by using batch linear solver policies to project the kernel and data to a lower-dimensional subspace, while accounting for the extra uncertainty imposed by observing only this subspace.</p> <p>Attributes:</p> Name Type Description <code>posterior</code> <code>ConjugatePosterior</code> <p>The original (exact) posterior.</p> <code>policy</code> <code>AbstractBatchLinearSolverPolicy</code> <p>The batch linear solver policy.</p> <code>solver_method</code> <code>AbstractLinearSolverMethod</code> <p>The linear solver method to use for solving linear systems with positive semi-definite operators.</p> Notes <ul> <li>Only single-output models are currently supported.</li> </ul> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>class ComputationAwareGP(AbstractComputationAwareGP):\n    \"\"\"Computation-aware Gaussian Process model.\n\n    This model implements scalable GP inference by using batch linear solver\n    policies to project the kernel and data to a lower-dimensional subspace, while\n    accounting for the extra uncertainty imposed by observing only this subspace.\n\n    Attributes:\n        posterior: The original (exact) posterior.\n        policy: The batch linear solver policy.\n        solver_method: The linear solver method to use for solving linear systems\n            with positive semi-definite operators.\n\n    Notes:\n        - Only single-output models are currently supported.\n    \"\"\"\n\n    posterior: ConjugatePosterior\n    policy: AbstractBatchLinearSolverPolicy\n    solver_method: AbstractLinearSolverMethod\n\n    def __init__(\n        self,\n        posterior: ConjugatePosterior,\n        policy: AbstractBatchLinearSolverPolicy,\n        solver_method: AbstractLinearSolverMethod = Cholesky(1e-6),\n    ):\n        \"\"\"Initialize the Computation-Aware GP model.\n\n        Args:\n            posterior: GPJax conjugate posterior.\n            policy: The batch linear solver policy that defines the subspace into\n                which the data is projected.\n            solver_method: The linear solver method to use for solving linear systems with\n                positive semi-definite operators.\n        \"\"\"\n        super().__init__(posterior)\n        self.policy = policy\n        self.solver_method = solver_method\n        self._posterior_params: _ProjectedPosteriorParameters | None = None\n\n    @property\n    def is_conditioned(self) -&gt; bool:\n        \"\"\"Whether the model has been conditioned on training data.\"\"\"\n        return self._posterior_params is not None\n\n    def condition(self, train_data: Dataset) -&gt; None:\n        \"\"\"Compute and store the projected quantities of the conditioned GP posterior.\n\n        Args:\n            train_data: The training data used to fit the GP.\n        \"\"\"\n        # Ensure we have supervised training data\n        if train_data.X is None or train_data.y is None:\n            raise ValueError(\"Training data must be supervised.\")\n\n        # Unpack training data\n        x = jnp.atleast_2d(train_data.X)\n        y = jnp.atleast_1d(train_data.y).squeeze()\n\n        # Unpack prior and likelihood\n        prior = self.posterior.prior\n        likelihood = self.posterior.likelihood\n\n        # Mean and covariance of prior-predictive distribution\n        mean_prior = prior.mean_function(x).squeeze()\n        # Work around GPJax promoting dtype of mean to float64 (See JaxGaussianProcesses/GPJax#523)\n        if isinstance(prior.mean_function, Constant):\n            mean_prior = mean_prior.astype(prior.mean_function.constant.value.dtype)\n        cov_xx = prior.kernel.gram(x)\n        obs_cov = diag_like(cov_xx, likelihood.obs_stddev.value**2)\n        cov_prior = cov_xx + obs_cov\n\n        # Project quantities to subspace\n        actions = self.policy.to_actions(cov_prior)\n        obs_cov_proj = congruence_transform(actions, obs_cov)\n        cov_prior_proj = congruence_transform(actions, cov_prior)\n        cov_prior_proj_solver = self.solver_method(cov_prior_proj)\n\n        residual_proj = actions.T @ (y - mean_prior)\n        repr_weights_proj = cov_prior_proj_solver.solve(residual_proj)\n\n        self._posterior_params = _ProjectedPosteriorParameters(\n            x=x,\n            actions=actions,\n            obs_cov_proj=obs_cov_proj,\n            cov_prior_proj_solver=cov_prior_proj_solver,\n            residual_proj=residual_proj,\n            repr_weights_proj=repr_weights_proj,\n        )\n\n    @override\n    def predict(\n        self, test_inputs: Float[Array, \"N D\"] | None = None\n    ) -&gt; GaussianDistribution:\n        \"\"\"Compute the predictive distribution of the GP at the test inputs.\n\n        ``condition`` must be called before this method can be used.\n\n        Args:\n            test_inputs: The test inputs at which to make predictions. If not provided,\n                predictions are made at the training inputs.\n\n        Returns:\n            GaussianDistribution: The predictive distribution of the GP at the\n                test inputs.\n        \"\"\"\n        if not self.is_conditioned:\n            raise ValueError(\"Model is not yet conditioned. Call ``condition`` first.\")\n\n        # help out pyright\n        assert self._posterior_params is not None\n\n        # Unpack posterior parameters\n        x = self._posterior_params.x\n        actions = self._posterior_params.actions\n        cov_prior_proj_solver = self._posterior_params.cov_prior_proj_solver\n        repr_weights_proj = self._posterior_params.repr_weights_proj\n\n        # Predictions at test points\n        z = test_inputs if test_inputs is not None else x\n        prior = self.posterior.prior\n        mean_z = prior.mean_function(z).squeeze()\n        # Work around GPJax promoting dtype of mean to float64 (See JaxGaussianProcesses/GPJax#523)\n        if isinstance(prior.mean_function, Constant):\n            mean_z = mean_z.astype(prior.mean_function.constant.value.dtype)\n        cov_zz = prior.kernel.gram(z)\n        cov_zx = cov_zz if test_inputs is None else prior.kernel.cross_covariance(z, x)\n        cov_zx_proj = cov_zx @ actions\n\n        # Posterior predictive distribution\n        mean_pred = jnp.atleast_1d(mean_z + cov_zx_proj @ repr_weights_proj)\n        cov_pred = cov_zz - cov_prior_proj_solver.inv_congruence_transform(\n            cov_zx_proj.T\n        )\n        cov_pred = cola.PSD(cov_pred + diag_like(cov_pred, self.posterior.jitter))\n\n        return GaussianDistribution(mean_pred, cov_pred)\n\n    def prior_kl(self) -&gt; ScalarFloat:\n        r\"\"\"Compute KL divergence between CaGP posterior and GP prior..\n\n        Calculates $\\mathrm{KL}[q(f) || p(f)]$, where $q(f)$ is the CaGP\n        posterior approximation and $p(f)$ is the GP prior.\n\n        ``condition`` must be called before this method can be used.\n\n        Returns:\n            KL divergence value (scalar).\n        \"\"\"\n        if not self.is_conditioned:\n            raise ValueError(\"Model is not yet conditioned. Call ``condition`` first.\")\n\n        # help out pyright\n        assert self._posterior_params is not None\n\n        # Unpack posterior parameters\n        obs_cov_proj = self._posterior_params.obs_cov_proj\n        cov_prior_proj_solver = self._posterior_params.cov_prior_proj_solver\n        residual_proj = self._posterior_params.residual_proj\n        repr_weights_proj = self._posterior_params.repr_weights_proj\n\n        obs_cov_proj_solver = self.solver_method(obs_cov_proj)\n\n        kl = (\n            _kl_divergence_from_solvers(\n                residual_proj,\n                obs_cov_proj_solver,\n                jnp.zeros_like(residual_proj),\n                cov_prior_proj_solver,\n            )\n            - 0.5 * congruence_transform(repr_weights_proj.T, obs_cov_proj).squeeze()\n        )\n\n        return kl\n</code></pre>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGP.is_conditioned","title":"<code>is_conditioned</code>  <code>property</code>","text":"<p>Whether the model has been conditioned on training data.</p>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGP.__init__","title":"<code>__init__(posterior, policy, solver_method=Cholesky(1e-06))</code>","text":"<p>Initialize the Computation-Aware GP model.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>ConjugatePosterior</code> <p>GPJax conjugate posterior.</p> required <code>policy</code> <code>AbstractBatchLinearSolverPolicy</code> <p>The batch linear solver policy that defines the subspace into which the data is projected.</p> required <code>solver_method</code> <code>AbstractLinearSolverMethod</code> <p>The linear solver method to use for solving linear systems with positive semi-definite operators.</p> <code>Cholesky(1e-06)</code> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def __init__(\n    self,\n    posterior: ConjugatePosterior,\n    policy: AbstractBatchLinearSolverPolicy,\n    solver_method: AbstractLinearSolverMethod = Cholesky(1e-6),\n):\n    \"\"\"Initialize the Computation-Aware GP model.\n\n    Args:\n        posterior: GPJax conjugate posterior.\n        policy: The batch linear solver policy that defines the subspace into\n            which the data is projected.\n        solver_method: The linear solver method to use for solving linear systems with\n            positive semi-definite operators.\n    \"\"\"\n    super().__init__(posterior)\n    self.policy = policy\n    self.solver_method = solver_method\n    self._posterior_params: _ProjectedPosteriorParameters | None = None\n</code></pre>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGP.condition","title":"<code>condition(train_data)</code>","text":"<p>Compute and store the projected quantities of the conditioned GP posterior.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>Dataset</code> <p>The training data used to fit the GP.</p> required Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def condition(self, train_data: Dataset) -&gt; None:\n    \"\"\"Compute and store the projected quantities of the conditioned GP posterior.\n\n    Args:\n        train_data: The training data used to fit the GP.\n    \"\"\"\n    # Ensure we have supervised training data\n    if train_data.X is None or train_data.y is None:\n        raise ValueError(\"Training data must be supervised.\")\n\n    # Unpack training data\n    x = jnp.atleast_2d(train_data.X)\n    y = jnp.atleast_1d(train_data.y).squeeze()\n\n    # Unpack prior and likelihood\n    prior = self.posterior.prior\n    likelihood = self.posterior.likelihood\n\n    # Mean and covariance of prior-predictive distribution\n    mean_prior = prior.mean_function(x).squeeze()\n    # Work around GPJax promoting dtype of mean to float64 (See JaxGaussianProcesses/GPJax#523)\n    if isinstance(prior.mean_function, Constant):\n        mean_prior = mean_prior.astype(prior.mean_function.constant.value.dtype)\n    cov_xx = prior.kernel.gram(x)\n    obs_cov = diag_like(cov_xx, likelihood.obs_stddev.value**2)\n    cov_prior = cov_xx + obs_cov\n\n    # Project quantities to subspace\n    actions = self.policy.to_actions(cov_prior)\n    obs_cov_proj = congruence_transform(actions, obs_cov)\n    cov_prior_proj = congruence_transform(actions, cov_prior)\n    cov_prior_proj_solver = self.solver_method(cov_prior_proj)\n\n    residual_proj = actions.T @ (y - mean_prior)\n    repr_weights_proj = cov_prior_proj_solver.solve(residual_proj)\n\n    self._posterior_params = _ProjectedPosteriorParameters(\n        x=x,\n        actions=actions,\n        obs_cov_proj=obs_cov_proj,\n        cov_prior_proj_solver=cov_prior_proj_solver,\n        residual_proj=residual_proj,\n        repr_weights_proj=repr_weights_proj,\n    )\n</code></pre>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGP.predict","title":"<code>predict(test_inputs=None)</code>","text":"<p>Compute the predictive distribution of the GP at the test inputs.</p> <p><code>condition</code> must be called before this method can be used.</p> <p>Parameters:</p> Name Type Description Default <code>test_inputs</code> <code>Float[Array, 'N D'] | None</code> <p>The test inputs at which to make predictions. If not provided, predictions are made at the training inputs.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GaussianDistribution</code> <code>GaussianDistribution</code> <p>The predictive distribution of the GP at the test inputs.</p> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>@override\ndef predict(\n    self, test_inputs: Float[Array, \"N D\"] | None = None\n) -&gt; GaussianDistribution:\n    \"\"\"Compute the predictive distribution of the GP at the test inputs.\n\n    ``condition`` must be called before this method can be used.\n\n    Args:\n        test_inputs: The test inputs at which to make predictions. If not provided,\n            predictions are made at the training inputs.\n\n    Returns:\n        GaussianDistribution: The predictive distribution of the GP at the\n            test inputs.\n    \"\"\"\n    if not self.is_conditioned:\n        raise ValueError(\"Model is not yet conditioned. Call ``condition`` first.\")\n\n    # help out pyright\n    assert self._posterior_params is not None\n\n    # Unpack posterior parameters\n    x = self._posterior_params.x\n    actions = self._posterior_params.actions\n    cov_prior_proj_solver = self._posterior_params.cov_prior_proj_solver\n    repr_weights_proj = self._posterior_params.repr_weights_proj\n\n    # Predictions at test points\n    z = test_inputs if test_inputs is not None else x\n    prior = self.posterior.prior\n    mean_z = prior.mean_function(z).squeeze()\n    # Work around GPJax promoting dtype of mean to float64 (See JaxGaussianProcesses/GPJax#523)\n    if isinstance(prior.mean_function, Constant):\n        mean_z = mean_z.astype(prior.mean_function.constant.value.dtype)\n    cov_zz = prior.kernel.gram(z)\n    cov_zx = cov_zz if test_inputs is None else prior.kernel.cross_covariance(z, x)\n    cov_zx_proj = cov_zx @ actions\n\n    # Posterior predictive distribution\n    mean_pred = jnp.atleast_1d(mean_z + cov_zx_proj @ repr_weights_proj)\n    cov_pred = cov_zz - cov_prior_proj_solver.inv_congruence_transform(\n        cov_zx_proj.T\n    )\n    cov_pred = cola.PSD(cov_pred + diag_like(cov_pred, self.posterior.jitter))\n\n    return GaussianDistribution(mean_pred, cov_pred)\n</code></pre>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGP.prior_kl","title":"<code>prior_kl()</code>","text":"<p>Compute KL divergence between CaGP posterior and GP prior..</p> <p>Calculates \\(\\mathrm{KL}[q(f) || p(f)]\\), where \\(q(f)\\) is the CaGP posterior approximation and \\(p(f)\\) is the GP prior.</p> <p><code>condition</code> must be called before this method can be used.</p> <p>Returns:</p> Type Description <code>ScalarFloat</code> <p>KL divergence value (scalar).</p> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def prior_kl(self) -&gt; ScalarFloat:\n    r\"\"\"Compute KL divergence between CaGP posterior and GP prior..\n\n    Calculates $\\mathrm{KL}[q(f) || p(f)]$, where $q(f)$ is the CaGP\n    posterior approximation and $p(f)$ is the GP prior.\n\n    ``condition`` must be called before this method can be used.\n\n    Returns:\n        KL divergence value (scalar).\n    \"\"\"\n    if not self.is_conditioned:\n        raise ValueError(\"Model is not yet conditioned. Call ``condition`` first.\")\n\n    # help out pyright\n    assert self._posterior_params is not None\n\n    # Unpack posterior parameters\n    obs_cov_proj = self._posterior_params.obs_cov_proj\n    cov_prior_proj_solver = self._posterior_params.cov_prior_proj_solver\n    residual_proj = self._posterior_params.residual_proj\n    repr_weights_proj = self._posterior_params.repr_weights_proj\n\n    obs_cov_proj_solver = self.solver_method(obs_cov_proj)\n\n    kl = (\n        _kl_divergence_from_solvers(\n            residual_proj,\n            obs_cov_proj_solver,\n            jnp.zeros_like(residual_proj),\n            cov_prior_proj_solver,\n        )\n        - 0.5 * congruence_transform(repr_weights_proj.T, obs_cov_proj).squeeze()\n    )\n\n    return kl\n</code></pre>"},{"location":"reference/cagpjax/operators/","title":"operators","text":""},{"location":"reference/cagpjax/operators/#cagpjax.operators","title":"<code>cagpjax.operators</code>","text":"<p>Custom linear operators.</p>"},{"location":"reference/cagpjax/operators/#cagpjax.operators.BlockDiagonalSparse","title":"<code>BlockDiagonalSparse</code>","text":"<p>               Bases: <code>LinearOperator</code></p> <p>Block-diagonal sparse linear operator.</p> <p>This operator represents a block-diagonal matrix structure where the blocks are contiguous, and each contains a column vector, so that exactly one value is non-zero in each row.</p> <p>Parameters:</p> Name Type Description Default <code>nz_values</code> <code>Float[Array, N]</code> <p>Non-zero values to be distributed across diagonal blocks.</p> required <code>n_blocks</code> <code>int</code> <p>Number of diagonal blocks in the matrix.</p> required"},{"location":"reference/cagpjax/operators/#cagpjax.operators.BlockDiagonalSparse--examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; from cagpjax.operators import BlockDiagonalSparse\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create a 3x6 block-diagonal matrix with 3 blocks\n&gt;&gt;&gt; nz_values = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n&gt;&gt;&gt; op = BlockDiagonalSparse(nz_values, n_blocks=3)\n&gt;&gt;&gt; print(op.shape)\n(6, 3)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Apply to identity matrices\n&gt;&gt;&gt; op @ jnp.eye(3)\nArray([[1., 0., 0.],\n       [2., 0., 0.],\n       [0., 3., 0.],\n       [0., 4., 0.],\n       [0., 0., 5.],\n       [0., 0., 6.]], dtype=float32)\n</code></pre> Source code in <code>src/cagpjax/operators/block_diagonal_sparse.py</code> <pre><code>class BlockDiagonalSparse(LinearOperator):\n    \"\"\"Block-diagonal sparse linear operator.\n\n    This operator represents a block-diagonal matrix structure where the blocks are contiguous, and\n    each contains a column vector, so that exactly one value is non-zero in each row.\n\n    Args:\n        nz_values: Non-zero values to be distributed across diagonal blocks.\n        n_blocks: Number of diagonal blocks in the matrix.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; import jax.numpy as jnp\n    &gt;&gt;&gt; from cagpjax.operators import BlockDiagonalSparse\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Create a 3x6 block-diagonal matrix with 3 blocks\n    &gt;&gt;&gt; nz_values = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n    &gt;&gt;&gt; op = BlockDiagonalSparse(nz_values, n_blocks=3)\n    &gt;&gt;&gt; print(op.shape)\n    (6, 3)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Apply to identity matrices\n    &gt;&gt;&gt; op @ jnp.eye(3)\n    Array([[1., 0., 0.],\n           [2., 0., 0.],\n           [0., 3., 0.],\n           [0., 4., 0.],\n           [0., 0., 5.],\n           [0., 0., 6.]], dtype=float32)\n    ```\n    \"\"\"\n\n    def __init__(self, nz_values: Float[Array, \"N\"], n_blocks: int):\n        n = nz_values.shape[0]\n        super().__init__(nz_values.dtype, (n, n_blocks), annotations={ScaledOrthogonal})\n        self.nz_values = nz_values\n\n    def _matmat(self, X: Float[Array, \"K M\"]) -&gt; Float[Array, \"N M\"]:\n        n, n_blocks = self.shape\n        block_size = n // n_blocks\n        n_blocks_main = n_blocks if n % n_blocks == 0 else n_blocks - 1\n        n_main = n_blocks_main * block_size\n        m = X.shape[1]\n\n        # block-wise multiplication for main blocks\n        blocks_main = self.nz_values[:n_main].reshape(n_blocks_main, block_size)\n        X_main = X[:n_blocks_main, :]\n        res_main = (blocks_main[..., None] * X_main[:, None, :]).reshape(n_main, m)\n\n        # handle overhang if any\n        if n &gt; n_main:\n            n_overhang = n - n_main\n            X_overhang = X[n_blocks_main, :]\n            block_overhang = self.nz_values[n_main:]\n            res_overhang = jnp.outer(block_overhang, X_overhang).reshape(n_overhang, m)\n            res = jnp.concatenate([res_main, res_overhang], axis=0)\n        else:\n            res = res_main\n\n        return res\n\n    def _rmatmat(self, X: Float[Array, \"M N\"]) -&gt; Float[Array, \"M K\"]:\n        # figure out size of main blocks\n        n, n_blocks = self.shape\n        block_size = n // n_blocks\n        n_blocks_main = n_blocks if n % n_blocks == 0 else n_blocks - 1\n        n_main = n_blocks_main * block_size\n        m = X.shape[0]\n\n        # block-wise multiplication for main blocks\n        blocks_main = self.nz_values[:n_main].reshape(n_blocks_main, block_size)\n        X_main = X[:, :n_main].reshape(m, n_blocks_main, block_size)\n        res_main = jnp.einsum(\"ik,jik-&gt;ji\", blocks_main, X_main)\n\n        # handle overhang if any\n        if n &gt; n_main:\n            n_overhang = n - n_main\n            X_overhang = X[:, n_main:].reshape(m, n_overhang)\n            block_overhang = self.nz_values[n_main:]\n            res_overhang = (X_overhang @ block_overhang)[:, None]\n            res = jnp.concatenate([res_main, res_overhang], axis=1)\n        else:\n            res = res_main\n\n        return res\n</code></pre>"},{"location":"reference/cagpjax/operators/annotations/","title":"annotations","text":""},{"location":"reference/cagpjax/operators/annotations/#cagpjax.operators.annotations","title":"<code>cagpjax.operators.annotations</code>","text":"<p>Annotations for operators.</p>"},{"location":"reference/cagpjax/operators/annotations/#cagpjax.operators.annotations.ScaledOrthogonal","title":"<code>ScaledOrthogonal</code>","text":"<p>               Bases: <code>Annotation</code></p> <p>Annotation for an operator whose columns are orthogonal (but not necessarily orthonormal).</p> Source code in <code>src/cagpjax/operators/annotations.py</code> <pre><code>class ScaledOrthogonal(cola.annotations.Annotation):\n    \"\"\"Annotation for an operator whose columns are orthogonal (but not necessarily orthonormal).\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/cagpjax/operators/block_diagonal_sparse/","title":"block_diagonal_sparse","text":""},{"location":"reference/cagpjax/operators/block_diagonal_sparse/#cagpjax.operators.block_diagonal_sparse","title":"<code>cagpjax.operators.block_diagonal_sparse</code>","text":"<p>Block-diagonal sparse linear operator.</p>"},{"location":"reference/cagpjax/operators/block_diagonal_sparse/#cagpjax.operators.block_diagonal_sparse.BlockDiagonalSparse","title":"<code>BlockDiagonalSparse</code>","text":"<p>               Bases: <code>LinearOperator</code></p> <p>Block-diagonal sparse linear operator.</p> <p>This operator represents a block-diagonal matrix structure where the blocks are contiguous, and each contains a column vector, so that exactly one value is non-zero in each row.</p> <p>Parameters:</p> Name Type Description Default <code>nz_values</code> <code>Float[Array, N]</code> <p>Non-zero values to be distributed across diagonal blocks.</p> required <code>n_blocks</code> <code>int</code> <p>Number of diagonal blocks in the matrix.</p> required"},{"location":"reference/cagpjax/operators/block_diagonal_sparse/#cagpjax.operators.block_diagonal_sparse.BlockDiagonalSparse--examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; from cagpjax.operators import BlockDiagonalSparse\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create a 3x6 block-diagonal matrix with 3 blocks\n&gt;&gt;&gt; nz_values = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n&gt;&gt;&gt; op = BlockDiagonalSparse(nz_values, n_blocks=3)\n&gt;&gt;&gt; print(op.shape)\n(6, 3)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Apply to identity matrices\n&gt;&gt;&gt; op @ jnp.eye(3)\nArray([[1., 0., 0.],\n       [2., 0., 0.],\n       [0., 3., 0.],\n       [0., 4., 0.],\n       [0., 0., 5.],\n       [0., 0., 6.]], dtype=float32)\n</code></pre> Source code in <code>src/cagpjax/operators/block_diagonal_sparse.py</code> <pre><code>class BlockDiagonalSparse(LinearOperator):\n    \"\"\"Block-diagonal sparse linear operator.\n\n    This operator represents a block-diagonal matrix structure where the blocks are contiguous, and\n    each contains a column vector, so that exactly one value is non-zero in each row.\n\n    Args:\n        nz_values: Non-zero values to be distributed across diagonal blocks.\n        n_blocks: Number of diagonal blocks in the matrix.\n\n    Examples\n    --------\n    ```python\n    &gt;&gt;&gt; import jax.numpy as jnp\n    &gt;&gt;&gt; from cagpjax.operators import BlockDiagonalSparse\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Create a 3x6 block-diagonal matrix with 3 blocks\n    &gt;&gt;&gt; nz_values = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n    &gt;&gt;&gt; op = BlockDiagonalSparse(nz_values, n_blocks=3)\n    &gt;&gt;&gt; print(op.shape)\n    (6, 3)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Apply to identity matrices\n    &gt;&gt;&gt; op @ jnp.eye(3)\n    Array([[1., 0., 0.],\n           [2., 0., 0.],\n           [0., 3., 0.],\n           [0., 4., 0.],\n           [0., 0., 5.],\n           [0., 0., 6.]], dtype=float32)\n    ```\n    \"\"\"\n\n    def __init__(self, nz_values: Float[Array, \"N\"], n_blocks: int):\n        n = nz_values.shape[0]\n        super().__init__(nz_values.dtype, (n, n_blocks), annotations={ScaledOrthogonal})\n        self.nz_values = nz_values\n\n    def _matmat(self, X: Float[Array, \"K M\"]) -&gt; Float[Array, \"N M\"]:\n        n, n_blocks = self.shape\n        block_size = n // n_blocks\n        n_blocks_main = n_blocks if n % n_blocks == 0 else n_blocks - 1\n        n_main = n_blocks_main * block_size\n        m = X.shape[1]\n\n        # block-wise multiplication for main blocks\n        blocks_main = self.nz_values[:n_main].reshape(n_blocks_main, block_size)\n        X_main = X[:n_blocks_main, :]\n        res_main = (blocks_main[..., None] * X_main[:, None, :]).reshape(n_main, m)\n\n        # handle overhang if any\n        if n &gt; n_main:\n            n_overhang = n - n_main\n            X_overhang = X[n_blocks_main, :]\n            block_overhang = self.nz_values[n_main:]\n            res_overhang = jnp.outer(block_overhang, X_overhang).reshape(n_overhang, m)\n            res = jnp.concatenate([res_main, res_overhang], axis=0)\n        else:\n            res = res_main\n\n        return res\n\n    def _rmatmat(self, X: Float[Array, \"M N\"]) -&gt; Float[Array, \"M K\"]:\n        # figure out size of main blocks\n        n, n_blocks = self.shape\n        block_size = n // n_blocks\n        n_blocks_main = n_blocks if n % n_blocks == 0 else n_blocks - 1\n        n_main = n_blocks_main * block_size\n        m = X.shape[0]\n\n        # block-wise multiplication for main blocks\n        blocks_main = self.nz_values[:n_main].reshape(n_blocks_main, block_size)\n        X_main = X[:, :n_main].reshape(m, n_blocks_main, block_size)\n        res_main = jnp.einsum(\"ik,jik-&gt;ji\", blocks_main, X_main)\n\n        # handle overhang if any\n        if n &gt; n_main:\n            n_overhang = n - n_main\n            X_overhang = X[:, n_main:].reshape(m, n_overhang)\n            block_overhang = self.nz_values[n_main:]\n            res_overhang = (X_overhang @ block_overhang)[:, None]\n            res = jnp.concatenate([res_main, res_overhang], axis=1)\n        else:\n            res = res_main\n\n        return res\n</code></pre>"},{"location":"reference/cagpjax/operators/diag_like/","title":"diag_like","text":""},{"location":"reference/cagpjax/operators/diag_like/#cagpjax.operators.diag_like","title":"<code>cagpjax.operators.diag_like</code>","text":""},{"location":"reference/cagpjax/operators/diag_like/#cagpjax.operators.diag_like.diag_like","title":"<code>diag_like(operator, values)</code>","text":"<p>Create a diagonal operator with the same shape, dtype, and device as the operator.</p> <p>Parameters:</p> Name Type Description Default <code>operator</code> <code>LinearOperator</code> <p>Linear operator.</p> required <code>values</code> <code>ScalarFloat | Float[Array, N]</code> <p>Scalar for a scalar matrix or array of diagonal values for a diagonal matrix.</p> required <p>Returns:</p> Type Description <code>Diagonal | ScalarMul</code> <p>Diagonal or scalar operator.</p> Source code in <code>src/cagpjax/operators/diag_like.py</code> <pre><code>def diag_like(\n    operator: LinearOperator, values: ScalarFloat | Float[Array, \"N\"]\n) -&gt; Diagonal | ScalarMul:\n    \"\"\"Create a diagonal operator with the same shape, dtype, and device as the operator.\n\n    Args:\n        operator: Linear operator.\n        values: Scalar for a scalar matrix or array of diagonal values for a diagonal matrix.\n\n    Returns:\n            Diagonal or scalar operator.\n    \"\"\"\n    device = operator.device\n    dtype = operator.dtype\n    if jnp.isscalar(values):\n        return ScalarMul(values, operator.shape, dtype=dtype, device=device)\n    else:\n        values = values.astype(dtype).to_device(device)\n        return Diagonal(values)\n</code></pre>"},{"location":"reference/cagpjax/policies/","title":"policies","text":""},{"location":"reference/cagpjax/policies/#cagpjax.policies","title":"<code>cagpjax.policies</code>","text":""},{"location":"reference/cagpjax/policies/#cagpjax.policies.AbstractBatchLinearSolverPolicy","title":"<code>AbstractBatchLinearSolverPolicy</code>","text":"<p>               Bases: <code>AbstractLinearSolverPolicy</code>, <code>ABC</code></p> <p>Abstract base class for policies that product action matrices.</p> Source code in <code>src/cagpjax/policies/base.py</code> <pre><code>class AbstractBatchLinearSolverPolicy(AbstractLinearSolverPolicy, abc.ABC):\n    \"\"\"Abstract base class for policies that product action matrices.\"\"\"\n\n    @property\n    @abc.abstractmethod\n    def n_actions(self) -&gt; int:\n        \"\"\"Number of actions in this policy.\"\"\"\n        ...\n\n    @abc.abstractmethod\n    def to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n        r\"\"\"Compute all actions used to solve the linear system $Ax=b$.\n\n        For a matrix $A$ with shape ``(n, n)``, the action matrix has shape\n        ``(n, n_actions)``.\n\n        Args:\n            A: Linear operator representing the linear system.\n\n        Returns:\n            Linear operator representing the action matrix.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.AbstractBatchLinearSolverPolicy.n_actions","title":"<code>n_actions</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Number of actions in this policy.</p>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.AbstractBatchLinearSolverPolicy.to_actions","title":"<code>to_actions(A)</code>  <code>abstractmethod</code>","text":"<p>Compute all actions used to solve the linear system \\(Ax=b\\).</p> <p>For a matrix \\(A\\) with shape <code>(n, n)</code>, the action matrix has shape <code>(n, n_actions)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>LinearOperator</code> <p>Linear operator representing the linear system.</p> required <p>Returns:</p> Type Description <code>LinearOperator</code> <p>Linear operator representing the action matrix.</p> Source code in <code>src/cagpjax/policies/base.py</code> <pre><code>@abc.abstractmethod\ndef to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n    r\"\"\"Compute all actions used to solve the linear system $Ax=b$.\n\n    For a matrix $A$ with shape ``(n, n)``, the action matrix has shape\n    ``(n, n_actions)``.\n\n    Args:\n        A: Linear operator representing the linear system.\n\n    Returns:\n        Linear operator representing the action matrix.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.AbstractLinearSolverPolicy","title":"<code>AbstractLinearSolverPolicy</code>","text":"<p>               Bases: <code>Module</code></p> <p>Abstract base class for all linear solver policies.</p> <p>Policies define actions used to solve a linear system \\(A x = b\\), where \\(A\\) is a square linear operator.</p> Source code in <code>src/cagpjax/policies/base.py</code> <pre><code>class AbstractLinearSolverPolicy(nnx.Module):\n    r\"\"\"Abstract base class for all linear solver policies.\n\n    Policies define actions used to solve a linear system $A x = b$, where $A$ is a\n    square linear operator.\n    \"\"\"\n\n    ...\n</code></pre>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.BlockSparsePolicy","title":"<code>BlockSparsePolicy</code>","text":"<p>               Bases: <code>AbstractBatchLinearSolverPolicy</code></p> <p>Block-sparse linear solver policy.</p> <p>This policy uses a fixed block-diagonal sparse structure to define independent learnable actions. The matrix has the following structure:</p> \\[ S = \\begin{bmatrix}     s_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\     0 &amp; s_2 &amp; \\cdots &amp; 0 \\\\     \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\     0 &amp; 0 &amp; \\cdots &amp; s_{\\text{n_actions}} \\end{bmatrix} \\] <p>These are stacked and stored as a single trainable parameter <code>nz_values</code>.</p> Source code in <code>src/cagpjax/policies/block_sparse.py</code> <pre><code>class BlockSparsePolicy(AbstractBatchLinearSolverPolicy):\n    r\"\"\"Block-sparse linear solver policy.\n\n    This policy uses a fixed block-diagonal sparse structure to define\n    independent learnable actions. The matrix has the following structure:\n\n    $$\n    S = \\begin{bmatrix}\n        s_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\\n        0 &amp; s_2 &amp; \\cdots &amp; 0 \\\\\n        \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\\n        0 &amp; 0 &amp; \\cdots &amp; s_{\\text{n_actions}}\n    \\end{bmatrix}\n    $$\n\n    These are stacked and stored as a single trainable parameter ``nz_values``.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_actions: int,\n        n: int | None = None,\n        nz_values: Float[Array, \"N\"] | nnx.Variable[Float[Array, \"N\"]] | None = None,\n        key: PRNGKeyArray | None = None,\n        **kwargs,\n    ):\n        \"\"\"Initialize the block sparse policy.\n\n        Args:\n            n_actions: Number of actions to use.\n            n: Number of rows and columns of the full operator. Must be provided if ``nz_values`` is\n                not provided.\n            nz_values: Non-zero values of the block-diagonal sparse matrix (shape ``(n,)``). If not\n                provided, random actions are sampled using the key if provided.\n            key: Random key for sampling actions if ``nz_values`` is not provided.\n            **kwargs: Additional keyword arguments for ``jax.random.normal`` (e.g. ``dtype``)\n        \"\"\"\n        if nz_values is None:\n            if n is None:\n                raise ValueError(\"n must be provided if nz_values is not provided\")\n            if key is None:\n                key = jax.random.PRNGKey(0)\n            block_size = n // n_actions\n            nz_values = jax.random.normal(key, (n,), **kwargs)\n            nz_values /= jnp.sqrt(block_size)\n        elif n is not None:\n            warnings.warn(\"n is ignored because nz_values is provided\")\n\n        if not isinstance(nz_values, nnx.Variable):\n            nz_values = Real(nz_values)\n\n        self.nz_values: nnx.Variable[Float[Array, \"N\"]] = nz_values\n        self._n_actions: int = n_actions\n\n    @property\n    @override\n    def n_actions(self) -&gt; int:\n        \"\"\"Number of actions to be used.\"\"\"\n        return self._n_actions\n\n    @override\n    def to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n        \"\"\"Convert to block diagonal sparse action operators.\n\n        Args:\n            A: Linear operator (unused).\n\n        Returns:\n            BlockDiagonalSparse: Sparse action structure representing the blocks.\n        \"\"\"\n        return BlockDiagonalSparse(self.nz_values.value, self.n_actions)\n</code></pre>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.BlockSparsePolicy.n_actions","title":"<code>n_actions</code>  <code>property</code>","text":"<p>Number of actions to be used.</p>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.BlockSparsePolicy.__init__","title":"<code>__init__(n_actions, n=None, nz_values=None, key=None, **kwargs)</code>","text":"<p>Initialize the block sparse policy.</p> <p>Parameters:</p> Name Type Description Default <code>n_actions</code> <code>int</code> <p>Number of actions to use.</p> required <code>n</code> <code>int | None</code> <p>Number of rows and columns of the full operator. Must be provided if <code>nz_values</code> is not provided.</p> <code>None</code> <code>nz_values</code> <code>Float[Array, N] | Variable[Float[Array, N]] | None</code> <p>Non-zero values of the block-diagonal sparse matrix (shape <code>(n,)</code>). If not provided, random actions are sampled using the key if provided.</p> <code>None</code> <code>key</code> <code>PRNGKeyArray | None</code> <p>Random key for sampling actions if <code>nz_values</code> is not provided.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for <code>jax.random.normal</code> (e.g. <code>dtype</code>)</p> <code>{}</code> Source code in <code>src/cagpjax/policies/block_sparse.py</code> <pre><code>def __init__(\n    self,\n    n_actions: int,\n    n: int | None = None,\n    nz_values: Float[Array, \"N\"] | nnx.Variable[Float[Array, \"N\"]] | None = None,\n    key: PRNGKeyArray | None = None,\n    **kwargs,\n):\n    \"\"\"Initialize the block sparse policy.\n\n    Args:\n        n_actions: Number of actions to use.\n        n: Number of rows and columns of the full operator. Must be provided if ``nz_values`` is\n            not provided.\n        nz_values: Non-zero values of the block-diagonal sparse matrix (shape ``(n,)``). If not\n            provided, random actions are sampled using the key if provided.\n        key: Random key for sampling actions if ``nz_values`` is not provided.\n        **kwargs: Additional keyword arguments for ``jax.random.normal`` (e.g. ``dtype``)\n    \"\"\"\n    if nz_values is None:\n        if n is None:\n            raise ValueError(\"n must be provided if nz_values is not provided\")\n        if key is None:\n            key = jax.random.PRNGKey(0)\n        block_size = n // n_actions\n        nz_values = jax.random.normal(key, (n,), **kwargs)\n        nz_values /= jnp.sqrt(block_size)\n    elif n is not None:\n        warnings.warn(\"n is ignored because nz_values is provided\")\n\n    if not isinstance(nz_values, nnx.Variable):\n        nz_values = Real(nz_values)\n\n    self.nz_values: nnx.Variable[Float[Array, \"N\"]] = nz_values\n    self._n_actions: int = n_actions\n</code></pre>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.BlockSparsePolicy.to_actions","title":"<code>to_actions(A)</code>","text":"<p>Convert to block diagonal sparse action operators.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>LinearOperator</code> <p>Linear operator (unused).</p> required <p>Returns:</p> Name Type Description <code>BlockDiagonalSparse</code> <code>LinearOperator</code> <p>Sparse action structure representing the blocks.</p> Source code in <code>src/cagpjax/policies/block_sparse.py</code> <pre><code>@override\ndef to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n    \"\"\"Convert to block diagonal sparse action operators.\n\n    Args:\n        A: Linear operator (unused).\n\n    Returns:\n        BlockDiagonalSparse: Sparse action structure representing the blocks.\n    \"\"\"\n    return BlockDiagonalSparse(self.nz_values.value, self.n_actions)\n</code></pre>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.LanczosPolicy","title":"<code>LanczosPolicy</code>","text":"<p>               Bases: <code>AbstractBatchLinearSolverPolicy</code></p> <p>Lanczos-based policy for eigenvalue decomposition approximation.</p> <p>This policy uses the Lanczos algorithm to compute the top <code>n_actions</code> eigenvectors of the linear operator \\(A\\).</p> <p>Attributes:</p> Name Type Description <code>n_actions</code> <code>int</code> <p>Number of Lanczos vectors/actions to compute.</p> <code>key</code> <code>PRNGKeyArray | None</code> <p>Random key for reproducible Lanczos iterations.</p> Source code in <code>src/cagpjax/policies/lanczos.py</code> <pre><code>class LanczosPolicy(AbstractBatchLinearSolverPolicy):\n    \"\"\"Lanczos-based policy for eigenvalue decomposition approximation.\n\n    This policy uses the Lanczos algorithm to compute the top ``n_actions`` eigenvectors\n    of the linear operator $A$.\n\n    Attributes:\n        n_actions: Number of Lanczos vectors/actions to compute.\n        key: Random key for reproducible Lanczos iterations.\n    \"\"\"\n\n    key: PRNGKeyArray | None\n    grad_rtol: float | None\n\n    def __init__(\n        self,\n        n_actions: int | None,\n        key: PRNGKeyArray | None = None,\n        grad_rtol: float | None = 0.0,\n    ):\n        \"\"\"Initialize the Lanczos policy.\n\n        Args:\n            n_actions: Number of Lanczos vectors to compute.\n            key: Random key for initialization.\n            grad_rtol: Specifies the cutoff for similar eigenvalues, used to improve\n                gradient computation for (almost-)degenerate matrices.\n                If not provided, the default is 0.0.\n                If None or negative, all eigenvalues are treated as distinct.\n                (see [`cagpjax.linalg.eigh`][] for more details)\n        \"\"\"\n        self._n_actions: int = n_actions\n        self.key = key\n        self.grad_rtol = grad_rtol\n\n    @property\n    @override\n    def n_actions(self) -&gt; int:\n        return self._n_actions\n\n    @override\n    def to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n        \"\"\"Compute action matrix.\n\n        Args:\n            A: Symmetric linear operator representing the linear system.\n\n        Returns:\n            Linear operator containing the Lanczos vectors as columns.\n        \"\"\"\n        vecs = eigh(\n            A, alg=Lanczos(self.n_actions, key=self.key), grad_rtol=self.grad_rtol\n        ).eigenvectors\n        return vecs\n</code></pre>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.LanczosPolicy.__init__","title":"<code>__init__(n_actions, key=None, grad_rtol=0.0)</code>","text":"<p>Initialize the Lanczos policy.</p> <p>Parameters:</p> Name Type Description Default <code>n_actions</code> <code>int | None</code> <p>Number of Lanczos vectors to compute.</p> required <code>key</code> <code>PRNGKeyArray | None</code> <p>Random key for initialization.</p> <code>None</code> <code>grad_rtol</code> <code>float | None</code> <p>Specifies the cutoff for similar eigenvalues, used to improve gradient computation for (almost-)degenerate matrices. If not provided, the default is 0.0. If None or negative, all eigenvalues are treated as distinct. (see <code>cagpjax.linalg.eigh</code> for more details)</p> <code>0.0</code> Source code in <code>src/cagpjax/policies/lanczos.py</code> <pre><code>def __init__(\n    self,\n    n_actions: int | None,\n    key: PRNGKeyArray | None = None,\n    grad_rtol: float | None = 0.0,\n):\n    \"\"\"Initialize the Lanczos policy.\n\n    Args:\n        n_actions: Number of Lanczos vectors to compute.\n        key: Random key for initialization.\n        grad_rtol: Specifies the cutoff for similar eigenvalues, used to improve\n            gradient computation for (almost-)degenerate matrices.\n            If not provided, the default is 0.0.\n            If None or negative, all eigenvalues are treated as distinct.\n            (see [`cagpjax.linalg.eigh`][] for more details)\n    \"\"\"\n    self._n_actions: int = n_actions\n    self.key = key\n    self.grad_rtol = grad_rtol\n</code></pre>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.LanczosPolicy.to_actions","title":"<code>to_actions(A)</code>","text":"<p>Compute action matrix.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>LinearOperator</code> <p>Symmetric linear operator representing the linear system.</p> required <p>Returns:</p> Type Description <code>LinearOperator</code> <p>Linear operator containing the Lanczos vectors as columns.</p> Source code in <code>src/cagpjax/policies/lanczos.py</code> <pre><code>@override\ndef to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n    \"\"\"Compute action matrix.\n\n    Args:\n        A: Symmetric linear operator representing the linear system.\n\n    Returns:\n        Linear operator containing the Lanczos vectors as columns.\n    \"\"\"\n    vecs = eigh(\n        A, alg=Lanczos(self.n_actions, key=self.key), grad_rtol=self.grad_rtol\n    ).eigenvectors\n    return vecs\n</code></pre>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.OrthogonalizationPolicy","title":"<code>OrthogonalizationPolicy</code>","text":"<p>               Bases: <code>AbstractBatchLinearSolverPolicy</code></p> <p>Orthogonalization policy.</p> <p>This policy orthogonalizes (if necessary) the action operator produced by the base policy.</p> <p>Parameters:</p> Name Type Description Default <code>base_policy</code> <code>AbstractBatchLinearSolverPolicy</code> <p>The base policy that produces the action operator to be orthogonalized.</p> required <code>method</code> <code>OrthogonalizationMethod</code> <p>The method to use for orthogonalization.</p> <code>QR</code> <code>n_reortho</code> <code>int</code> <p>The number of times to re-orthogonalize each column. Reorthogonalizing once is generally sufficient to improve orthogonality for Gram-Schmidt variants (see e.g. 10.1007/s00211-005-0615-4).</p> <code>0</code> Source code in <code>src/cagpjax/policies/orthogonalization.py</code> <pre><code>class OrthogonalizationPolicy(AbstractBatchLinearSolverPolicy):\n    \"\"\"Orthogonalization policy.\n\n    This policy orthogonalizes (if necessary) the action operator produced by the base policy.\n\n    Args:\n        base_policy: The base policy that produces the action operator to be orthogonalized.\n        method: The method to use for orthogonalization.\n        n_reortho: The number of times to _re_-orthogonalize each column.\n            Reorthogonalizing once is generally sufficient to improve orthogonality\n            for Gram-Schmidt variants\n            (see e.g. [10.1007/s00211-005-0615-4](https://doi.org/10.1007/s00211-005-0615-4)).\n    \"\"\"\n\n    base_policy: AbstractBatchLinearSolverPolicy\n    method: OrthogonalizationMethod\n    n_reortho: int\n\n    def __init__(\n        self,\n        base_policy: AbstractBatchLinearSolverPolicy,\n        method: OrthogonalizationMethod = OrthogonalizationMethod.QR,\n        n_reortho: int = 0,\n    ):\n        self.base_policy = base_policy\n        self.method = method\n        self.n_reortho = n_reortho\n\n    @property\n    @override\n    def n_actions(self):\n        return self.base_policy.n_actions\n\n    @override\n    def to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n        op = self.base_policy.to_actions(A)\n        return cola.lazify(\n            orthogonalize(op, method=self.method, n_reortho=self.n_reortho)\n        )\n</code></pre>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.PseudoInputPolicy","title":"<code>PseudoInputPolicy</code>","text":"<p>               Bases: <code>AbstractBatchLinearSolverPolicy</code></p> <p>Pseudo-input linear solver policy.</p> <p>This policy constructs actions from the cross-covariance between the training inputs and pseudo-inputs in the same input space. These pseudo-inputs are conceptually similar to inducing points and can be marked as trainable.</p> <p>Parameters:</p> Name Type Description Default <code>pseudo_inputs</code> <code>Float[Array, 'M D'] | Variable</code> <p>Pseudo-inputs for the kernel. If wrapped as a <code>gpjax.parameters.Parameter</code>, they will be treated as trainable.</p> required <code>train_inputs</code> <p>Training inputs or a dataset containing training inputs. These must be the same inputs in the same order as the training data used to condition the CaGP model.</p> required <code>kernel</code> <code>AbstractKernel</code> <p>Kernel for the GP prior. It must be able to take <code>train_inputs</code> and <code>pseudo_inputs</code> as arguments to its <code>cross_covariance</code> method.</p> required Note <p>When training with many pseudo-inputs, it is common for the cross-covariance matrix to become poorly conditioned. Performance can be significantly improved by orthogonalizing the actions using an <code>OrthogonalizationPolicy</code>.</p> Source code in <code>src/cagpjax/policies/pseudoinput.py</code> <pre><code>class PseudoInputPolicy(AbstractBatchLinearSolverPolicy):\n    \"\"\"Pseudo-input linear solver policy.\n\n    This policy constructs actions from the cross-covariance between the training inputs and\n    pseudo-inputs in the same input space. These pseudo-inputs are conceptually similar to\n    inducing points and can be marked as trainable.\n\n    Args:\n        pseudo_inputs: Pseudo-inputs for the kernel. If wrapped as a `gpjax.parameters.Parameter`,\n            they will be treated as trainable.\n        train_inputs: Training inputs or a dataset containing training inputs. These must be the\n            same inputs in the same order as the training data used to condition the CaGP model.\n        kernel: Kernel for the GP prior. It must be able to take `train_inputs` and `pseudo_inputs`\n            as arguments to its `cross_covariance` method.\n\n    Note:\n        When training with many pseudo-inputs, it is common for the cross-covariance matrix to\n        become poorly conditioned. Performance can be significantly improved by orthogonalizing\n        the actions using an [`OrthogonalizationPolicy`][cagpjax.policies.OrthogonalizationPolicy].\n    \"\"\"\n\n    pseudo_inputs: nnx.Variable\n    train_inputs: Float[Array, \"N D\"]\n    kernel: gpjax.kernels.AbstractKernel\n\n    def __init__(\n        self,\n        pseudo_inputs: Float[Array, \"M D\"] | nnx.Variable,\n        train_inputs_or_dataset: Float[Array, \"N D\"] | gpjax.dataset.Dataset,\n        kernel: gpjax.kernels.AbstractKernel,\n    ):\n        if isinstance(train_inputs_or_dataset, gpjax.dataset.Dataset):\n            train_data = train_inputs_or_dataset\n            if train_data.X is None:\n                raise ValueError(\"Dataset must contain training inputs.\")\n            train_inputs = train_data.X\n        else:\n            train_inputs = train_inputs_or_dataset\n        if not isinstance(pseudo_inputs, nnx.Variable):\n            pseudo_inputs = gpjax.parameters.Static(jnp.atleast_2d(pseudo_inputs))\n        self.pseudo_inputs = pseudo_inputs\n        self.train_inputs = jnp.atleast_2d(train_inputs)\n        self.kernel = kernel\n\n    @property\n    def n_actions(self):\n        return self.pseudo_inputs.shape[0]\n\n    def to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n        S = self.kernel.cross_covariance(self.train_inputs, self.pseudo_inputs.value)\n        return cola.lazify(S)\n</code></pre>"},{"location":"reference/cagpjax/policies/base/","title":"base","text":""},{"location":"reference/cagpjax/policies/base/#cagpjax.policies.base","title":"<code>cagpjax.policies.base</code>","text":""},{"location":"reference/cagpjax/policies/base/#cagpjax.policies.base.AbstractBatchLinearSolverPolicy","title":"<code>AbstractBatchLinearSolverPolicy</code>","text":"<p>               Bases: <code>AbstractLinearSolverPolicy</code>, <code>ABC</code></p> <p>Abstract base class for policies that product action matrices.</p> Source code in <code>src/cagpjax/policies/base.py</code> <pre><code>class AbstractBatchLinearSolverPolicy(AbstractLinearSolverPolicy, abc.ABC):\n    \"\"\"Abstract base class for policies that product action matrices.\"\"\"\n\n    @property\n    @abc.abstractmethod\n    def n_actions(self) -&gt; int:\n        \"\"\"Number of actions in this policy.\"\"\"\n        ...\n\n    @abc.abstractmethod\n    def to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n        r\"\"\"Compute all actions used to solve the linear system $Ax=b$.\n\n        For a matrix $A$ with shape ``(n, n)``, the action matrix has shape\n        ``(n, n_actions)``.\n\n        Args:\n            A: Linear operator representing the linear system.\n\n        Returns:\n            Linear operator representing the action matrix.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/cagpjax/policies/base/#cagpjax.policies.base.AbstractBatchLinearSolverPolicy.n_actions","title":"<code>n_actions</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Number of actions in this policy.</p>"},{"location":"reference/cagpjax/policies/base/#cagpjax.policies.base.AbstractBatchLinearSolverPolicy.to_actions","title":"<code>to_actions(A)</code>  <code>abstractmethod</code>","text":"<p>Compute all actions used to solve the linear system \\(Ax=b\\).</p> <p>For a matrix \\(A\\) with shape <code>(n, n)</code>, the action matrix has shape <code>(n, n_actions)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>LinearOperator</code> <p>Linear operator representing the linear system.</p> required <p>Returns:</p> Type Description <code>LinearOperator</code> <p>Linear operator representing the action matrix.</p> Source code in <code>src/cagpjax/policies/base.py</code> <pre><code>@abc.abstractmethod\ndef to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n    r\"\"\"Compute all actions used to solve the linear system $Ax=b$.\n\n    For a matrix $A$ with shape ``(n, n)``, the action matrix has shape\n    ``(n, n_actions)``.\n\n    Args:\n        A: Linear operator representing the linear system.\n\n    Returns:\n        Linear operator representing the action matrix.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/cagpjax/policies/base/#cagpjax.policies.base.AbstractLinearSolverPolicy","title":"<code>AbstractLinearSolverPolicy</code>","text":"<p>               Bases: <code>Module</code></p> <p>Abstract base class for all linear solver policies.</p> <p>Policies define actions used to solve a linear system \\(A x = b\\), where \\(A\\) is a square linear operator.</p> Source code in <code>src/cagpjax/policies/base.py</code> <pre><code>class AbstractLinearSolverPolicy(nnx.Module):\n    r\"\"\"Abstract base class for all linear solver policies.\n\n    Policies define actions used to solve a linear system $A x = b$, where $A$ is a\n    square linear operator.\n    \"\"\"\n\n    ...\n</code></pre>"},{"location":"reference/cagpjax/policies/block_sparse/","title":"block_sparse","text":""},{"location":"reference/cagpjax/policies/block_sparse/#cagpjax.policies.block_sparse","title":"<code>cagpjax.policies.block_sparse</code>","text":"<p>Block-sparse policy.</p>"},{"location":"reference/cagpjax/policies/block_sparse/#cagpjax.policies.block_sparse.BlockSparsePolicy","title":"<code>BlockSparsePolicy</code>","text":"<p>               Bases: <code>AbstractBatchLinearSolverPolicy</code></p> <p>Block-sparse linear solver policy.</p> <p>This policy uses a fixed block-diagonal sparse structure to define independent learnable actions. The matrix has the following structure:</p> \\[ S = \\begin{bmatrix}     s_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\     0 &amp; s_2 &amp; \\cdots &amp; 0 \\\\     \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\     0 &amp; 0 &amp; \\cdots &amp; s_{\\text{n_actions}} \\end{bmatrix} \\] <p>These are stacked and stored as a single trainable parameter <code>nz_values</code>.</p> Source code in <code>src/cagpjax/policies/block_sparse.py</code> <pre><code>class BlockSparsePolicy(AbstractBatchLinearSolverPolicy):\n    r\"\"\"Block-sparse linear solver policy.\n\n    This policy uses a fixed block-diagonal sparse structure to define\n    independent learnable actions. The matrix has the following structure:\n\n    $$\n    S = \\begin{bmatrix}\n        s_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\\n        0 &amp; s_2 &amp; \\cdots &amp; 0 \\\\\n        \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\\n        0 &amp; 0 &amp; \\cdots &amp; s_{\\text{n_actions}}\n    \\end{bmatrix}\n    $$\n\n    These are stacked and stored as a single trainable parameter ``nz_values``.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_actions: int,\n        n: int | None = None,\n        nz_values: Float[Array, \"N\"] | nnx.Variable[Float[Array, \"N\"]] | None = None,\n        key: PRNGKeyArray | None = None,\n        **kwargs,\n    ):\n        \"\"\"Initialize the block sparse policy.\n\n        Args:\n            n_actions: Number of actions to use.\n            n: Number of rows and columns of the full operator. Must be provided if ``nz_values`` is\n                not provided.\n            nz_values: Non-zero values of the block-diagonal sparse matrix (shape ``(n,)``). If not\n                provided, random actions are sampled using the key if provided.\n            key: Random key for sampling actions if ``nz_values`` is not provided.\n            **kwargs: Additional keyword arguments for ``jax.random.normal`` (e.g. ``dtype``)\n        \"\"\"\n        if nz_values is None:\n            if n is None:\n                raise ValueError(\"n must be provided if nz_values is not provided\")\n            if key is None:\n                key = jax.random.PRNGKey(0)\n            block_size = n // n_actions\n            nz_values = jax.random.normal(key, (n,), **kwargs)\n            nz_values /= jnp.sqrt(block_size)\n        elif n is not None:\n            warnings.warn(\"n is ignored because nz_values is provided\")\n\n        if not isinstance(nz_values, nnx.Variable):\n            nz_values = Real(nz_values)\n\n        self.nz_values: nnx.Variable[Float[Array, \"N\"]] = nz_values\n        self._n_actions: int = n_actions\n\n    @property\n    @override\n    def n_actions(self) -&gt; int:\n        \"\"\"Number of actions to be used.\"\"\"\n        return self._n_actions\n\n    @override\n    def to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n        \"\"\"Convert to block diagonal sparse action operators.\n\n        Args:\n            A: Linear operator (unused).\n\n        Returns:\n            BlockDiagonalSparse: Sparse action structure representing the blocks.\n        \"\"\"\n        return BlockDiagonalSparse(self.nz_values.value, self.n_actions)\n</code></pre>"},{"location":"reference/cagpjax/policies/block_sparse/#cagpjax.policies.block_sparse.BlockSparsePolicy.n_actions","title":"<code>n_actions</code>  <code>property</code>","text":"<p>Number of actions to be used.</p>"},{"location":"reference/cagpjax/policies/block_sparse/#cagpjax.policies.block_sparse.BlockSparsePolicy.__init__","title":"<code>__init__(n_actions, n=None, nz_values=None, key=None, **kwargs)</code>","text":"<p>Initialize the block sparse policy.</p> <p>Parameters:</p> Name Type Description Default <code>n_actions</code> <code>int</code> <p>Number of actions to use.</p> required <code>n</code> <code>int | None</code> <p>Number of rows and columns of the full operator. Must be provided if <code>nz_values</code> is not provided.</p> <code>None</code> <code>nz_values</code> <code>Float[Array, N] | Variable[Float[Array, N]] | None</code> <p>Non-zero values of the block-diagonal sparse matrix (shape <code>(n,)</code>). If not provided, random actions are sampled using the key if provided.</p> <code>None</code> <code>key</code> <code>PRNGKeyArray | None</code> <p>Random key for sampling actions if <code>nz_values</code> is not provided.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for <code>jax.random.normal</code> (e.g. <code>dtype</code>)</p> <code>{}</code> Source code in <code>src/cagpjax/policies/block_sparse.py</code> <pre><code>def __init__(\n    self,\n    n_actions: int,\n    n: int | None = None,\n    nz_values: Float[Array, \"N\"] | nnx.Variable[Float[Array, \"N\"]] | None = None,\n    key: PRNGKeyArray | None = None,\n    **kwargs,\n):\n    \"\"\"Initialize the block sparse policy.\n\n    Args:\n        n_actions: Number of actions to use.\n        n: Number of rows and columns of the full operator. Must be provided if ``nz_values`` is\n            not provided.\n        nz_values: Non-zero values of the block-diagonal sparse matrix (shape ``(n,)``). If not\n            provided, random actions are sampled using the key if provided.\n        key: Random key for sampling actions if ``nz_values`` is not provided.\n        **kwargs: Additional keyword arguments for ``jax.random.normal`` (e.g. ``dtype``)\n    \"\"\"\n    if nz_values is None:\n        if n is None:\n            raise ValueError(\"n must be provided if nz_values is not provided\")\n        if key is None:\n            key = jax.random.PRNGKey(0)\n        block_size = n // n_actions\n        nz_values = jax.random.normal(key, (n,), **kwargs)\n        nz_values /= jnp.sqrt(block_size)\n    elif n is not None:\n        warnings.warn(\"n is ignored because nz_values is provided\")\n\n    if not isinstance(nz_values, nnx.Variable):\n        nz_values = Real(nz_values)\n\n    self.nz_values: nnx.Variable[Float[Array, \"N\"]] = nz_values\n    self._n_actions: int = n_actions\n</code></pre>"},{"location":"reference/cagpjax/policies/block_sparse/#cagpjax.policies.block_sparse.BlockSparsePolicy.to_actions","title":"<code>to_actions(A)</code>","text":"<p>Convert to block diagonal sparse action operators.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>LinearOperator</code> <p>Linear operator (unused).</p> required <p>Returns:</p> Name Type Description <code>BlockDiagonalSparse</code> <code>LinearOperator</code> <p>Sparse action structure representing the blocks.</p> Source code in <code>src/cagpjax/policies/block_sparse.py</code> <pre><code>@override\ndef to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n    \"\"\"Convert to block diagonal sparse action operators.\n\n    Args:\n        A: Linear operator (unused).\n\n    Returns:\n        BlockDiagonalSparse: Sparse action structure representing the blocks.\n    \"\"\"\n    return BlockDiagonalSparse(self.nz_values.value, self.n_actions)\n</code></pre>"},{"location":"reference/cagpjax/policies/lanczos/","title":"lanczos","text":""},{"location":"reference/cagpjax/policies/lanczos/#cagpjax.policies.lanczos","title":"<code>cagpjax.policies.lanczos</code>","text":"<p>Lanczos-based policies.</p>"},{"location":"reference/cagpjax/policies/lanczos/#cagpjax.policies.lanczos.LanczosPolicy","title":"<code>LanczosPolicy</code>","text":"<p>               Bases: <code>AbstractBatchLinearSolverPolicy</code></p> <p>Lanczos-based policy for eigenvalue decomposition approximation.</p> <p>This policy uses the Lanczos algorithm to compute the top <code>n_actions</code> eigenvectors of the linear operator \\(A\\).</p> <p>Attributes:</p> Name Type Description <code>n_actions</code> <code>int</code> <p>Number of Lanczos vectors/actions to compute.</p> <code>key</code> <code>PRNGKeyArray | None</code> <p>Random key for reproducible Lanczos iterations.</p> Source code in <code>src/cagpjax/policies/lanczos.py</code> <pre><code>class LanczosPolicy(AbstractBatchLinearSolverPolicy):\n    \"\"\"Lanczos-based policy for eigenvalue decomposition approximation.\n\n    This policy uses the Lanczos algorithm to compute the top ``n_actions`` eigenvectors\n    of the linear operator $A$.\n\n    Attributes:\n        n_actions: Number of Lanczos vectors/actions to compute.\n        key: Random key for reproducible Lanczos iterations.\n    \"\"\"\n\n    key: PRNGKeyArray | None\n    grad_rtol: float | None\n\n    def __init__(\n        self,\n        n_actions: int | None,\n        key: PRNGKeyArray | None = None,\n        grad_rtol: float | None = 0.0,\n    ):\n        \"\"\"Initialize the Lanczos policy.\n\n        Args:\n            n_actions: Number of Lanczos vectors to compute.\n            key: Random key for initialization.\n            grad_rtol: Specifies the cutoff for similar eigenvalues, used to improve\n                gradient computation for (almost-)degenerate matrices.\n                If not provided, the default is 0.0.\n                If None or negative, all eigenvalues are treated as distinct.\n                (see [`cagpjax.linalg.eigh`][] for more details)\n        \"\"\"\n        self._n_actions: int = n_actions\n        self.key = key\n        self.grad_rtol = grad_rtol\n\n    @property\n    @override\n    def n_actions(self) -&gt; int:\n        return self._n_actions\n\n    @override\n    def to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n        \"\"\"Compute action matrix.\n\n        Args:\n            A: Symmetric linear operator representing the linear system.\n\n        Returns:\n            Linear operator containing the Lanczos vectors as columns.\n        \"\"\"\n        vecs = eigh(\n            A, alg=Lanczos(self.n_actions, key=self.key), grad_rtol=self.grad_rtol\n        ).eigenvectors\n        return vecs\n</code></pre>"},{"location":"reference/cagpjax/policies/lanczos/#cagpjax.policies.lanczos.LanczosPolicy.__init__","title":"<code>__init__(n_actions, key=None, grad_rtol=0.0)</code>","text":"<p>Initialize the Lanczos policy.</p> <p>Parameters:</p> Name Type Description Default <code>n_actions</code> <code>int | None</code> <p>Number of Lanczos vectors to compute.</p> required <code>key</code> <code>PRNGKeyArray | None</code> <p>Random key for initialization.</p> <code>None</code> <code>grad_rtol</code> <code>float | None</code> <p>Specifies the cutoff for similar eigenvalues, used to improve gradient computation for (almost-)degenerate matrices. If not provided, the default is 0.0. If None or negative, all eigenvalues are treated as distinct. (see <code>cagpjax.linalg.eigh</code> for more details)</p> <code>0.0</code> Source code in <code>src/cagpjax/policies/lanczos.py</code> <pre><code>def __init__(\n    self,\n    n_actions: int | None,\n    key: PRNGKeyArray | None = None,\n    grad_rtol: float | None = 0.0,\n):\n    \"\"\"Initialize the Lanczos policy.\n\n    Args:\n        n_actions: Number of Lanczos vectors to compute.\n        key: Random key for initialization.\n        grad_rtol: Specifies the cutoff for similar eigenvalues, used to improve\n            gradient computation for (almost-)degenerate matrices.\n            If not provided, the default is 0.0.\n            If None or negative, all eigenvalues are treated as distinct.\n            (see [`cagpjax.linalg.eigh`][] for more details)\n    \"\"\"\n    self._n_actions: int = n_actions\n    self.key = key\n    self.grad_rtol = grad_rtol\n</code></pre>"},{"location":"reference/cagpjax/policies/lanczos/#cagpjax.policies.lanczos.LanczosPolicy.to_actions","title":"<code>to_actions(A)</code>","text":"<p>Compute action matrix.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>LinearOperator</code> <p>Symmetric linear operator representing the linear system.</p> required <p>Returns:</p> Type Description <code>LinearOperator</code> <p>Linear operator containing the Lanczos vectors as columns.</p> Source code in <code>src/cagpjax/policies/lanczos.py</code> <pre><code>@override\ndef to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n    \"\"\"Compute action matrix.\n\n    Args:\n        A: Symmetric linear operator representing the linear system.\n\n    Returns:\n        Linear operator containing the Lanczos vectors as columns.\n    \"\"\"\n    vecs = eigh(\n        A, alg=Lanczos(self.n_actions, key=self.key), grad_rtol=self.grad_rtol\n    ).eigenvectors\n    return vecs\n</code></pre>"},{"location":"reference/cagpjax/policies/orthogonalization/","title":"orthogonalization","text":""},{"location":"reference/cagpjax/policies/orthogonalization/#cagpjax.policies.orthogonalization","title":"<code>cagpjax.policies.orthogonalization</code>","text":""},{"location":"reference/cagpjax/policies/orthogonalization/#cagpjax.policies.orthogonalization.OrthogonalizationPolicy","title":"<code>OrthogonalizationPolicy</code>","text":"<p>               Bases: <code>AbstractBatchLinearSolverPolicy</code></p> <p>Orthogonalization policy.</p> <p>This policy orthogonalizes (if necessary) the action operator produced by the base policy.</p> <p>Parameters:</p> Name Type Description Default <code>base_policy</code> <code>AbstractBatchLinearSolverPolicy</code> <p>The base policy that produces the action operator to be orthogonalized.</p> required <code>method</code> <code>OrthogonalizationMethod</code> <p>The method to use for orthogonalization.</p> <code>QR</code> <code>n_reortho</code> <code>int</code> <p>The number of times to re-orthogonalize each column. Reorthogonalizing once is generally sufficient to improve orthogonality for Gram-Schmidt variants (see e.g. 10.1007/s00211-005-0615-4).</p> <code>0</code> Source code in <code>src/cagpjax/policies/orthogonalization.py</code> <pre><code>class OrthogonalizationPolicy(AbstractBatchLinearSolverPolicy):\n    \"\"\"Orthogonalization policy.\n\n    This policy orthogonalizes (if necessary) the action operator produced by the base policy.\n\n    Args:\n        base_policy: The base policy that produces the action operator to be orthogonalized.\n        method: The method to use for orthogonalization.\n        n_reortho: The number of times to _re_-orthogonalize each column.\n            Reorthogonalizing once is generally sufficient to improve orthogonality\n            for Gram-Schmidt variants\n            (see e.g. [10.1007/s00211-005-0615-4](https://doi.org/10.1007/s00211-005-0615-4)).\n    \"\"\"\n\n    base_policy: AbstractBatchLinearSolverPolicy\n    method: OrthogonalizationMethod\n    n_reortho: int\n\n    def __init__(\n        self,\n        base_policy: AbstractBatchLinearSolverPolicy,\n        method: OrthogonalizationMethod = OrthogonalizationMethod.QR,\n        n_reortho: int = 0,\n    ):\n        self.base_policy = base_policy\n        self.method = method\n        self.n_reortho = n_reortho\n\n    @property\n    @override\n    def n_actions(self):\n        return self.base_policy.n_actions\n\n    @override\n    def to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n        op = self.base_policy.to_actions(A)\n        return cola.lazify(\n            orthogonalize(op, method=self.method, n_reortho=self.n_reortho)\n        )\n</code></pre>"},{"location":"reference/cagpjax/policies/pseudoinput/","title":"pseudoinput","text":""},{"location":"reference/cagpjax/policies/pseudoinput/#cagpjax.policies.pseudoinput","title":"<code>cagpjax.policies.pseudoinput</code>","text":"<p>Pseodo-input linear solver policy.</p>"},{"location":"reference/cagpjax/policies/pseudoinput/#cagpjax.policies.pseudoinput.PseudoInputPolicy","title":"<code>PseudoInputPolicy</code>","text":"<p>               Bases: <code>AbstractBatchLinearSolverPolicy</code></p> <p>Pseudo-input linear solver policy.</p> <p>This policy constructs actions from the cross-covariance between the training inputs and pseudo-inputs in the same input space. These pseudo-inputs are conceptually similar to inducing points and can be marked as trainable.</p> <p>Parameters:</p> Name Type Description Default <code>pseudo_inputs</code> <code>Float[Array, 'M D'] | Variable</code> <p>Pseudo-inputs for the kernel. If wrapped as a <code>gpjax.parameters.Parameter</code>, they will be treated as trainable.</p> required <code>train_inputs</code> <p>Training inputs or a dataset containing training inputs. These must be the same inputs in the same order as the training data used to condition the CaGP model.</p> required <code>kernel</code> <code>AbstractKernel</code> <p>Kernel for the GP prior. It must be able to take <code>train_inputs</code> and <code>pseudo_inputs</code> as arguments to its <code>cross_covariance</code> method.</p> required Note <p>When training with many pseudo-inputs, it is common for the cross-covariance matrix to become poorly conditioned. Performance can be significantly improved by orthogonalizing the actions using an <code>OrthogonalizationPolicy</code>.</p> Source code in <code>src/cagpjax/policies/pseudoinput.py</code> <pre><code>class PseudoInputPolicy(AbstractBatchLinearSolverPolicy):\n    \"\"\"Pseudo-input linear solver policy.\n\n    This policy constructs actions from the cross-covariance between the training inputs and\n    pseudo-inputs in the same input space. These pseudo-inputs are conceptually similar to\n    inducing points and can be marked as trainable.\n\n    Args:\n        pseudo_inputs: Pseudo-inputs for the kernel. If wrapped as a `gpjax.parameters.Parameter`,\n            they will be treated as trainable.\n        train_inputs: Training inputs or a dataset containing training inputs. These must be the\n            same inputs in the same order as the training data used to condition the CaGP model.\n        kernel: Kernel for the GP prior. It must be able to take `train_inputs` and `pseudo_inputs`\n            as arguments to its `cross_covariance` method.\n\n    Note:\n        When training with many pseudo-inputs, it is common for the cross-covariance matrix to\n        become poorly conditioned. Performance can be significantly improved by orthogonalizing\n        the actions using an [`OrthogonalizationPolicy`][cagpjax.policies.OrthogonalizationPolicy].\n    \"\"\"\n\n    pseudo_inputs: nnx.Variable\n    train_inputs: Float[Array, \"N D\"]\n    kernel: gpjax.kernels.AbstractKernel\n\n    def __init__(\n        self,\n        pseudo_inputs: Float[Array, \"M D\"] | nnx.Variable,\n        train_inputs_or_dataset: Float[Array, \"N D\"] | gpjax.dataset.Dataset,\n        kernel: gpjax.kernels.AbstractKernel,\n    ):\n        if isinstance(train_inputs_or_dataset, gpjax.dataset.Dataset):\n            train_data = train_inputs_or_dataset\n            if train_data.X is None:\n                raise ValueError(\"Dataset must contain training inputs.\")\n            train_inputs = train_data.X\n        else:\n            train_inputs = train_inputs_or_dataset\n        if not isinstance(pseudo_inputs, nnx.Variable):\n            pseudo_inputs = gpjax.parameters.Static(jnp.atleast_2d(pseudo_inputs))\n        self.pseudo_inputs = pseudo_inputs\n        self.train_inputs = jnp.atleast_2d(train_inputs)\n        self.kernel = kernel\n\n    @property\n    def n_actions(self):\n        return self.pseudo_inputs.shape[0]\n\n    def to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n        S = self.kernel.cross_covariance(self.train_inputs, self.pseudo_inputs.value)\n        return cola.lazify(S)\n</code></pre>"},{"location":"reference/cagpjax/solvers/","title":"solvers","text":""},{"location":"reference/cagpjax/solvers/#cagpjax.solvers","title":"<code>cagpjax.solvers</code>","text":""},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver","title":"<code>AbstractLinearSolver</code>","text":"<p>               Bases: <code>Module</code></p> <p>Base class for linear solvers.</p> <p>These solvers are used to exactly or approximately solve the linear system \\(Ax = b\\) for \\(x\\), where \\(A\\) is a positive (semi-)definite (PSD) linear operator.</p> <p>Solvers should always be constructed by a <code>AbstractLinearSolverMethod</code>.</p> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>class AbstractLinearSolver(nnx.Module):\n    \"\"\"\n    Base class for linear solvers.\n\n    These solvers are used to exactly or approximately solve the linear\n    system $Ax = b$ for $x$, where $A$ is a positive (semi-)definite (PSD)\n    linear operator.\n\n    Solvers should always be constructed by a `AbstractLinearSolverMethod`.\n    \"\"\"\n\n    @abstractmethod\n    def solve(self, b: Float[Array, \"N #K\"]) -&gt; Float[Array, \"N #K\"]:\n        \"\"\"Compute a solution to the linear system $Ax = b$.\n\n        Arguments:\n            b: Right-hand side of the linear system.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def logdet(self) -&gt; ScalarFloat:\n        \"\"\"Compute the logarithm of the (pseudo-)determinant of $A$.\"\"\"\n        pass\n\n    @abstractmethod\n    def inv_congruence_transform(\n        self, B: LinearOperator | Float[Array, \"N K\"]\n    ) -&gt; LinearOperator | Float[Array, \"K K\"]:\n        \"\"\"Compute the inverse congruence transform $B^T x$ for $x$ in $Ax = B$.\n\n        Arguments:\n            B: Linear operator or array to be applied.\n\n        Returns:\n            Linear operator or array resulting from the congruence transform.\n        \"\"\"\n        pass\n\n    def inv_quad(self, b: Float[Array, \"N #1\"]) -&gt; ScalarFloat:\n        \"\"\"Compute the inverse quadratic form $b^T x$, for $x$ in $Ax = b$.\n\n        Arguments:\n            b: Right-hand side of the linear system.\n        \"\"\"\n        return self.inv_congruence_transform(b[:, None]).squeeze()\n\n    @abstractmethod\n    def trace_solve(self, B: Self) -&gt; ScalarFloat:\n        r\"\"\"Compute $\\mathrm{trace}(X)$ in $AX=B$ for PSD $B$.\n\n        Arguments:\n            B: An `AbstractLinearSolver` of the same type as `self` representing\n                the PSD linear operator $B$.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.inv_congruence_transform","title":"<code>inv_congruence_transform(B)</code>  <code>abstractmethod</code>","text":"<p>Compute the inverse congruence transform \\(B^T x\\) for \\(x\\) in \\(Ax = B\\).</p> <p>Parameters:</p> Name Type Description Default <code>B</code> <code>LinearOperator | Float[Array, 'N K']</code> <p>Linear operator or array to be applied.</p> required <p>Returns:</p> Type Description <code>LinearOperator | Float[Array, 'K K']</code> <p>Linear operator or array resulting from the congruence transform.</p> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>@abstractmethod\ndef inv_congruence_transform(\n    self, B: LinearOperator | Float[Array, \"N K\"]\n) -&gt; LinearOperator | Float[Array, \"K K\"]:\n    \"\"\"Compute the inverse congruence transform $B^T x$ for $x$ in $Ax = B$.\n\n    Arguments:\n        B: Linear operator or array to be applied.\n\n    Returns:\n        Linear operator or array resulting from the congruence transform.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.inv_quad","title":"<code>inv_quad(b)</code>","text":"<p>Compute the inverse quadratic form \\(b^T x\\), for \\(x\\) in \\(Ax = b\\).</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>Float[Array, N]</code> <p>Right-hand side of the linear system.</p> required Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>def inv_quad(self, b: Float[Array, \"N #1\"]) -&gt; ScalarFloat:\n    \"\"\"Compute the inverse quadratic form $b^T x$, for $x$ in $Ax = b$.\n\n    Arguments:\n        b: Right-hand side of the linear system.\n    \"\"\"\n    return self.inv_congruence_transform(b[:, None]).squeeze()\n</code></pre>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.logdet","title":"<code>logdet()</code>  <code>abstractmethod</code>","text":"<p>Compute the logarithm of the (pseudo-)determinant of \\(A\\).</p> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>@abstractmethod\ndef logdet(self) -&gt; ScalarFloat:\n    \"\"\"Compute the logarithm of the (pseudo-)determinant of $A$.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.solve","title":"<code>solve(b)</code>  <code>abstractmethod</code>","text":"<p>Compute a solution to the linear system \\(Ax = b\\).</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>Float[Array, N]</code> <p>Right-hand side of the linear system.</p> required Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>@abstractmethod\ndef solve(self, b: Float[Array, \"N #K\"]) -&gt; Float[Array, \"N #K\"]:\n    \"\"\"Compute a solution to the linear system $Ax = b$.\n\n    Arguments:\n        b: Right-hand side of the linear system.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.trace_solve","title":"<code>trace_solve(B)</code>  <code>abstractmethod</code>","text":"<p>Compute \\(\\mathrm{trace}(X)\\) in \\(AX=B\\) for PSD \\(B\\).</p> <p>Parameters:</p> Name Type Description Default <code>B</code> <code>Self</code> <p>An <code>AbstractLinearSolver</code> of the same type as <code>self</code> representing the PSD linear operator \\(B\\).</p> required Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>@abstractmethod\ndef trace_solve(self, B: Self) -&gt; ScalarFloat:\n    r\"\"\"Compute $\\mathrm{trace}(X)$ in $AX=B$ for PSD $B$.\n\n    Arguments:\n        B: An `AbstractLinearSolver` of the same type as `self` representing\n            the PSD linear operator $B$.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolverMethod","title":"<code>AbstractLinearSolverMethod</code>","text":"<p>               Bases: <code>Module</code></p> <p>Base class for linear solver methods.</p> <p>These methods are used to construct <code>AbstractLinearSolver</code> instances.</p> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>class AbstractLinearSolverMethod(nnx.Module):\n    \"\"\"\n    Base class for linear solver methods.\n\n    These methods are used to construct `AbstractLinearSolver` instances.\n    \"\"\"\n\n    @abstractmethod\n    def __call__(self, A: LinearOperator) -&gt; AbstractLinearSolver:\n        \"\"\"Construct a solver from the positive (semi-)definite linear operator.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolverMethod.__call__","title":"<code>__call__(A)</code>  <code>abstractmethod</code>","text":"<p>Construct a solver from the positive (semi-)definite linear operator.</p> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>@abstractmethod\ndef __call__(self, A: LinearOperator) -&gt; AbstractLinearSolver:\n    \"\"\"Construct a solver from the positive (semi-)definite linear operator.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.Cholesky","title":"<code>Cholesky</code>","text":"<p>               Bases: <code>AbstractLinearSolverMethod</code></p> <p>Solve a linear system using the Cholesky decomposition.</p> <p>Due to numerical imprecision, Cholesky factorization may fail even for positive-definite \\(A\\). Optionally, a small amount of <code>jitter</code> (\\(\\epsilon\\)) can be added to \\(A\\) to ensure positive-definiteness. Note that the resulting system solved is slightly different from the original system.</p> <p>Attributes:</p> Name Type Description <code>jitter</code> <code>ScalarFloat | None</code> <p>Small amount of jitter to add to \\(A\\) to ensure positive-definiteness.</p> Source code in <code>src/cagpjax/solvers/cholesky.py</code> <pre><code>class Cholesky(AbstractLinearSolverMethod):\n    \"\"\"\n    Solve a linear system using the Cholesky decomposition.\n\n    Due to numerical imprecision, Cholesky factorization may fail even for\n    positive-definite $A$. Optionally, a small amount of `jitter` ($\\\\epsilon$) can\n    be added to $A$ to ensure positive-definiteness. Note that the resulting system\n    solved is slightly different from the original system.\n\n    Attributes:\n        jitter: Small amount of jitter to add to $A$ to ensure positive-definiteness.\n    \"\"\"\n\n    jitter: ScalarFloat | None\n\n    def __init__(self, jitter: ScalarFloat | None = None):\n        self.jitter = jitter\n\n    @override\n    def __call__(self, A: LinearOperator) -&gt; AbstractLinearSolver:\n        return CholeskySolver(A, jitter=self.jitter)\n</code></pre>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.PseudoInverse","title":"<code>PseudoInverse</code>","text":"<p>               Bases: <code>AbstractLinearSolverMethod</code></p> <p>Solve a linear system using the Moore-Penrose pseudoinverse.</p> <p>This solver computes the least-squares solution \\(x = A^+ b\\) for any \\(A\\), where \\(A^+\\) is the Moore-Penrose pseudoinverse. This is equivalent to the exact solution for non-singular \\(A\\) but generalizes to singular \\(A\\) and improves stability for almost-singular \\(A\\); note, however, that if the rank of \\(A\\) is dependent on hyperparameters being optimized, because the pseudoinverse is discontinuous, the optimization problem may be ill-posed.</p> <p>Note that if \\(A\\) is (almost-)degenerate (some eigenvalues repeat), then the gradient of its solves in JAX may be non-computable or numerically unstable (see jax#669). For degenerate operators, it may be necessary to increase <code>grad_rtol</code> to improve stability of gradients. See <code>cagpjax.linalg.eigh</code> for more details.</p> <p>Attributes:</p> Name Type Description <code>rtol</code> <code>ScalarFloat | None</code> <p>Specifies the cutoff for small eigenvalues.   Eigenvalues smaller than <code>rtol * largest_nonzero_eigenvalue</code> are treated as zero.   The default is determined based on the floating point precision of the dtype   of the operator (see <code>jax.numpy.linalg.pinv</code>).</p> <code>grad_rtol</code> <code>float | None</code> <p>Specifies the cutoff for similar eigenvalues, used to improve gradient computation for (almost-)degenerate matrices. If not provided, the default is 0.0. If None or negative, all eigenvalues are treated as distinct.</p> <code>alg</code> <code>Algorithm</code> <p>Algorithm for eigenvalue decomposition passed to <code>cagpjax.linalg.eigh</code>.</p> Source code in <code>src/cagpjax/solvers/pseudoinverse.py</code> <pre><code>class PseudoInverse(AbstractLinearSolverMethod):\n    \"\"\"\n    Solve a linear system using the Moore-Penrose pseudoinverse.\n\n    This solver computes the least-squares solution $x = A^+ b$ for any $A$,\n    where $A^+$ is the Moore-Penrose pseudoinverse. This is equivalent to\n    the exact solution for non-singular $A$ but generalizes to singular $A$\n    and improves stability for almost-singular $A$; note, however, that if the\n    rank of $A$ is dependent on hyperparameters being optimized, because the\n    pseudoinverse is discontinuous, the optimization problem may be ill-posed.\n\n    Note that if $A$ is (almost-)degenerate (some eigenvalues repeat), then\n    the gradient of its solves in JAX may be non-computable or numerically unstable\n    (see [jax#669](https://github.com/jax-ml/jax/issues/669)).\n    For degenerate operators, it may be necessary to increase `grad_rtol` to improve\n    stability of gradients.\n    See [`cagpjax.linalg.eigh`][] for more details.\n\n    Attributes:\n        rtol: Specifies the cutoff for small eigenvalues.\n              Eigenvalues smaller than `rtol * largest_nonzero_eigenvalue` are treated as zero.\n              The default is determined based on the floating point precision of the dtype\n              of the operator (see [`jax.numpy.linalg.pinv`][]).\n        grad_rtol: Specifies the cutoff for similar eigenvalues, used to improve\n            gradient computation for (almost-)degenerate matrices.\n            If not provided, the default is 0.0.\n            If None or negative, all eigenvalues are treated as distinct.\n        alg: Algorithm for eigenvalue decomposition passed to [`cagpjax.linalg.eigh`][].\n    \"\"\"\n\n    rtol: ScalarFloat | None\n    grad_rtol: float | None\n    alg: cola.linalg.Algorithm\n\n    def __init__(\n        self,\n        rtol: ScalarFloat | None = None,\n        grad_rtol: float | None = None,\n        alg: cola.linalg.Algorithm = Eigh(),\n    ):\n        self.rtol = rtol\n        self.grad_rtol = grad_rtol\n        self.alg = alg\n\n    @override\n    def __call__(self, A: LinearOperator) -&gt; AbstractLinearSolver:\n        return PseudoInverseSolver(\n            A, rtol=self.rtol, grad_rtol=self.grad_rtol, alg=self.alg\n        )\n</code></pre>"},{"location":"reference/cagpjax/solvers/base/","title":"base","text":""},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base","title":"<code>cagpjax.solvers.base</code>","text":"<p>Base classes for linear solvers and methods.</p>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver","title":"<code>AbstractLinearSolver</code>","text":"<p>               Bases: <code>Module</code></p> <p>Base class for linear solvers.</p> <p>These solvers are used to exactly or approximately solve the linear system \\(Ax = b\\) for \\(x\\), where \\(A\\) is a positive (semi-)definite (PSD) linear operator.</p> <p>Solvers should always be constructed by a <code>AbstractLinearSolverMethod</code>.</p> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>class AbstractLinearSolver(nnx.Module):\n    \"\"\"\n    Base class for linear solvers.\n\n    These solvers are used to exactly or approximately solve the linear\n    system $Ax = b$ for $x$, where $A$ is a positive (semi-)definite (PSD)\n    linear operator.\n\n    Solvers should always be constructed by a `AbstractLinearSolverMethod`.\n    \"\"\"\n\n    @abstractmethod\n    def solve(self, b: Float[Array, \"N #K\"]) -&gt; Float[Array, \"N #K\"]:\n        \"\"\"Compute a solution to the linear system $Ax = b$.\n\n        Arguments:\n            b: Right-hand side of the linear system.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def logdet(self) -&gt; ScalarFloat:\n        \"\"\"Compute the logarithm of the (pseudo-)determinant of $A$.\"\"\"\n        pass\n\n    @abstractmethod\n    def inv_congruence_transform(\n        self, B: LinearOperator | Float[Array, \"N K\"]\n    ) -&gt; LinearOperator | Float[Array, \"K K\"]:\n        \"\"\"Compute the inverse congruence transform $B^T x$ for $x$ in $Ax = B$.\n\n        Arguments:\n            B: Linear operator or array to be applied.\n\n        Returns:\n            Linear operator or array resulting from the congruence transform.\n        \"\"\"\n        pass\n\n    def inv_quad(self, b: Float[Array, \"N #1\"]) -&gt; ScalarFloat:\n        \"\"\"Compute the inverse quadratic form $b^T x$, for $x$ in $Ax = b$.\n\n        Arguments:\n            b: Right-hand side of the linear system.\n        \"\"\"\n        return self.inv_congruence_transform(b[:, None]).squeeze()\n\n    @abstractmethod\n    def trace_solve(self, B: Self) -&gt; ScalarFloat:\n        r\"\"\"Compute $\\mathrm{trace}(X)$ in $AX=B$ for PSD $B$.\n\n        Arguments:\n            B: An `AbstractLinearSolver` of the same type as `self` representing\n                the PSD linear operator $B$.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.inv_congruence_transform","title":"<code>inv_congruence_transform(B)</code>  <code>abstractmethod</code>","text":"<p>Compute the inverse congruence transform \\(B^T x\\) for \\(x\\) in \\(Ax = B\\).</p> <p>Parameters:</p> Name Type Description Default <code>B</code> <code>LinearOperator | Float[Array, 'N K']</code> <p>Linear operator or array to be applied.</p> required <p>Returns:</p> Type Description <code>LinearOperator | Float[Array, 'K K']</code> <p>Linear operator or array resulting from the congruence transform.</p> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>@abstractmethod\ndef inv_congruence_transform(\n    self, B: LinearOperator | Float[Array, \"N K\"]\n) -&gt; LinearOperator | Float[Array, \"K K\"]:\n    \"\"\"Compute the inverse congruence transform $B^T x$ for $x$ in $Ax = B$.\n\n    Arguments:\n        B: Linear operator or array to be applied.\n\n    Returns:\n        Linear operator or array resulting from the congruence transform.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.inv_quad","title":"<code>inv_quad(b)</code>","text":"<p>Compute the inverse quadratic form \\(b^T x\\), for \\(x\\) in \\(Ax = b\\).</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>Float[Array, N]</code> <p>Right-hand side of the linear system.</p> required Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>def inv_quad(self, b: Float[Array, \"N #1\"]) -&gt; ScalarFloat:\n    \"\"\"Compute the inverse quadratic form $b^T x$, for $x$ in $Ax = b$.\n\n    Arguments:\n        b: Right-hand side of the linear system.\n    \"\"\"\n    return self.inv_congruence_transform(b[:, None]).squeeze()\n</code></pre>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.logdet","title":"<code>logdet()</code>  <code>abstractmethod</code>","text":"<p>Compute the logarithm of the (pseudo-)determinant of \\(A\\).</p> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>@abstractmethod\ndef logdet(self) -&gt; ScalarFloat:\n    \"\"\"Compute the logarithm of the (pseudo-)determinant of $A$.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.solve","title":"<code>solve(b)</code>  <code>abstractmethod</code>","text":"<p>Compute a solution to the linear system \\(Ax = b\\).</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>Float[Array, N]</code> <p>Right-hand side of the linear system.</p> required Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>@abstractmethod\ndef solve(self, b: Float[Array, \"N #K\"]) -&gt; Float[Array, \"N #K\"]:\n    \"\"\"Compute a solution to the linear system $Ax = b$.\n\n    Arguments:\n        b: Right-hand side of the linear system.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.trace_solve","title":"<code>trace_solve(B)</code>  <code>abstractmethod</code>","text":"<p>Compute \\(\\mathrm{trace}(X)\\) in \\(AX=B\\) for PSD \\(B\\).</p> <p>Parameters:</p> Name Type Description Default <code>B</code> <code>Self</code> <p>An <code>AbstractLinearSolver</code> of the same type as <code>self</code> representing the PSD linear operator \\(B\\).</p> required Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>@abstractmethod\ndef trace_solve(self, B: Self) -&gt; ScalarFloat:\n    r\"\"\"Compute $\\mathrm{trace}(X)$ in $AX=B$ for PSD $B$.\n\n    Arguments:\n        B: An `AbstractLinearSolver` of the same type as `self` representing\n            the PSD linear operator $B$.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolverMethod","title":"<code>AbstractLinearSolverMethod</code>","text":"<p>               Bases: <code>Module</code></p> <p>Base class for linear solver methods.</p> <p>These methods are used to construct <code>AbstractLinearSolver</code> instances.</p> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>class AbstractLinearSolverMethod(nnx.Module):\n    \"\"\"\n    Base class for linear solver methods.\n\n    These methods are used to construct `AbstractLinearSolver` instances.\n    \"\"\"\n\n    @abstractmethod\n    def __call__(self, A: LinearOperator) -&gt; AbstractLinearSolver:\n        \"\"\"Construct a solver from the positive (semi-)definite linear operator.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolverMethod.__call__","title":"<code>__call__(A)</code>  <code>abstractmethod</code>","text":"<p>Construct a solver from the positive (semi-)definite linear operator.</p> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>@abstractmethod\ndef __call__(self, A: LinearOperator) -&gt; AbstractLinearSolver:\n    \"\"\"Construct a solver from the positive (semi-)definite linear operator.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/cholesky/","title":"cholesky","text":""},{"location":"reference/cagpjax/solvers/cholesky/#cagpjax.solvers.cholesky","title":"<code>cagpjax.solvers.cholesky</code>","text":"<p>Linear solvers based on Cholesky decomposition.</p>"},{"location":"reference/cagpjax/solvers/cholesky/#cagpjax.solvers.cholesky.Cholesky","title":"<code>Cholesky</code>","text":"<p>               Bases: <code>AbstractLinearSolverMethod</code></p> <p>Solve a linear system using the Cholesky decomposition.</p> <p>Due to numerical imprecision, Cholesky factorization may fail even for positive-definite \\(A\\). Optionally, a small amount of <code>jitter</code> (\\(\\epsilon\\)) can be added to \\(A\\) to ensure positive-definiteness. Note that the resulting system solved is slightly different from the original system.</p> <p>Attributes:</p> Name Type Description <code>jitter</code> <code>ScalarFloat | None</code> <p>Small amount of jitter to add to \\(A\\) to ensure positive-definiteness.</p> Source code in <code>src/cagpjax/solvers/cholesky.py</code> <pre><code>class Cholesky(AbstractLinearSolverMethod):\n    \"\"\"\n    Solve a linear system using the Cholesky decomposition.\n\n    Due to numerical imprecision, Cholesky factorization may fail even for\n    positive-definite $A$. Optionally, a small amount of `jitter` ($\\\\epsilon$) can\n    be added to $A$ to ensure positive-definiteness. Note that the resulting system\n    solved is slightly different from the original system.\n\n    Attributes:\n        jitter: Small amount of jitter to add to $A$ to ensure positive-definiteness.\n    \"\"\"\n\n    jitter: ScalarFloat | None\n\n    def __init__(self, jitter: ScalarFloat | None = None):\n        self.jitter = jitter\n\n    @override\n    def __call__(self, A: LinearOperator) -&gt; AbstractLinearSolver:\n        return CholeskySolver(A, jitter=self.jitter)\n</code></pre>"},{"location":"reference/cagpjax/solvers/cholesky/#cagpjax.solvers.cholesky.CholeskySolver","title":"<code>CholeskySolver</code>","text":"<p>               Bases: <code>AbstractLinearSolver</code></p> <p>Solve a linear system by computing the Cholesky decomposition.</p> Source code in <code>src/cagpjax/solvers/cholesky.py</code> <pre><code>class CholeskySolver(AbstractLinearSolver):\n    \"\"\"\n    Solve a linear system by computing the Cholesky decomposition.\n    \"\"\"\n\n    lchol: LinearOperator\n\n    def __init__(self, A: LinearOperator, jitter: ScalarFloat | None = None):\n        self.lchol = lower_cholesky(A, jitter)\n\n    @override\n    def solve(self, b: Float[Array, \"N #K\"]) -&gt; Float[Array, \"N #K\"]:\n        Linv = cola.linalg.inv(self.lchol)\n        return Linv.T @ (Linv @ b)\n\n    @override\n    def logdet(self) -&gt; ScalarFloat:\n        return 2 * jnp.sum(jnp.log(cola.linalg.diag(self.lchol)))\n\n    @override\n    def inv_congruence_transform(\n        self, B: LinearOperator | Float[Array, \"K N\"]\n    ) -&gt; LinearOperator | Float[Array, \"K K\"]:\n        Linv = cola.linalg.inv(self.lchol)\n        right_term = Linv @ B\n        return right_term.T @ right_term\n\n    @override\n    def trace_solve(self, B: Self) -&gt; ScalarFloat:\n        L = cola.linalg.inv(self.lchol) @ B.lchol.to_dense()\n        return jnp.sum(jnp.square(L))\n</code></pre>"},{"location":"reference/cagpjax/solvers/pseudoinverse/","title":"pseudoinverse","text":""},{"location":"reference/cagpjax/solvers/pseudoinverse/#cagpjax.solvers.pseudoinverse","title":"<code>cagpjax.solvers.pseudoinverse</code>","text":""},{"location":"reference/cagpjax/solvers/pseudoinverse/#cagpjax.solvers.pseudoinverse.PseudoInverse","title":"<code>PseudoInverse</code>","text":"<p>               Bases: <code>AbstractLinearSolverMethod</code></p> <p>Solve a linear system using the Moore-Penrose pseudoinverse.</p> <p>This solver computes the least-squares solution \\(x = A^+ b\\) for any \\(A\\), where \\(A^+\\) is the Moore-Penrose pseudoinverse. This is equivalent to the exact solution for non-singular \\(A\\) but generalizes to singular \\(A\\) and improves stability for almost-singular \\(A\\); note, however, that if the rank of \\(A\\) is dependent on hyperparameters being optimized, because the pseudoinverse is discontinuous, the optimization problem may be ill-posed.</p> <p>Note that if \\(A\\) is (almost-)degenerate (some eigenvalues repeat), then the gradient of its solves in JAX may be non-computable or numerically unstable (see jax#669). For degenerate operators, it may be necessary to increase <code>grad_rtol</code> to improve stability of gradients. See <code>cagpjax.linalg.eigh</code> for more details.</p> <p>Attributes:</p> Name Type Description <code>rtol</code> <code>ScalarFloat | None</code> <p>Specifies the cutoff for small eigenvalues.   Eigenvalues smaller than <code>rtol * largest_nonzero_eigenvalue</code> are treated as zero.   The default is determined based on the floating point precision of the dtype   of the operator (see <code>jax.numpy.linalg.pinv</code>).</p> <code>grad_rtol</code> <code>float | None</code> <p>Specifies the cutoff for similar eigenvalues, used to improve gradient computation for (almost-)degenerate matrices. If not provided, the default is 0.0. If None or negative, all eigenvalues are treated as distinct.</p> <code>alg</code> <code>Algorithm</code> <p>Algorithm for eigenvalue decomposition passed to <code>cagpjax.linalg.eigh</code>.</p> Source code in <code>src/cagpjax/solvers/pseudoinverse.py</code> <pre><code>class PseudoInverse(AbstractLinearSolverMethod):\n    \"\"\"\n    Solve a linear system using the Moore-Penrose pseudoinverse.\n\n    This solver computes the least-squares solution $x = A^+ b$ for any $A$,\n    where $A^+$ is the Moore-Penrose pseudoinverse. This is equivalent to\n    the exact solution for non-singular $A$ but generalizes to singular $A$\n    and improves stability for almost-singular $A$; note, however, that if the\n    rank of $A$ is dependent on hyperparameters being optimized, because the\n    pseudoinverse is discontinuous, the optimization problem may be ill-posed.\n\n    Note that if $A$ is (almost-)degenerate (some eigenvalues repeat), then\n    the gradient of its solves in JAX may be non-computable or numerically unstable\n    (see [jax#669](https://github.com/jax-ml/jax/issues/669)).\n    For degenerate operators, it may be necessary to increase `grad_rtol` to improve\n    stability of gradients.\n    See [`cagpjax.linalg.eigh`][] for more details.\n\n    Attributes:\n        rtol: Specifies the cutoff for small eigenvalues.\n              Eigenvalues smaller than `rtol * largest_nonzero_eigenvalue` are treated as zero.\n              The default is determined based on the floating point precision of the dtype\n              of the operator (see [`jax.numpy.linalg.pinv`][]).\n        grad_rtol: Specifies the cutoff for similar eigenvalues, used to improve\n            gradient computation for (almost-)degenerate matrices.\n            If not provided, the default is 0.0.\n            If None or negative, all eigenvalues are treated as distinct.\n        alg: Algorithm for eigenvalue decomposition passed to [`cagpjax.linalg.eigh`][].\n    \"\"\"\n\n    rtol: ScalarFloat | None\n    grad_rtol: float | None\n    alg: cola.linalg.Algorithm\n\n    def __init__(\n        self,\n        rtol: ScalarFloat | None = None,\n        grad_rtol: float | None = None,\n        alg: cola.linalg.Algorithm = Eigh(),\n    ):\n        self.rtol = rtol\n        self.grad_rtol = grad_rtol\n        self.alg = alg\n\n    @override\n    def __call__(self, A: LinearOperator) -&gt; AbstractLinearSolver:\n        return PseudoInverseSolver(\n            A, rtol=self.rtol, grad_rtol=self.grad_rtol, alg=self.alg\n        )\n</code></pre>"},{"location":"reference/cagpjax/solvers/pseudoinverse/#cagpjax.solvers.pseudoinverse.PseudoInverseSolver","title":"<code>PseudoInverseSolver</code>","text":"<p>               Bases: <code>AbstractLinearSolver</code></p> <p>Solve a linear system using the Moore-Penrose pseudoinverse.</p> Source code in <code>src/cagpjax/solvers/pseudoinverse.py</code> <pre><code>class PseudoInverseSolver(AbstractLinearSolver):\n    \"\"\"\n    Solve a linear system using the Moore-Penrose pseudoinverse.\n    \"\"\"\n\n    A: LinearOperator\n    eigh_result: EighResult\n    eigenvalues_safe: Float[Array, \"N\"]\n\n    def __init__(\n        self,\n        A: LinearOperator,\n        rtol: ScalarFloat | None = None,\n        grad_rtol: float | None = None,\n        alg: cola.linalg.Algorithm = Eigh(),\n    ):\n        n = A.shape[0]\n        # select rtol using same heuristic as jax.numpy.linalg.lstsq\n        if rtol is None:\n            rtol = float(jnp.finfo(A.dtype).eps) * n\n        self.eigh_result = eigh(A, alg=alg, grad_rtol=grad_rtol)\n        svdmax = jnp.max(jnp.abs(self.eigh_result.eigenvalues))\n        cutoff = jnp.array(rtol * svdmax, dtype=svdmax.dtype)\n        mask = self.eigh_result.eigenvalues &gt;= cutoff\n        self.eigvals_safe = jnp.where(mask, self.eigh_result.eigenvalues, 1)\n        self.eigvals_inv = jnp.where(mask, jnp.reciprocal(self.eigvals_safe), 0)\n        self.A = A\n\n    @override\n    def solve(self, b: Float[Array, \"N #K\"]) -&gt; Float[Array, \"N #K\"]:\n        # return jnp.linalg.lstsq(self.A.to_dense(), b)[0]\n        b_ndim = b.ndim\n        b = b if b_ndim == 2 else b[:, None]\n        with jax.default_matmul_precision(\"highest\"):\n            x = self.eigh_result.eigenvectors.T @ b\n        x = x * self.eigvals_inv[:, None]\n        with jax.default_matmul_precision(\"highest\"):\n            x = self.eigh_result.eigenvectors @ x\n        x = x if b_ndim == 2 else x.squeeze(axis=1)\n        return x\n\n    @override\n    def logdet(self) -&gt; ScalarFloat:\n        return jnp.sum(jnp.log(self.eigvals_safe))\n\n    @override\n    def inv_quad(self, b: Float[Array, \"N #1\"]) -&gt; ScalarFloat:\n        z = self.eigh_result.eigenvectors.T @ b\n        return jnp.dot(jnp.square(z), self.eigvals_inv).squeeze()\n\n    @override\n    def inv_congruence_transform(\n        self, B: LinearOperator | Float[Array, \"K N\"]\n    ) -&gt; LinearOperator | Float[Array, \"K K\"]:\n        eigenvectors = self.eigh_result.eigenvectors\n        z = eigenvectors.T @ B\n        z = z.T @ cola.ops.Diagonal(self.eigvals_inv) @ z\n        return z\n\n    @override\n    def trace_solve(self, B: Self) -&gt; ScalarFloat:\n        if isinstance(B.eigh_result.eigenvectors, cola.ops.Dense):\n            vectors_mat = self.eigh_result.eigenvectors.to_dense()\n            return jnp.einsum(\n                \"ij,j,kj,ik\",\n                vectors_mat,\n                self.eigvals_inv,\n                vectors_mat,\n                B.A.to_dense(),\n            )\n        else:\n            W = B.eigh_result.eigenvectors.T @ self.eigh_result.eigenvectors.to_dense()\n            return jnp.einsum(\n                \"ij,j,ij,i\", W, self.eigvals_inv, W, B.eigh_result.eigenvalues\n            )\n</code></pre>"}]}