{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"CAGPJax","text":"<p>Computation-Aware Gaussian Processes (CaGPs) for JAX</p> <p>CAGPJax provides efficient Gaussian processes by leveraging structured kernel approximations and sparse matrix operations, built on JAX and GPJax.</p> <p>For \\(n\\) data-points, the computational cost of exact Gaussian processes scales as \\(\\mathcal{O}(n^3)\\) due to matrix inversions, while the memory requirements scale as \\(\\mathcal{O}(n^2)\\). CaGPs project the data to a \\(k(\\ll n)\\)-dimensional subspace to perform inference, reducing the computational cost to \\(\\mathcal{O}(n^2k)\\) and the memory requirements to \\(\\mathcal{O}(nk)\\). Using sparse projections further reduces the computational cost to \\(\\mathcal{O}(n^2)\\).</p> <p>Compared to other apprximate GP inference approaches such as inducing point methods, the prediction uncertainty of CaGPs accounts for the additional uncertainty due to only observing a subspace of the data.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install git+https://github.com/sethaxen/CAGPJax.git\n</code></pre>"},{"location":"#citation","title":"Citation","text":"<p>There's not yet a citation for this package. If using the code, please cite</p> <pre><code>@inproceedings{wenger2022itergp,\n  title         = {Posterior and Computational Uncertainty in {G}aussian Processes},\n  author        = {Wenger, Jonathan and Pleiss, Geoff and Pf\\\"{o}rtner, Marvin and Hennig, Philipp and Cunningham, John P},\n  year          = 2022,\n  booktitle     = {Advances in Neural Information Processing Systems},\n  publisher     = {Curran Associates, Inc.},\n  volume        = 35,\n  pages         = {10876--10890},\n  url           = {https://proceedings.neurips.cc/paper_files/paper/2022/file/4683beb6bab325650db13afd05d1a14a-Paper-Conference.pdf},\n  editor        = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},\n  eprint        = {2205.15449},\n  archiveprefix = {arXiv},\n  primaryclass  = {cs.LG}\n}\n@inproceedings{wenger2024cagp,\n  title         = {Computation-Aware {G}aussian Processes: Model Selection And Linear-Time Inference},\n  author        = {Wenger, Jonathan and Wu, Kaiwen and Hennig, Philipp and Gardner, Jacob R. and Pleiss, Geoff and Cunningham, John P.},\n  year          = 2024,\n  booktitle     = {Advances in Neural Information Processing Systems},\n  publisher     = {Curran Associates, Inc.},\n  volume        = 37,\n  pages         = {31316--31349},\n  url           = {https://proceedings.neurips.cc/paper_files/paper/2024/file/379ea6eb0faad176b570c2e26d58ff2b-Paper-Conference.pdf},\n  editor        = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},\n  eprint        = {2411.01036},\n  archiveprefix = {arXiv},\n  primaryclass  = {cs.LG}\n}\n</code></pre>"},{"location":"#api-reference","title":"API Reference","text":"<p>See the Reference section for detailed API documentation.</p>"},{"location":"reference/cagpjax/","title":"cagpjax","text":""},{"location":"reference/cagpjax/#cagpjax","title":"cagpjax","text":"<p>Computation-Aware Gaussian Processes for GPJax.</p> <p>Modules:</p> <ul> <li> <code>computations</code>           \u2013            <p>Kernel computation methods.</p> </li> <li> <code>distributions</code>           \u2013            </li> <li> <code>linalg</code>           \u2013            <p>Linear algebra functions.</p> </li> <li> <code>models</code>           \u2013            <p>Gaussian process models.</p> </li> <li> <code>operators</code>           \u2013            <p>Custom linear operators.</p> </li> <li> <code>policies</code>           \u2013            </li> <li> <code>solvers</code>           \u2013            </li> </ul> <p>Classes:</p> <ul> <li> <code>BlockDiagonalSparse</code>           \u2013            <p>Block-diagonal sparse linear operator.</p> </li> <li> <code>BlockSparsePolicy</code>           \u2013            <p>Block-sparse linear solver policy.</p> </li> <li> <code>ComputationAwareGP</code>           \u2013            <p>Computation-aware Gaussian Process model.</p> </li> <li> <code>LanczosPolicy</code>           \u2013            <p>Lanczos-based policy for eigenvalue decomposition approximation.</p> </li> </ul>"},{"location":"reference/cagpjax/#cagpjax.BlockDiagonalSparse","title":"BlockDiagonalSparse","text":"<pre><code>BlockDiagonalSparse(nz_values: Float[Array, N], n_blocks: int)\n</code></pre> <p>               Bases: <code>LinearOperator</code></p> <p>Block-diagonal sparse linear operator.</p> <p>This operator represents a block-diagonal matrix structure where the blocks are contiguous, and each contains a column vector, so that exactly one value is non-zero in each row.</p> <p>Parameters:</p>"},{"location":"reference/cagpjax/#cagpjax.BlockDiagonalSparse(nz_values)","title":"<code>nz_values</code>","text":"(<code>Float[Array, N]</code>)           \u2013            <p>Non-zero values to be distributed across diagonal blocks.</p>"},{"location":"reference/cagpjax/#cagpjax.BlockDiagonalSparse(n_blocks)","title":"<code>n_blocks</code>","text":"(<code>int</code>)           \u2013            <p>Number of diagonal blocks in the matrix.</p>"},{"location":"reference/cagpjax/#cagpjax.BlockDiagonalSparse--examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; from cagpjax.operators import BlockDiagonalSparse\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create a 3x6 block-diagonal matrix with 3 blocks\n&gt;&gt;&gt; nz_values = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n&gt;&gt;&gt; op = BlockDiagonalSparse(nz_values, n_blocks=3)\n&gt;&gt;&gt; print(op.shape)\n(6, 3)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Apply to identity matrices\n&gt;&gt;&gt; op @ jnp.eye(3)\nArray([[1., 0., 0.],\n       [2., 0., 0.],\n       [0., 3., 0.],\n       [0., 4., 0.],\n       [0., 0., 5.],\n       [0., 0., 6.]], dtype=float32)\n</code></pre> Source code in <code>src/cagpjax/operators/block_diagonal_sparse.py</code> <pre><code>def __init__(self, nz_values: Float[Array, \"N\"], n_blocks: int):\n    n = nz_values.shape[0]\n    super().__init__(nz_values.dtype, (n, n_blocks), annotations={ScaledOrthogonal})\n    self.nz_values = nz_values\n</code></pre>"},{"location":"reference/cagpjax/#cagpjax.BlockSparsePolicy","title":"BlockSparsePolicy","text":"<pre><code>BlockSparsePolicy(n_actions: int, n: int | None = None, nz_values: Float[Array, N] | Variable[Float[Array, N]] | None = None, key: PRNGKeyArray | None = None, **kwargs)\n</code></pre> <p>               Bases: <code>AbstractBatchLinearSolverPolicy</code></p> <p>Block-sparse linear solver policy.</p> <p>This policy uses a fixed block-diagonal sparse structure to define independent learnable actions. The matrix has the following structure:</p> \\[ S = \\begin{bmatrix}     s_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\     0 &amp; s_2 &amp; \\cdots &amp; 0 \\\\     \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\     0 &amp; 0 &amp; \\cdots &amp; s_{\\text{n_actions}} \\end{bmatrix} \\] <p>These are stacked and stored as a single trainable parameter <code>nz_values</code>.</p> <p>Initialize the block sparse policy.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>to_actions</code>             \u2013              <p>Convert to block diagonal sparse action operators.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>n_actions</code>               (<code>int</code>)           \u2013            <p>Number of actions to be used.</p> </li> </ul> Source code in <code>src/cagpjax/policies/block_sparse.py</code> <pre><code>def __init__(\n    self,\n    n_actions: int,\n    n: int | None = None,\n    nz_values: Float[Array, \"N\"] | nnx.Variable[Float[Array, \"N\"]] | None = None,\n    key: PRNGKeyArray | None = None,\n    **kwargs,\n):\n    \"\"\"Initialize the block sparse policy.\n\n    Args:\n        n_actions: Number of actions to use.\n        n: Number of rows and columns of the full operator. Must be provided if ``nz_values`` is\n            not provided.\n        nz_values: Non-zero values of the block-diagonal sparse matrix (shape ``(n,)``). If not\n            provided, random actions are sampled using the key if provided.\n        key: Random key for sampling actions if ``nz_values`` is not provided.\n        **kwargs: Additional keyword arguments for ``jax.random.normal`` (e.g. ``dtype``)\n    \"\"\"\n    if nz_values is None:\n        if n is None:\n            raise ValueError(\"n must be provided if nz_values is not provided\")\n        if key is None:\n            key = jax.random.PRNGKey(0)\n        block_size = n // n_actions\n        nz_values = jax.random.normal(key, (n,), **kwargs)\n        nz_values /= jnp.sqrt(block_size)\n    elif n is not None:\n        warnings.warn(\"n is ignored because nz_values is provided\")\n\n    if not isinstance(nz_values, nnx.Variable):\n        nz_values = Real(nz_values)\n\n    self.nz_values: nnx.Variable[Float[Array, \"N\"]] = nz_values\n    self._n_actions: int = n_actions\n</code></pre>"},{"location":"reference/cagpjax/#cagpjax.BlockSparsePolicy(n_actions)","title":"<code>n_actions</code>","text":"(<code>int</code>)           \u2013            <p>Number of actions to use.</p>"},{"location":"reference/cagpjax/#cagpjax.BlockSparsePolicy(n)","title":"<code>n</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of rows and columns of the full operator. Must be provided if <code>nz_values</code> is not provided.</p>"},{"location":"reference/cagpjax/#cagpjax.BlockSparsePolicy(nz_values)","title":"<code>nz_values</code>","text":"(<code>Float[Array, N] | Variable[Float[Array, N]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Non-zero values of the block-diagonal sparse matrix (shape <code>(n,)</code>). If not provided, random actions are sampled using the key if provided.</p>"},{"location":"reference/cagpjax/#cagpjax.BlockSparsePolicy(key)","title":"<code>key</code>","text":"(<code>PRNGKeyArray | None</code>, default:                   <code>None</code> )           \u2013            <p>Random key for sampling actions if <code>nz_values</code> is not provided.</p>"},{"location":"reference/cagpjax/#cagpjax.BlockSparsePolicy(**kwargs)","title":"<code>**kwargs</code>","text":"\u2013            <p>Additional keyword arguments for <code>jax.random.normal</code> (e.g. <code>dtype</code>)</p>"},{"location":"reference/cagpjax/#cagpjax.BlockSparsePolicy.n_actions","title":"n_actions  <code>property</code>","text":"<pre><code>n_actions: int\n</code></pre> <p>Number of actions to be used.</p>"},{"location":"reference/cagpjax/#cagpjax.BlockSparsePolicy.to_actions","title":"to_actions","text":"<pre><code>to_actions(A: LinearOperator) -&gt; LinearOperator\n</code></pre> <p>Convert to block diagonal sparse action operators.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>BlockDiagonalSparse</code> (              <code>LinearOperator</code> )          \u2013            <p>Sparse action structure representing the blocks.</p> </li> </ul> Source code in <code>src/cagpjax/policies/block_sparse.py</code> <pre><code>@override\ndef to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n    \"\"\"Convert to block diagonal sparse action operators.\n\n    Args:\n        A: Linear operator (unused).\n\n    Returns:\n        BlockDiagonalSparse: Sparse action structure representing the blocks.\n    \"\"\"\n    return BlockDiagonalSparse(self.nz_values[...], self.n_actions)\n</code></pre>"},{"location":"reference/cagpjax/#cagpjax.BlockSparsePolicy.to_actions(A)","title":"<code>A</code>","text":"(<code>LinearOperator</code>)           \u2013            <p>Linear operator (unused).</p>"},{"location":"reference/cagpjax/#cagpjax.ComputationAwareGP","title":"ComputationAwareGP","text":"<pre><code>ComputationAwareGP(posterior: ConjugatePosterior, policy: AbstractBatchLinearSolverPolicy, solver: AbstractLinearSolver[_LinearSolverState] = Cholesky(1e-06))\n</code></pre> <p>               Bases: <code>Module</code>, <code>Generic[_LinearSolverState]</code></p> <p>Computation-aware Gaussian Process model.</p> <p>This model implements scalable GP inference by using batch linear solver policies to project the kernel and data to a lower-dimensional subspace, while accounting for the extra uncertainty imposed by observing only this subspace.</p> <p>Attributes:</p> <ul> <li> <code>posterior</code>               (<code>ConjugatePosterior</code>)           \u2013            <p>The original (exact) posterior.</p> </li> <li> <code>policy</code>               (<code>AbstractBatchLinearSolverPolicy</code>)           \u2013            <p>The batch linear solver policy.</p> </li> <li> <code>solver</code>               (<code>AbstractLinearSolver[_LinearSolverState]</code>)           \u2013            <p>The linear solver method to use for solving linear systems with positive semi-definite operators.</p> </li> </ul> Notes <ul> <li>Only single-output models are currently supported.</li> </ul> <p>Initialize the Computation-Aware GP model.</p> <p>Parameters:</p> <ul> <li> </li> <li> </li> <li> </li> </ul> <p>Methods:</p> <ul> <li> <code>elbo</code>             \u2013              <p>Compute the evidence lower bound.</p> </li> <li> <code>init</code>             \u2013              <p>Compute the state of the conditioned GP posterior.</p> </li> <li> <code>predict</code>             \u2013              <p>Compute the predictive distribution of the GP at the test inputs.</p> </li> <li> <code>prior_kl</code>             \u2013              <p>Compute KL divergence between CaGP posterior and GP prior..</p> </li> <li> <code>variational_expectation</code>             \u2013              <p>Compute the variational expectation.</p> </li> </ul> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def __init__(\n    self,\n    posterior: ConjugatePosterior,\n    policy: AbstractBatchLinearSolverPolicy,\n    solver: AbstractLinearSolver[_LinearSolverState] = Cholesky(1e-6),\n):\n    \"\"\"Initialize the Computation-Aware GP model.\n\n    Args:\n        posterior: GPJax conjugate posterior.\n        policy: The batch linear solver policy that defines the subspace into\n            which the data is projected.\n        solver: The linear solver method to use for solving linear systems with\n            positive semi-definite operators.\n    \"\"\"\n    self.posterior = posterior\n    self.policy = policy\n    self.solver = solver\n</code></pre>"},{"location":"reference/cagpjax/#cagpjax.ComputationAwareGP(posterior)","title":"<code>posterior</code>","text":"(<code>ConjugatePosterior</code>)           \u2013            <p>GPJax conjugate posterior.</p>"},{"location":"reference/cagpjax/#cagpjax.ComputationAwareGP(policy)","title":"<code>policy</code>","text":"(<code>AbstractBatchLinearSolverPolicy</code>)           \u2013            <p>The batch linear solver policy that defines the subspace into which the data is projected.</p>"},{"location":"reference/cagpjax/#cagpjax.ComputationAwareGP(solver)","title":"<code>solver</code>","text":"(<code>AbstractLinearSolver[_LinearSolverState]</code>, default:                   <code>Cholesky(1e-06)</code> )           \u2013            <p>The linear solver method to use for solving linear systems with positive semi-definite operators.</p>"},{"location":"reference/cagpjax/#cagpjax.ComputationAwareGP.elbo","title":"elbo","text":"<pre><code>elbo(state: ComputationAwareGPState[_LinearSolverState]) -&gt; ScalarFloat\n</code></pre> <p>Compute the evidence lower bound.</p> <p>Computes the evidence lower bound (ELBO) under this model's variational distribution.</p> Note <p>This should be used instead of <code>gpjax.objectives.elbo</code></p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>ScalarFloat</code>           \u2013            <p>ELBO value (scalar).</p> </li> </ul> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def elbo(self, state: ComputationAwareGPState[_LinearSolverState]) -&gt; ScalarFloat:\n    \"\"\"Compute the evidence lower bound.\n\n    Computes the evidence lower bound (ELBO) under this model's variational distribution.\n\n    Note:\n        This should be used instead of ``gpjax.objectives.elbo``\n\n    Args:\n        state: State of the conditioned GP computed by [`init`][..init]\n\n    Returns:\n        ELBO value (scalar).\n    \"\"\"\n    var_exp = self.variational_expectation(state)\n    kl = self.prior_kl(state)\n    return jnp.sum(var_exp) - kl\n</code></pre>"},{"location":"reference/cagpjax/#cagpjax.ComputationAwareGP.elbo(state)","title":"<code>state</code>","text":"(<code>ComputationAwareGPState[_LinearSolverState]</code>)           \u2013            <p>State of the conditioned GP computed by <code>init</code></p>"},{"location":"reference/cagpjax/#cagpjax.ComputationAwareGP.init","title":"init","text":"<pre><code>init(train_data: Dataset) -&gt; ComputationAwareGPState[_LinearSolverState]\n</code></pre> <p>Compute the state of the conditioned GP posterior.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>state</code> (              <code>ComputationAwareGPState[_LinearSolverState]</code> )          \u2013            <p>State of the conditioned CaGP posterior, which stores any necessary intermediate values for prediction and computing objectives.</p> </li> </ul> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def init(self, train_data: Dataset) -&gt; ComputationAwareGPState[_LinearSolverState]:\n    \"\"\"Compute the state of the conditioned GP posterior.\n\n    Args:\n        train_data: The training data used to fit the GP.\n\n    Returns:\n        state: State of the conditioned CaGP posterior, which stores any necessary\n            intermediate values for prediction and computing objectives.\n    \"\"\"\n    # Ensure we have supervised training data\n    if train_data.X is None or train_data.y is None:\n        raise ValueError(\"Training data must be supervised.\")\n\n    # Unpack training data\n    x = jnp.atleast_2d(train_data.X)\n    y = jnp.atleast_1d(train_data.y).squeeze()\n\n    # Unpack prior and likelihood\n    prior = self.posterior.prior\n    likelihood = self.posterior.likelihood\n\n    # Mean and covariance of prior-predictive distribution\n    mean_prior = prior.mean_function(x).squeeze()\n    # Work around GPJax promoting dtype of mean to float64 (See JaxGaussianProcesses/GPJax#523)\n    if isinstance(prior.mean_function, Constant):\n        constant = prior.mean_function.constant[...]\n        mean_prior = mean_prior.astype(constant.dtype)\n    cov_xx = lazify(prior.kernel.gram(x))\n    obs_cov = diag_like(cov_xx, likelihood.obs_stddev[...] ** 2)\n    cov_prior = cov_xx + obs_cov\n\n    # Project quantities to subspace\n    actions = self.policy.to_actions(cov_prior)\n    obs_cov_proj = congruence_transform(actions, obs_cov)\n    cov_prior_proj = congruence_transform(actions, cov_prior)\n    cov_prior_proj_state = self.solver.init(cov_prior_proj)\n\n    residual_proj = actions.T @ (y - mean_prior)\n    repr_weights_proj = self.solver.solve(cov_prior_proj_state, residual_proj)\n\n    return ComputationAwareGPState(\n        train_data=train_data,\n        actions=actions,\n        obs_cov_proj=obs_cov_proj,\n        cov_prior_proj_state=cov_prior_proj_state,\n        residual_proj=residual_proj,\n        repr_weights_proj=repr_weights_proj,\n    )\n</code></pre>"},{"location":"reference/cagpjax/#cagpjax.ComputationAwareGP.init(train_data)","title":"<code>train_data</code>","text":"(<code>Dataset</code>)           \u2013            <p>The training data used to fit the GP.</p>"},{"location":"reference/cagpjax/#cagpjax.ComputationAwareGP.predict","title":"predict","text":"<pre><code>predict(state: ComputationAwareGPState[_LinearSolverState], test_inputs: Float[Array, 'N D'] | None = None) -&gt; GaussianDistribution\n</code></pre> <p>Compute the predictive distribution of the GP at the test inputs.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>The predictive distribution of the GP at the test inputs.</p> </li> </ul> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def predict(\n    self,\n    state: ComputationAwareGPState[_LinearSolverState],\n    test_inputs: Float[Array, \"N D\"] | None = None,\n) -&gt; GaussianDistribution:\n    \"\"\"Compute the predictive distribution of the GP at the test inputs.\n\n    Args:\n        state: State of the conditioned GP computed by [`init`][..init]\n        test_inputs: The test inputs at which to make predictions. If not provided,\n            predictions are made at the training inputs.\n\n    Returns:\n        GaussianDistribution: The predictive distribution of the GP at the\n            test inputs.\n    \"\"\"\n    train_data = state.train_data\n    assert train_data.X is not None  # help out pyright\n    x = train_data.X\n\n    # Predictions at test points\n    z = test_inputs if test_inputs is not None else x\n    prior = self.posterior.prior\n    mean_z = prior.mean_function(z).squeeze()\n    # Work around GPJax promoting dtype of mean to float64 (See JaxGaussianProcesses/GPJax#523)\n    if isinstance(prior.mean_function, Constant):\n        constant = prior.mean_function.constant[...]\n        mean_z = mean_z.astype(constant.dtype)\n    cov_zz = lazify(prior.kernel.gram(z))\n    cov_zx = cov_zz if test_inputs is None else prior.kernel.cross_covariance(z, x)\n    cov_zx_proj = cov_zx @ state.actions\n\n    # Posterior predictive distribution\n    mean_pred = jnp.atleast_1d(mean_z + cov_zx_proj @ state.repr_weights_proj)\n    cov_pred = cov_zz - self.solver.inv_congruence_transform(\n        state.cov_prior_proj_state, cov_zx_proj.T\n    )\n    cov_pred = cola.PSD(cov_pred)\n\n    return GaussianDistribution(mean_pred, cov_pred, solver=self.solver)\n</code></pre>"},{"location":"reference/cagpjax/#cagpjax.ComputationAwareGP.predict(state)","title":"<code>state</code>","text":"(<code>ComputationAwareGPState[_LinearSolverState]</code>)           \u2013            <p>State of the conditioned GP computed by <code>init</code></p>"},{"location":"reference/cagpjax/#cagpjax.ComputationAwareGP.predict(test_inputs)","title":"<code>test_inputs</code>","text":"(<code>Float[Array, 'N D'] | None</code>, default:                   <code>None</code> )           \u2013            <p>The test inputs at which to make predictions. If not provided, predictions are made at the training inputs.</p>"},{"location":"reference/cagpjax/#cagpjax.ComputationAwareGP.prior_kl","title":"prior_kl","text":"<pre><code>prior_kl(state: ComputationAwareGPState[_LinearSolverState]) -&gt; ScalarFloat\n</code></pre> <p>Compute KL divergence between CaGP posterior and GP prior..</p> <p>Calculates \\(\\mathrm{KL}[q(f) || p(f)]\\), where \\(q(f)\\) is the CaGP posterior approximation and \\(p(f)\\) is the GP prior.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>ScalarFloat</code>           \u2013            <p>KL divergence value (scalar).</p> </li> </ul> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def prior_kl(\n    self, state: ComputationAwareGPState[_LinearSolverState]\n) -&gt; ScalarFloat:\n    r\"\"\"Compute KL divergence between CaGP posterior and GP prior..\n\n    Calculates $\\mathrm{KL}[q(f) || p(f)]$, where $q(f)$ is the CaGP\n    posterior approximation and $p(f)$ is the GP prior.\n\n    Args:\n        state: State of the conditioned GP computed by [`init`][..init]\n\n    Returns:\n        KL divergence value (scalar).\n    \"\"\"\n    obs_cov_proj_solver_state = self.solver.init(state.obs_cov_proj)\n\n    kl = (\n        _kl_divergence_from_solvers(\n            self.solver,\n            state.residual_proj,\n            obs_cov_proj_solver_state,\n            jnp.zeros_like(state.residual_proj),\n            state.cov_prior_proj_state,\n        )\n        - 0.5\n        * congruence_transform(\n            state.repr_weights_proj.T, state.obs_cov_proj\n        ).squeeze()\n    )\n\n    return kl\n</code></pre>"},{"location":"reference/cagpjax/#cagpjax.ComputationAwareGP.prior_kl(state)","title":"<code>state</code>","text":"(<code>ComputationAwareGPState[_LinearSolverState]</code>)           \u2013            <p>State of the conditioned GP computed by <code>init</code></p>"},{"location":"reference/cagpjax/#cagpjax.ComputationAwareGP.variational_expectation","title":"variational_expectation","text":"<pre><code>variational_expectation(state: ComputationAwareGPState[_LinearSolverState]) -&gt; Float[Array, N]\n</code></pre> <p>Compute the variational expectation.</p> <p>Compute the pointwise expected log-likelihood under the variational distribution.</p> Note <p>This should be used instead of <code>gpjax.objectives.variational_expectation</code></p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>expectation</code> (              <code>Float[Array, N]</code> )          \u2013            <p>The pointwise expected log-likelihood under the variational distribution.</p> </li> </ul> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def variational_expectation(\n    self, state: ComputationAwareGPState[_LinearSolverState]\n) -&gt; Float[Array, \"N\"]:\n    \"\"\"Compute the variational expectation.\n\n    Compute the pointwise expected log-likelihood under the variational distribution.\n\n    Note:\n        This should be used instead of ``gpjax.objectives.variational_expectation``\n\n    Args:\n        state: State of the conditioned GP computed by [`init`][..init]\n\n    Returns:\n        expectation: The pointwise expected log-likelihood under the variational distribution.\n    \"\"\"\n\n    # Unpack data\n    y = state.train_data.y\n\n    # Predict and compute expectation\n    qpred = self.predict(state)\n    mean = qpred.mean\n    variance = qpred.variance\n    expectation = self.posterior.likelihood.expected_log_likelihood(\n        y, mean[:, None], variance[:, None]\n    )\n\n    return expectation\n</code></pre>"},{"location":"reference/cagpjax/#cagpjax.ComputationAwareGP.variational_expectation(state)","title":"<code>state</code>","text":"(<code>ComputationAwareGPState[_LinearSolverState]</code>)           \u2013            <p>State of the conditioned GP computed by <code>init</code></p>"},{"location":"reference/cagpjax/#cagpjax.LanczosPolicy","title":"LanczosPolicy","text":"<pre><code>LanczosPolicy(n_actions: int | None, key: PRNGKeyArray | None = None, grad_rtol: float | None = 0.0)\n</code></pre> <p>               Bases: <code>AbstractBatchLinearSolverPolicy</code></p> <p>Lanczos-based policy for eigenvalue decomposition approximation.</p> <p>This policy uses the Lanczos algorithm to compute the top <code>n_actions</code> eigenvectors of the linear operator \\(A\\).</p> <p>Attributes:</p> <ul> <li> <code>n_actions</code>               (<code>int</code>)           \u2013            <p>Number of Lanczos vectors/actions to compute.</p> </li> <li> <code>key</code>               (<code>PRNGKeyArray | None</code>)           \u2013            <p>Random key for reproducible Lanczos iterations.</p> </li> </ul> <p>Initialize the Lanczos policy.</p> <p>Parameters:</p> <ul> <li> </li> <li> </li> <li> </li> </ul> <p>Methods:</p> <ul> <li> <code>to_actions</code>             \u2013              <p>Compute action matrix.</p> </li> </ul> Source code in <code>src/cagpjax/policies/lanczos.py</code> <pre><code>def __init__(\n    self,\n    n_actions: int | None,\n    key: PRNGKeyArray | None = None,\n    grad_rtol: float | None = 0.0,\n):\n    \"\"\"Initialize the Lanczos policy.\n\n    Args:\n        n_actions: Number of Lanczos vectors to compute.\n        key: Random key for initialization.\n        grad_rtol: Specifies the cutoff for similar eigenvalues, used to improve\n            gradient computation for (almost-)degenerate matrices.\n            If not provided, the default is 0.0.\n            If None or negative, all eigenvalues are treated as distinct.\n            (see [`cagpjax.linalg.eigh`][] for more details)\n    \"\"\"\n    self._n_actions: int = n_actions\n    self.key = key\n    self.grad_rtol = grad_rtol\n</code></pre>"},{"location":"reference/cagpjax/#cagpjax.LanczosPolicy(n_actions)","title":"<code>n_actions</code>","text":"(<code>int | None</code>)           \u2013            <p>Number of Lanczos vectors to compute.</p>"},{"location":"reference/cagpjax/#cagpjax.LanczosPolicy(key)","title":"<code>key</code>","text":"(<code>PRNGKeyArray | None</code>, default:                   <code>None</code> )           \u2013            <p>Random key for initialization.</p>"},{"location":"reference/cagpjax/#cagpjax.LanczosPolicy(grad_rtol)","title":"<code>grad_rtol</code>","text":"(<code>float | None</code>, default:                   <code>0.0</code> )           \u2013            <p>Specifies the cutoff for similar eigenvalues, used to improve gradient computation for (almost-)degenerate matrices. If not provided, the default is 0.0. If None or negative, all eigenvalues are treated as distinct. (see <code>cagpjax.linalg.eigh</code> for more details)</p>"},{"location":"reference/cagpjax/#cagpjax.LanczosPolicy.to_actions","title":"to_actions","text":"<pre><code>to_actions(A: LinearOperator) -&gt; LinearOperator\n</code></pre> <p>Compute action matrix.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LinearOperator</code>           \u2013            <p>Linear operator containing the Lanczos vectors as columns.</p> </li> </ul> Source code in <code>src/cagpjax/policies/lanczos.py</code> <pre><code>@override\ndef to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n    \"\"\"Compute action matrix.\n\n    Args:\n        A: Symmetric linear operator representing the linear system.\n\n    Returns:\n        Linear operator containing the Lanczos vectors as columns.\n    \"\"\"\n    vecs = eigh(\n        A, alg=Lanczos(self.n_actions, key=self.key), grad_rtol=self.grad_rtol\n    ).eigenvectors\n    return vecs\n</code></pre>"},{"location":"reference/cagpjax/#cagpjax.LanczosPolicy.to_actions(A)","title":"<code>A</code>","text":"(<code>LinearOperator</code>)           \u2013            <p>Symmetric linear operator representing the linear system.</p>"},{"location":"reference/cagpjax/computations/","title":"computations","text":""},{"location":"reference/cagpjax/computations/#cagpjax.computations","title":"cagpjax.computations","text":"<p>Kernel computation methods.</p> <p>Modules:</p> <ul> <li> <code>lazy</code>           \u2013            <p>Lazy kernel computation.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>LazyKernelComputation</code>           \u2013            <p>Lazy kernel computation.</p> </li> </ul>"},{"location":"reference/cagpjax/computations/#cagpjax.computations.LazyKernelComputation","title":"LazyKernelComputation","text":"<pre><code>LazyKernelComputation(*, batch_size: int | None = None, max_memory_mb: int = 2 ** 10, checkpoint: bool = True)\n</code></pre> <p>               Bases: <code>AbstractKernelComputation</code></p> <p>Lazy kernel computation.</p> <p>Compute the kernel matrix lazily, so that at most one submatrix of the kernel matrix is retained in memory at any time.</p> <p>The cross-covariance and Gram matrices are represented by a <code>LazyKernel</code> operator.</p> <p>Parameters:</p> Note <p>This class technically violates the API for <code>AbstractKernelComputation</code>, which expects that the return type of <code>cross_covariance</code> is an array, not a <code>LinearOperator</code>. While this class works as expected within this package, it should not be be used within GPJax itself.</p>"},{"location":"reference/cagpjax/computations/#cagpjax.computations.LazyKernelComputation(batch_size)","title":"<code>batch_size</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of rows/cols to materialize at once. If <code>None</code>, the batch size is determined based on <code>max_memory_mb</code>.</p>"},{"location":"reference/cagpjax/computations/#cagpjax.computations.LazyKernelComputation(max_memory_mb)","title":"<code>max_memory_mb</code>","text":"(<code>int</code>, default:                   <code>2 ** 10</code> )           \u2013            <p>The maximum number of megabytes of memory to allocate for batching the kernel matrix. If <code>batch_size</code> is provided, this is ignored.</p>"},{"location":"reference/cagpjax/computations/#cagpjax.computations.LazyKernelComputation(checkpoint)","title":"<code>checkpoint</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to checkpoint the computation. <code>checkpoint=True</code> is usually necessary to prevent all materialized submatrices from being retained in memory for gradient computation. However, this generally increases the computation time.</p>"},{"location":"reference/cagpjax/computations/#cagpjax.computations.LazyKernelComputation--examples","title":"Examples","text":"<p>We can construct a kernel with a <code>LazyKernelComputation</code> to avoid materializing the full kernel matrix in memory.</p> <pre><code>&gt;&gt;&gt; from gpjax.kernels import RBF\n&gt;&gt;&gt; from cagpjax.computations import LazyKernelComputation\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create a kernel that will be lazily evaluated\n&gt;&gt;&gt; compute_engine = LazyKernelComputation(max_memory_mb=2**10)  # 1GB\n&gt;&gt;&gt; kernel = RBF(compute_engine=compute_engine)\n</code></pre> <p>If we want to combine multiple kernels (e.g. for a product kernel), then we need to set the <code>compute_engine</code> attribute of the outermost kernel.</p> <pre><code>&gt;&gt;&gt; from gpjax.kernels import RBF, Matern32, ProductKernel\n&gt;&gt;&gt; from cagpjax.computations import LazyKernelComputation\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create a kernel that will be lazily evaluated\n&gt;&gt;&gt; compute_engine = LazyKernelComputation(max_memory_mb=2**10)  # 1GB\n&gt;&gt;&gt; kernel1 = RBF()\n&gt;&gt;&gt; kernel2 = Matern32()\n&gt;&gt;&gt; prod_kernel = kernel1 * kernel2\n&gt;&gt;&gt; prod_kernel.compute_engine = compute_engine\n&gt;&gt;&gt; # We can also explicitly construct the product kernel with a compute engine\n&gt;&gt;&gt; prod_kernel = ProductKernel(kernels=[kernel1, kernel2], compute_engine=compute_engine)\n</code></pre> <p>Initialize the lazy kernel computation.</p> Source code in <code>src/cagpjax/computations/lazy.py</code> <pre><code>def __init__(\n    self,\n    *,\n    batch_size: int | None = None,\n    max_memory_mb: int = 2**10,  # 1GB\n    checkpoint: bool = True,\n):\n    \"\"\"Initialize the lazy kernel computation.\"\"\"\n    self.batch_size = batch_size\n    self.max_memory_mb = max_memory_mb\n    self.checkpoint = checkpoint\n</code></pre>"},{"location":"reference/cagpjax/computations/lazy/","title":"lazy","text":""},{"location":"reference/cagpjax/computations/lazy/#cagpjax.computations.lazy","title":"cagpjax.computations.lazy","text":"<p>Lazy kernel computation.</p> <p>Classes:</p> <ul> <li> <code>LazyKernelComputation</code>           \u2013            <p>Lazy kernel computation.</p> </li> </ul>"},{"location":"reference/cagpjax/computations/lazy/#cagpjax.computations.lazy.LazyKernelComputation","title":"LazyKernelComputation","text":"<pre><code>LazyKernelComputation(*, batch_size: int | None = None, max_memory_mb: int = 2 ** 10, checkpoint: bool = True)\n</code></pre> <p>               Bases: <code>AbstractKernelComputation</code></p> <p>Lazy kernel computation.</p> <p>Compute the kernel matrix lazily, so that at most one submatrix of the kernel matrix is retained in memory at any time.</p> <p>The cross-covariance and Gram matrices are represented by a <code>LazyKernel</code> operator.</p> <p>Parameters:</p> Note <p>This class technically violates the API for <code>AbstractKernelComputation</code>, which expects that the return type of <code>cross_covariance</code> is an array, not a <code>LinearOperator</code>. While this class works as expected within this package, it should not be be used within GPJax itself.</p>"},{"location":"reference/cagpjax/computations/lazy/#cagpjax.computations.lazy.LazyKernelComputation(batch_size)","title":"<code>batch_size</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The number of rows/cols to materialize at once. If <code>None</code>, the batch size is determined based on <code>max_memory_mb</code>.</p>"},{"location":"reference/cagpjax/computations/lazy/#cagpjax.computations.lazy.LazyKernelComputation(max_memory_mb)","title":"<code>max_memory_mb</code>","text":"(<code>int</code>, default:                   <code>2 ** 10</code> )           \u2013            <p>The maximum number of megabytes of memory to allocate for batching the kernel matrix. If <code>batch_size</code> is provided, this is ignored.</p>"},{"location":"reference/cagpjax/computations/lazy/#cagpjax.computations.lazy.LazyKernelComputation(checkpoint)","title":"<code>checkpoint</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to checkpoint the computation. <code>checkpoint=True</code> is usually necessary to prevent all materialized submatrices from being retained in memory for gradient computation. However, this generally increases the computation time.</p>"},{"location":"reference/cagpjax/computations/lazy/#cagpjax.computations.lazy.LazyKernelComputation--examples","title":"Examples","text":"<p>We can construct a kernel with a <code>LazyKernelComputation</code> to avoid materializing the full kernel matrix in memory.</p> <pre><code>&gt;&gt;&gt; from gpjax.kernels import RBF\n&gt;&gt;&gt; from cagpjax.computations import LazyKernelComputation\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create a kernel that will be lazily evaluated\n&gt;&gt;&gt; compute_engine = LazyKernelComputation(max_memory_mb=2**10)  # 1GB\n&gt;&gt;&gt; kernel = RBF(compute_engine=compute_engine)\n</code></pre> <p>If we want to combine multiple kernels (e.g. for a product kernel), then we need to set the <code>compute_engine</code> attribute of the outermost kernel.</p> <pre><code>&gt;&gt;&gt; from gpjax.kernels import RBF, Matern32, ProductKernel\n&gt;&gt;&gt; from cagpjax.computations import LazyKernelComputation\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create a kernel that will be lazily evaluated\n&gt;&gt;&gt; compute_engine = LazyKernelComputation(max_memory_mb=2**10)  # 1GB\n&gt;&gt;&gt; kernel1 = RBF()\n&gt;&gt;&gt; kernel2 = Matern32()\n&gt;&gt;&gt; prod_kernel = kernel1 * kernel2\n&gt;&gt;&gt; prod_kernel.compute_engine = compute_engine\n&gt;&gt;&gt; # We can also explicitly construct the product kernel with a compute engine\n&gt;&gt;&gt; prod_kernel = ProductKernel(kernels=[kernel1, kernel2], compute_engine=compute_engine)\n</code></pre> <p>Initialize the lazy kernel computation.</p> Source code in <code>src/cagpjax/computations/lazy.py</code> <pre><code>def __init__(\n    self,\n    *,\n    batch_size: int | None = None,\n    max_memory_mb: int = 2**10,  # 1GB\n    checkpoint: bool = True,\n):\n    \"\"\"Initialize the lazy kernel computation.\"\"\"\n    self.batch_size = batch_size\n    self.max_memory_mb = max_memory_mb\n    self.checkpoint = checkpoint\n</code></pre>"},{"location":"reference/cagpjax/distributions/","title":"distributions","text":""},{"location":"reference/cagpjax/distributions/#cagpjax.distributions","title":"cagpjax.distributions","text":"<p>Classes:</p> <ul> <li> <code>GaussianDistribution</code>           \u2013            <p>Gaussian distribution with an implicit covariance and customizable linear solver.</p> </li> </ul>"},{"location":"reference/cagpjax/distributions/#cagpjax.distributions.GaussianDistribution","title":"GaussianDistribution","text":"<pre><code>GaussianDistribution(loc: Float[Array, ' N'], scale: LinearOperator, solver: AbstractLinearSolver = Cholesky(1e-06), **kwargs)\n</code></pre> <p>               Bases: <code>Distribution</code></p> <p>Gaussian distribution with an implicit covariance and customizable linear solver.</p> <p>Initialize the Gaussian distribution.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>covariance</code>             \u2013              <p>Operator representing the covariance of the distribution.</p> </li> <li> <code>log_prob</code>             \u2013              <p>Compute the log probability of the distribution at the given value.</p> </li> <li> <code>sample</code>             \u2013              <p>Sample from the distribution.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>mean</code>               (<code>Float[Array, ' N']</code>)           \u2013            <p>Mean of the distribution.</p> </li> <li> <code>stddev</code>               (<code>Float[Array, ' N']</code>)           \u2013            <p>Marginal standard deviation of the distribution.</p> </li> <li> <code>variance</code>               (<code>Float[Array, ' N']</code>)           \u2013            <p>Marginal variance of the distribution.</p> </li> </ul> Source code in <code>src/cagpjax/distributions.py</code> <pre><code>def __init__(\n    self,\n    loc: Float[Array, \" N\"],\n    scale: LinearOperator,\n    solver: AbstractLinearSolver = Cholesky(1e-6),\n    **kwargs,\n):\n    \"\"\"Initialize the Gaussian distribution.\n\n    Args:\n        loc: Mean of the distribution.\n        scale: Scale of the distribution.\n        solver: Method for solving the linear system of equations.\n    \"\"\"\n    self.loc = loc\n    self.scale = scale\n    batch_shape = ()\n    event_shape = jnp.shape(self.loc)\n    self.solver = solver\n    super().__init__(batch_shape, event_shape, **kwargs)\n</code></pre>"},{"location":"reference/cagpjax/distributions/#cagpjax.distributions.GaussianDistribution(loc)","title":"<code>loc</code>","text":"(<code>Float[Array, ' N']</code>)           \u2013            <p>Mean of the distribution.</p>"},{"location":"reference/cagpjax/distributions/#cagpjax.distributions.GaussianDistribution(scale)","title":"<code>scale</code>","text":"(<code>LinearOperator</code>)           \u2013            <p>Scale of the distribution.</p>"},{"location":"reference/cagpjax/distributions/#cagpjax.distributions.GaussianDistribution(solver)","title":"<code>solver</code>","text":"(<code>AbstractLinearSolver</code>, default:                   <code>Cholesky(1e-06)</code> )           \u2013            <p>Method for solving the linear system of equations.</p>"},{"location":"reference/cagpjax/distributions/#cagpjax.distributions.GaussianDistribution.mean","title":"mean  <code>property</code>","text":"<pre><code>mean: Float[Array, ' N']\n</code></pre> <p>Mean of the distribution.</p>"},{"location":"reference/cagpjax/distributions/#cagpjax.distributions.GaussianDistribution.stddev","title":"stddev  <code>property</code>","text":"<pre><code>stddev: Float[Array, ' N']\n</code></pre> <p>Marginal standard deviation of the distribution.</p>"},{"location":"reference/cagpjax/distributions/#cagpjax.distributions.GaussianDistribution.variance","title":"variance  <code>property</code>","text":"<pre><code>variance: Float[Array, ' N']\n</code></pre> <p>Marginal variance of the distribution.</p>"},{"location":"reference/cagpjax/distributions/#cagpjax.distributions.GaussianDistribution.covariance","title":"covariance","text":"<pre><code>covariance() -&gt; LinearOperator\n</code></pre> <p>Operator representing the covariance of the distribution.</p> Source code in <code>src/cagpjax/distributions.py</code> <pre><code>def covariance(self) -&gt; LinearOperator:\n    \"\"\"Operator representing the covariance of the distribution.\"\"\"\n    return self.scale\n</code></pre>"},{"location":"reference/cagpjax/distributions/#cagpjax.distributions.GaussianDistribution.log_prob","title":"log_prob","text":"<pre><code>log_prob(value: Float[Array, ' N']) -&gt; ScalarFloat\n</code></pre> <p>Compute the log probability of the distribution at the given value.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>ScalarFloat</code>           \u2013            <p>Log probability of the distribution at the given value.</p> </li> </ul> Source code in <code>src/cagpjax/distributions.py</code> <pre><code>def log_prob(self, value: Float[Array, \" N\"]) -&gt; ScalarFloat:  # pyright: ignore[reportIncompatibleMethodOverride]\n    \"\"\"Compute the log probability of the distribution at the given value.\n\n    Args:\n        value: Value at which to compute the log probability.\n\n    Returns:\n        Log probability of the distribution at the given value.\n    \"\"\"\n    mu = self.loc\n    sigma = self.scale\n    n = mu.shape[-1]\n    solver_state = self.solver.init(sigma)\n    return (\n        n * jnp.log(2 * jnp.pi)\n        + self.solver.logdet(solver_state)\n        + self.solver.inv_quad(solver_state, value - mu)\n    ) / -2\n</code></pre>"},{"location":"reference/cagpjax/distributions/#cagpjax.distributions.GaussianDistribution.log_prob(value)","title":"<code>value</code>","text":"(<code>Float[Array, ' N']</code>)           \u2013            <p>Value at which to compute the log probability.</p>"},{"location":"reference/cagpjax/distributions/#cagpjax.distributions.GaussianDistribution.sample","title":"sample","text":"<pre><code>sample(key: Key, sample_shape: tuple[int, ...] = ()) -&gt; Float[Array, '*sample_shape N']\n</code></pre> <p>Sample from the distribution.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Float[Array, '*sample_shape N']</code>           \u2013            <p>Sample from the distribution.</p> </li> </ul> Source code in <code>src/cagpjax/distributions.py</code> <pre><code>def sample(\n    self,\n    key: Key,\n    sample_shape: tuple[int, ...] = (),\n) -&gt; Float[Array, \"*sample_shape N\"]:\n    \"\"\"Sample from the distribution.\n\n    Args:\n        key: Random key for sampling.\n        sample_shape: Shape of the sample.\n\n    Returns:\n        Sample from the distribution.\n    \"\"\"\n    mu = self.loc\n    sigma = self.scale\n    n = mu.shape[-1]\n    solver_state = self.solver.init(sigma)\n    z = jax.random.normal(key, (n, math.prod(sample_shape)), dtype=mu.dtype)\n    x = self.solver.unwhiten(solver_state, z)\n    return x.T.reshape(sample_shape + (n,)) + mu\n</code></pre>"},{"location":"reference/cagpjax/distributions/#cagpjax.distributions.GaussianDistribution.sample(key)","title":"<code>key</code>","text":"(<code>Key</code>)           \u2013            <p>Random key for sampling.</p>"},{"location":"reference/cagpjax/distributions/#cagpjax.distributions.GaussianDistribution.sample(sample_shape)","title":"<code>sample_shape</code>","text":"(<code>tuple[int, ...]</code>, default:                   <code>()</code> )           \u2013            <p>Shape of the sample.</p>"},{"location":"reference/cagpjax/linalg/","title":"linalg","text":""},{"location":"reference/cagpjax/linalg/#cagpjax.linalg","title":"cagpjax.linalg","text":"<p>Linear algebra functions.</p> <p>Modules:</p> <ul> <li> <code>congruence</code>           \u2013            <p>Congruence transformations for linear operators.</p> </li> <li> <code>eigh</code>           \u2013            <p>Hermitian eigenvalue decomposition.</p> </li> <li> <code>lower_cholesky</code>           \u2013            <p>Lower Cholesky decomposition of positive semidefinite operators.</p> </li> <li> <code>orthogonalize</code>           \u2013            <p>Orthogonalization methods.</p> </li> <li> <code>utils</code>           \u2013            <p>Linear algebra utilities.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>Eigh</code>           \u2013            <p>Eigh algorithm for eigenvalue decomposition.</p> </li> <li> <code>Lanczos</code>           \u2013            <p>Lanczos algorithm for approximate partial eigenvalue decomposition.</p> </li> <li> <code>OrthogonalizationMethod</code>           \u2013            <p>Methods for orthogonalizing a matrix.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>congruence_transform</code>             \u2013              <p>Congruence transformation <code>A.T @ B @ A</code>.</p> </li> </ul>"},{"location":"reference/cagpjax/linalg/#cagpjax.linalg.Eigh","title":"Eigh","text":"<p>               Bases: <code>Algorithm</code></p> <p>Eigh algorithm for eigenvalue decomposition.</p>"},{"location":"reference/cagpjax/linalg/#cagpjax.linalg.Lanczos","title":"Lanczos","text":"<pre><code>Lanczos(max_iters: int | None = None, /, *, v0: Float[Array, N] | None = None, key: PRNGKeyArray | None = None)\n</code></pre> <p>               Bases: <code>Algorithm</code></p> <p>Lanczos algorithm for approximate partial eigenvalue decomposition.</p> <p>Parameters:</p> Source code in <code>src/cagpjax/linalg/eigh.py</code> <pre><code>def __init__(\n    self,\n    max_iters: int | None = None,\n    /,\n    *,\n    v0: Float[Array, \"N\"] | None = None,\n    key: PRNGKeyArray | None = None,\n):\n    self.max_iters = max_iters\n    self.v0 = v0\n    self.key = key\n</code></pre>"},{"location":"reference/cagpjax/linalg/#cagpjax.linalg.Lanczos(max_iters)","title":"<code>max_iters</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of iterations (number of eigenvalues/vectors to compute). If <code>None</code>, all eigenvalues/eigenvectors are computed.</p>"},{"location":"reference/cagpjax/linalg/#cagpjax.linalg.Lanczos(v0)","title":"<code>v0</code>","text":"(<code>Float[Array, N] | None</code>, default:                   <code>None</code> )           \u2013            <p>Initial vector. If <code>None</code>, a random vector is generated using <code>key</code>.</p>"},{"location":"reference/cagpjax/linalg/#cagpjax.linalg.Lanczos(key)","title":"<code>key</code>","text":"(<code>PRNGKeyArray | None</code>, default:                   <code>None</code> )           \u2013            <p>Random key for generating a random initial vector if <code>v0</code> is not provided.</p>"},{"location":"reference/cagpjax/linalg/#cagpjax.linalg.OrthogonalizationMethod","title":"OrthogonalizationMethod","text":"<p>               Bases: <code>Enum</code></p> <p>Methods for orthogonalizing a matrix.</p> <p>Attributes:</p> <ul> <li> <code>CGS</code>           \u2013            <p>Classical Gram\u2013Schmidt orthogonalization</p> </li> <li> <code>MGS</code>           \u2013            <p>Modified Gram\u2013Schmidt orthogonalization</p> </li> <li> <code>QR</code>           \u2013            <p>Householder QR decomposition</p> </li> </ul>"},{"location":"reference/cagpjax/linalg/#cagpjax.linalg.OrthogonalizationMethod.CGS","title":"CGS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CGS = 'cgs'\n</code></pre> <p>Classical Gram\u2013Schmidt orthogonalization</p>"},{"location":"reference/cagpjax/linalg/#cagpjax.linalg.OrthogonalizationMethod.MGS","title":"MGS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MGS = 'mgs'\n</code></pre> <p>Modified Gram\u2013Schmidt orthogonalization</p>"},{"location":"reference/cagpjax/linalg/#cagpjax.linalg.OrthogonalizationMethod.QR","title":"QR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>QR = 'qr'\n</code></pre> <p>Householder QR decomposition</p>"},{"location":"reference/cagpjax/linalg/#cagpjax.linalg.congruence_transform","title":"congruence_transform","text":"<pre><code>congruence_transform(A: Any, B: Any) -&gt; Any\n</code></pre><pre><code>congruence_transform(A: Diagonal, B: Diagonal) -&gt; Diagonal\n</code></pre><pre><code>congruence_transform(A: BlockDiagonalSparse, B: Diagonal | ScalarMul) -&gt; Diagonal\n</code></pre> <pre><code>congruence_transform(A: Any, B: Any) -&gt; Any\n</code></pre> <p>Congruence transformation <code>A.T @ B @ A</code>.</p> <p>Parameters:</p> Source code in <code>src/cagpjax/linalg/congruence.py</code> <pre><code>@cola.dispatch\ndef congruence_transform(A: Any, B: Any) -&gt; Any:\n    \"\"\"Congruence transformation ``A.T @ B @ A``.\n\n    Args:\n        A: Linear operator or array to be applied.\n        B: Square linear operator or array to be transformed.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/linalg/#cagpjax.linalg.congruence_transform(A)","title":"<code>A</code>","text":"(<code>Any</code>)           \u2013            <p>Linear operator or array to be applied.</p>"},{"location":"reference/cagpjax/linalg/#cagpjax.linalg.congruence_transform(B)","title":"<code>B</code>","text":"(<code>Any</code>)           \u2013            <p>Square linear operator or array to be transformed.</p>"},{"location":"reference/cagpjax/linalg/congruence/","title":"congruence","text":""},{"location":"reference/cagpjax/linalg/congruence/#cagpjax.linalg.congruence","title":"cagpjax.linalg.congruence","text":"<p>Congruence transformations for linear operators.</p> <p>Functions:</p> <ul> <li> <code>congruence_transform</code>             \u2013              <p>Congruence transformation <code>A.T @ B @ A</code>.</p> </li> </ul>"},{"location":"reference/cagpjax/linalg/congruence/#cagpjax.linalg.congruence.congruence_transform","title":"congruence_transform","text":"<pre><code>congruence_transform(A: Any, B: Any) -&gt; Any\n</code></pre><pre><code>congruence_transform(A: Diagonal, B: Diagonal) -&gt; Diagonal\n</code></pre><pre><code>congruence_transform(A: BlockDiagonalSparse, B: Diagonal | ScalarMul) -&gt; Diagonal\n</code></pre> <pre><code>congruence_transform(A: Any, B: Any) -&gt; Any\n</code></pre> <p>Congruence transformation <code>A.T @ B @ A</code>.</p> <p>Parameters:</p> Source code in <code>src/cagpjax/linalg/congruence.py</code> <pre><code>@cola.dispatch\ndef congruence_transform(A: Any, B: Any) -&gt; Any:\n    \"\"\"Congruence transformation ``A.T @ B @ A``.\n\n    Args:\n        A: Linear operator or array to be applied.\n        B: Square linear operator or array to be transformed.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/linalg/congruence/#cagpjax.linalg.congruence.congruence_transform(A)","title":"<code>A</code>","text":"(<code>Any</code>)           \u2013            <p>Linear operator or array to be applied.</p>"},{"location":"reference/cagpjax/linalg/congruence/#cagpjax.linalg.congruence.congruence_transform(B)","title":"<code>B</code>","text":"(<code>Any</code>)           \u2013            <p>Square linear operator or array to be transformed.</p>"},{"location":"reference/cagpjax/linalg/eigh/","title":"eigh","text":""},{"location":"reference/cagpjax/linalg/eigh/#cagpjax.linalg.eigh","title":"cagpjax.linalg.eigh","text":"<p>Hermitian eigenvalue decomposition.</p> <p>Classes:</p> <ul> <li> <code>Eigh</code>           \u2013            <p>Eigh algorithm for eigenvalue decomposition.</p> </li> <li> <code>EighResult</code>           \u2013            <p>Result of Hermitian eigenvalue decomposition.</p> </li> <li> <code>Lanczos</code>           \u2013            <p>Lanczos algorithm for approximate partial eigenvalue decomposition.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>eigh</code>             \u2013              <p>Compute the Hermitian eigenvalue decomposition of a linear operator.</p> </li> </ul>"},{"location":"reference/cagpjax/linalg/eigh/#cagpjax.linalg.eigh.Eigh","title":"Eigh","text":"<p>               Bases: <code>Algorithm</code></p> <p>Eigh algorithm for eigenvalue decomposition.</p>"},{"location":"reference/cagpjax/linalg/eigh/#cagpjax.linalg.eigh.EighResult","title":"EighResult","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Result of Hermitian eigenvalue decomposition.</p> <p>Attributes:</p> <ul> <li> <code>eigenvalues</code>               (<code>Float[Array, N]</code>)           \u2013            <p>Eigenvalues of the operator.</p> </li> <li> <code>eigenvectors</code>               (<code>LinearOperator</code>)           \u2013            <p>Eigenvectors of the operator.</p> </li> </ul>"},{"location":"reference/cagpjax/linalg/eigh/#cagpjax.linalg.eigh.Lanczos","title":"Lanczos","text":"<pre><code>Lanczos(max_iters: int | None = None, /, *, v0: Float[Array, N] | None = None, key: PRNGKeyArray | None = None)\n</code></pre> <p>               Bases: <code>Algorithm</code></p> <p>Lanczos algorithm for approximate partial eigenvalue decomposition.</p> <p>Parameters:</p> Source code in <code>src/cagpjax/linalg/eigh.py</code> <pre><code>def __init__(\n    self,\n    max_iters: int | None = None,\n    /,\n    *,\n    v0: Float[Array, \"N\"] | None = None,\n    key: PRNGKeyArray | None = None,\n):\n    self.max_iters = max_iters\n    self.v0 = v0\n    self.key = key\n</code></pre>"},{"location":"reference/cagpjax/linalg/eigh/#cagpjax.linalg.eigh.Lanczos(max_iters)","title":"<code>max_iters</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of iterations (number of eigenvalues/vectors to compute). If <code>None</code>, all eigenvalues/eigenvectors are computed.</p>"},{"location":"reference/cagpjax/linalg/eigh/#cagpjax.linalg.eigh.Lanczos(v0)","title":"<code>v0</code>","text":"(<code>Float[Array, N] | None</code>, default:                   <code>None</code> )           \u2013            <p>Initial vector. If <code>None</code>, a random vector is generated using <code>key</code>.</p>"},{"location":"reference/cagpjax/linalg/eigh/#cagpjax.linalg.eigh.Lanczos(key)","title":"<code>key</code>","text":"(<code>PRNGKeyArray | None</code>, default:                   <code>None</code> )           \u2013            <p>Random key for generating a random initial vector if <code>v0</code> is not provided.</p>"},{"location":"reference/cagpjax/linalg/eigh/#cagpjax.linalg.eigh.eigh","title":"eigh","text":"<pre><code>eigh(A: LinearOperator, alg: Algorithm = Eigh(), grad_rtol: float | None = None) -&gt; EighResult\n</code></pre> <p>Compute the Hermitian eigenvalue decomposition of a linear operator.</p> <p>For some algorithms, the decomposition may be approximate or partial.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>EighResult</code>           \u2013            <p>A named tuple of <code>(eigenvalues, eigenvectors)</code> where <code>eigenvectors</code> is a (semi-)orthogonal <code>LinearOperator</code>.</p> </li> </ul> Note <p>Degenerate matrices have repeated eigenvalues. The set of eigenvectors that correspond to the same eigenvalue is not unique but instead forms a subspace. <code>grad_rtol</code> only improves stability of gradient-computation if the function being differentiated depends only depends on these subspaces and not the specific eigenvectors themselves.</p> Source code in <code>src/cagpjax/linalg/eigh.py</code> <pre><code>def eigh(\n    A: LinearOperator,\n    alg: cola.linalg.Algorithm = Eigh(),\n    grad_rtol: float | None = None,\n) -&gt; EighResult:\n    \"\"\"Compute the Hermitian eigenvalue decomposition of a linear operator.\n\n    For some algorithms, the decomposition may be approximate or partial.\n\n    Args:\n        A: Hermitian linear operator.\n        alg: Algorithm for eigenvalue decomposition.\n        grad_rtol: Specifies the cutoff for similar eigenvalues, used to improve\n            gradient computation for (almost-)degenerate matrices.\n            If not provided, the default is 0.0.\n            If None or negative, all eigenvalues are treated as distinct.\n\n    Returns:\n        A named tuple of `(eigenvalues, eigenvectors)` where `eigenvectors` is a\n            (semi-)orthogonal `LinearOperator`.\n\n    Note:\n        Degenerate matrices have repeated eigenvalues.\n        The set of eigenvectors that correspond to the same eigenvalue is not unique\n        but instead forms a subspace.\n        `grad_rtol` only improves stability of gradient-computation if the function\n        being differentiated depends only depends on these subspaces and not the\n        specific eigenvectors themselves.\n    \"\"\"\n    if grad_rtol is None:\n        grad_rtol = -1.0\n    vals, vecs = _eigh(A, alg, grad_rtol)  # pyright: ignore[reportArgumentType]\n    if vecs.shape[-1] == A.shape[-1]:\n        vecs = cola.Unitary(vecs)\n    else:\n        vecs = cola.Stiefel(vecs)\n    return EighResult(vals, vecs)\n</code></pre>"},{"location":"reference/cagpjax/linalg/eigh/#cagpjax.linalg.eigh.eigh(A)","title":"<code>A</code>","text":"(<code>LinearOperator</code>)           \u2013            <p>Hermitian linear operator.</p>"},{"location":"reference/cagpjax/linalg/eigh/#cagpjax.linalg.eigh.eigh(alg)","title":"<code>alg</code>","text":"(<code>Algorithm</code>, default:                   <code>Eigh()</code> )           \u2013            <p>Algorithm for eigenvalue decomposition.</p>"},{"location":"reference/cagpjax/linalg/eigh/#cagpjax.linalg.eigh.eigh(grad_rtol)","title":"<code>grad_rtol</code>","text":"(<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Specifies the cutoff for similar eigenvalues, used to improve gradient computation for (almost-)degenerate matrices. If not provided, the default is 0.0. If None or negative, all eigenvalues are treated as distinct.</p>"},{"location":"reference/cagpjax/linalg/lower_cholesky/","title":"lower_cholesky","text":""},{"location":"reference/cagpjax/linalg/lower_cholesky/#cagpjax.linalg.lower_cholesky","title":"cagpjax.linalg.lower_cholesky","text":"<p>Lower Cholesky decomposition of positive semidefinite operators.</p> <p>Functions:</p> <ul> <li> <code>lower_cholesky</code>             \u2013              <p>Lower Cholesky decomposition of a positive semidefinite operator.</p> </li> </ul>"},{"location":"reference/cagpjax/linalg/lower_cholesky/#cagpjax.linalg.lower_cholesky.lower_cholesky","title":"lower_cholesky","text":"<pre><code>lower_cholesky(A: LinearOperator, jitter: ScalarFloat | None = None) -&gt; LinearOperator\n</code></pre> <p>Lower Cholesky decomposition of a positive semidefinite operator.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LinearOperator</code>           \u2013            <p>Lower Cholesky factor of A.</p> </li> </ul> Source code in <code>src/cagpjax/linalg/lower_cholesky.py</code> <pre><code>def lower_cholesky(\n    A: LinearOperator, jitter: ScalarFloat | None = None\n) -&gt; LinearOperator:\n    \"\"\"Lower Cholesky decomposition of a positive semidefinite operator.\n\n    Args:\n        A: Positive semidefinite operator\n        jitter: Positive jitter to add to the operator.\n\n    Returns:\n        Lower Cholesky factor of A.\n    \"\"\"\n    if jitter is None:\n        return _lower_cholesky(cola.PSD(A))\n    return _lower_cholesky_jittered(A, jitter)\n</code></pre>"},{"location":"reference/cagpjax/linalg/lower_cholesky/#cagpjax.linalg.lower_cholesky.lower_cholesky(A)","title":"<code>A</code>","text":"(<code>LinearOperator</code>)           \u2013            <p>Positive semidefinite operator</p>"},{"location":"reference/cagpjax/linalg/lower_cholesky/#cagpjax.linalg.lower_cholesky.lower_cholesky(jitter)","title":"<code>jitter</code>","text":"(<code>ScalarFloat | None</code>, default:                   <code>None</code> )           \u2013            <p>Positive jitter to add to the operator.</p>"},{"location":"reference/cagpjax/linalg/orthogonalize/","title":"orthogonalize","text":""},{"location":"reference/cagpjax/linalg/orthogonalize/#cagpjax.linalg.orthogonalize","title":"cagpjax.linalg.orthogonalize","text":"<p>Orthogonalization methods.</p> <p>Classes:</p> <ul> <li> <code>OrthogonalizationMethod</code>           \u2013            <p>Methods for orthogonalizing a matrix.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>orthogonalize</code>             \u2013              <p>Orthogonalize the operator using the specified method.</p> </li> </ul>"},{"location":"reference/cagpjax/linalg/orthogonalize/#cagpjax.linalg.orthogonalize.OrthogonalizationMethod","title":"OrthogonalizationMethod","text":"<p>               Bases: <code>Enum</code></p> <p>Methods for orthogonalizing a matrix.</p> <p>Attributes:</p> <ul> <li> <code>CGS</code>           \u2013            <p>Classical Gram\u2013Schmidt orthogonalization</p> </li> <li> <code>MGS</code>           \u2013            <p>Modified Gram\u2013Schmidt orthogonalization</p> </li> <li> <code>QR</code>           \u2013            <p>Householder QR decomposition</p> </li> </ul>"},{"location":"reference/cagpjax/linalg/orthogonalize/#cagpjax.linalg.orthogonalize.OrthogonalizationMethod.CGS","title":"CGS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CGS = 'cgs'\n</code></pre> <p>Classical Gram\u2013Schmidt orthogonalization</p>"},{"location":"reference/cagpjax/linalg/orthogonalize/#cagpjax.linalg.orthogonalize.OrthogonalizationMethod.MGS","title":"MGS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MGS = 'mgs'\n</code></pre> <p>Modified Gram\u2013Schmidt orthogonalization</p>"},{"location":"reference/cagpjax/linalg/orthogonalize/#cagpjax.linalg.orthogonalize.OrthogonalizationMethod.QR","title":"QR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>QR = 'qr'\n</code></pre> <p>Householder QR decomposition</p>"},{"location":"reference/cagpjax/linalg/orthogonalize/#cagpjax.linalg.orthogonalize.orthogonalize","title":"orthogonalize","text":"<pre><code>orthogonalize(A: Float[Array, 'm n'] | LinearOperator, /, method: OrthogonalizationMethod = OrthogonalizationMethod.QR, n_reortho: int = 0) -&gt; Float[Array, 'm n'] | cola.ops.LinearOperator\n</code></pre> <p>Orthogonalize the operator using the specified method.</p> <p>The columns of the resulting matrix should span a (super-)space of the columns of the input matrix and be mutually orthogonal. For column-rank-deficient matrices, some methods (e.g. Gram-Schmidt variants) may include columns of norm 0.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Float[Array, 'm n'] | LinearOperator</code>           \u2013            <p>The orthogonalized operator. If the input is a LinearOperator, then so is the output.</p> </li> </ul> Source code in <code>src/cagpjax/linalg/orthogonalize.py</code> <pre><code>def orthogonalize(\n    A: Float[Array, \"m n\"] | cola.ops.LinearOperator,\n    /,\n    method: OrthogonalizationMethod = OrthogonalizationMethod.QR,\n    n_reortho: int = 0,\n) -&gt; Float[Array, \"m n\"] | cola.ops.LinearOperator:\n    \"\"\"\n    Orthogonalize the operator using the specified method.\n\n    The columns of the resulting matrix should span a (super-)space of the columns of\n    the input matrix and be mutually orthogonal. For column-rank-deficient matrices,\n    some methods (e.g. Gram-Schmidt variants) may include columns of norm 0.\n\n    Args:\n        A: The operator to orthogonalize.\n        method: The method to use for orthogonalization.\n        n_reortho: The number of times to _re_-orthogonalize each column.\n            Reorthogonalizing once is generally sufficient to improve orthogonality\n            for Gram-Schmidt variants\n            (see e.g. [10.1007/s00211-005-0615-4](https://doi.org/10.1007/s00211-005-0615-4)).\n\n    Returns:\n        The orthogonalized operator. If the input is a LinearOperator, then so is the output.\n    \"\"\"\n    return _orthogonalize(A, method, n_reortho)\n</code></pre>"},{"location":"reference/cagpjax/linalg/orthogonalize/#cagpjax.linalg.orthogonalize.orthogonalize(A)","title":"<code>A</code>","text":"(<code>Float[Array, 'm n'] | LinearOperator</code>)           \u2013            <p>The operator to orthogonalize.</p>"},{"location":"reference/cagpjax/linalg/orthogonalize/#cagpjax.linalg.orthogonalize.orthogonalize(method)","title":"<code>method</code>","text":"(<code>OrthogonalizationMethod</code>, default:                   <code>QR</code> )           \u2013            <p>The method to use for orthogonalization.</p>"},{"location":"reference/cagpjax/linalg/orthogonalize/#cagpjax.linalg.orthogonalize.orthogonalize(n_reortho)","title":"<code>n_reortho</code>","text":"(<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The number of times to re-orthogonalize each column. Reorthogonalizing once is generally sufficient to improve orthogonality for Gram-Schmidt variants (see e.g. 10.1007/s00211-005-0615-4).</p>"},{"location":"reference/cagpjax/linalg/utils/","title":"utils","text":""},{"location":"reference/cagpjax/linalg/utils/#cagpjax.linalg.utils","title":"cagpjax.linalg.utils","text":"<p>Linear algebra utilities.</p>"},{"location":"reference/cagpjax/models/","title":"models","text":""},{"location":"reference/cagpjax/models/#cagpjax.models","title":"cagpjax.models","text":"<p>Gaussian process models.</p> <p>Modules:</p> <ul> <li> <code>cagp</code>           \u2013            <p>Computation-aware Gaussian Process models.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>ComputationAwareGP</code>           \u2013            <p>Computation-aware Gaussian Process model.</p> </li> </ul>"},{"location":"reference/cagpjax/models/#cagpjax.models.ComputationAwareGP","title":"ComputationAwareGP","text":"<pre><code>ComputationAwareGP(posterior: ConjugatePosterior, policy: AbstractBatchLinearSolverPolicy, solver: AbstractLinearSolver[_LinearSolverState] = Cholesky(1e-06))\n</code></pre> <p>               Bases: <code>Module</code>, <code>Generic[_LinearSolverState]</code></p> <p>Computation-aware Gaussian Process model.</p> <p>This model implements scalable GP inference by using batch linear solver policies to project the kernel and data to a lower-dimensional subspace, while accounting for the extra uncertainty imposed by observing only this subspace.</p> <p>Attributes:</p> <ul> <li> <code>posterior</code>               (<code>ConjugatePosterior</code>)           \u2013            <p>The original (exact) posterior.</p> </li> <li> <code>policy</code>               (<code>AbstractBatchLinearSolverPolicy</code>)           \u2013            <p>The batch linear solver policy.</p> </li> <li> <code>solver</code>               (<code>AbstractLinearSolver[_LinearSolverState]</code>)           \u2013            <p>The linear solver method to use for solving linear systems with positive semi-definite operators.</p> </li> </ul> Notes <ul> <li>Only single-output models are currently supported.</li> </ul> <p>Initialize the Computation-Aware GP model.</p> <p>Parameters:</p> <ul> <li> </li> <li> </li> <li> </li> </ul> <p>Methods:</p> <ul> <li> <code>elbo</code>             \u2013              <p>Compute the evidence lower bound.</p> </li> <li> <code>init</code>             \u2013              <p>Compute the state of the conditioned GP posterior.</p> </li> <li> <code>predict</code>             \u2013              <p>Compute the predictive distribution of the GP at the test inputs.</p> </li> <li> <code>prior_kl</code>             \u2013              <p>Compute KL divergence between CaGP posterior and GP prior..</p> </li> <li> <code>variational_expectation</code>             \u2013              <p>Compute the variational expectation.</p> </li> </ul> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def __init__(\n    self,\n    posterior: ConjugatePosterior,\n    policy: AbstractBatchLinearSolverPolicy,\n    solver: AbstractLinearSolver[_LinearSolverState] = Cholesky(1e-6),\n):\n    \"\"\"Initialize the Computation-Aware GP model.\n\n    Args:\n        posterior: GPJax conjugate posterior.\n        policy: The batch linear solver policy that defines the subspace into\n            which the data is projected.\n        solver: The linear solver method to use for solving linear systems with\n            positive semi-definite operators.\n    \"\"\"\n    self.posterior = posterior\n    self.policy = policy\n    self.solver = solver\n</code></pre>"},{"location":"reference/cagpjax/models/#cagpjax.models.ComputationAwareGP(posterior)","title":"<code>posterior</code>","text":"(<code>ConjugatePosterior</code>)           \u2013            <p>GPJax conjugate posterior.</p>"},{"location":"reference/cagpjax/models/#cagpjax.models.ComputationAwareGP(policy)","title":"<code>policy</code>","text":"(<code>AbstractBatchLinearSolverPolicy</code>)           \u2013            <p>The batch linear solver policy that defines the subspace into which the data is projected.</p>"},{"location":"reference/cagpjax/models/#cagpjax.models.ComputationAwareGP(solver)","title":"<code>solver</code>","text":"(<code>AbstractLinearSolver[_LinearSolverState]</code>, default:                   <code>Cholesky(1e-06)</code> )           \u2013            <p>The linear solver method to use for solving linear systems with positive semi-definite operators.</p>"},{"location":"reference/cagpjax/models/#cagpjax.models.ComputationAwareGP.elbo","title":"elbo","text":"<pre><code>elbo(state: ComputationAwareGPState[_LinearSolverState]) -&gt; ScalarFloat\n</code></pre> <p>Compute the evidence lower bound.</p> <p>Computes the evidence lower bound (ELBO) under this model's variational distribution.</p> Note <p>This should be used instead of <code>gpjax.objectives.elbo</code></p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>ScalarFloat</code>           \u2013            <p>ELBO value (scalar).</p> </li> </ul> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def elbo(self, state: ComputationAwareGPState[_LinearSolverState]) -&gt; ScalarFloat:\n    \"\"\"Compute the evidence lower bound.\n\n    Computes the evidence lower bound (ELBO) under this model's variational distribution.\n\n    Note:\n        This should be used instead of ``gpjax.objectives.elbo``\n\n    Args:\n        state: State of the conditioned GP computed by [`init`][..init]\n\n    Returns:\n        ELBO value (scalar).\n    \"\"\"\n    var_exp = self.variational_expectation(state)\n    kl = self.prior_kl(state)\n    return jnp.sum(var_exp) - kl\n</code></pre>"},{"location":"reference/cagpjax/models/#cagpjax.models.ComputationAwareGP.elbo(state)","title":"<code>state</code>","text":"(<code>ComputationAwareGPState[_LinearSolverState]</code>)           \u2013            <p>State of the conditioned GP computed by <code>init</code></p>"},{"location":"reference/cagpjax/models/#cagpjax.models.ComputationAwareGP.init","title":"init","text":"<pre><code>init(train_data: Dataset) -&gt; ComputationAwareGPState[_LinearSolverState]\n</code></pre> <p>Compute the state of the conditioned GP posterior.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>state</code> (              <code>ComputationAwareGPState[_LinearSolverState]</code> )          \u2013            <p>State of the conditioned CaGP posterior, which stores any necessary intermediate values for prediction and computing objectives.</p> </li> </ul> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def init(self, train_data: Dataset) -&gt; ComputationAwareGPState[_LinearSolverState]:\n    \"\"\"Compute the state of the conditioned GP posterior.\n\n    Args:\n        train_data: The training data used to fit the GP.\n\n    Returns:\n        state: State of the conditioned CaGP posterior, which stores any necessary\n            intermediate values for prediction and computing objectives.\n    \"\"\"\n    # Ensure we have supervised training data\n    if train_data.X is None or train_data.y is None:\n        raise ValueError(\"Training data must be supervised.\")\n\n    # Unpack training data\n    x = jnp.atleast_2d(train_data.X)\n    y = jnp.atleast_1d(train_data.y).squeeze()\n\n    # Unpack prior and likelihood\n    prior = self.posterior.prior\n    likelihood = self.posterior.likelihood\n\n    # Mean and covariance of prior-predictive distribution\n    mean_prior = prior.mean_function(x).squeeze()\n    # Work around GPJax promoting dtype of mean to float64 (See JaxGaussianProcesses/GPJax#523)\n    if isinstance(prior.mean_function, Constant):\n        constant = prior.mean_function.constant[...]\n        mean_prior = mean_prior.astype(constant.dtype)\n    cov_xx = lazify(prior.kernel.gram(x))\n    obs_cov = diag_like(cov_xx, likelihood.obs_stddev[...] ** 2)\n    cov_prior = cov_xx + obs_cov\n\n    # Project quantities to subspace\n    actions = self.policy.to_actions(cov_prior)\n    obs_cov_proj = congruence_transform(actions, obs_cov)\n    cov_prior_proj = congruence_transform(actions, cov_prior)\n    cov_prior_proj_state = self.solver.init(cov_prior_proj)\n\n    residual_proj = actions.T @ (y - mean_prior)\n    repr_weights_proj = self.solver.solve(cov_prior_proj_state, residual_proj)\n\n    return ComputationAwareGPState(\n        train_data=train_data,\n        actions=actions,\n        obs_cov_proj=obs_cov_proj,\n        cov_prior_proj_state=cov_prior_proj_state,\n        residual_proj=residual_proj,\n        repr_weights_proj=repr_weights_proj,\n    )\n</code></pre>"},{"location":"reference/cagpjax/models/#cagpjax.models.ComputationAwareGP.init(train_data)","title":"<code>train_data</code>","text":"(<code>Dataset</code>)           \u2013            <p>The training data used to fit the GP.</p>"},{"location":"reference/cagpjax/models/#cagpjax.models.ComputationAwareGP.predict","title":"predict","text":"<pre><code>predict(state: ComputationAwareGPState[_LinearSolverState], test_inputs: Float[Array, 'N D'] | None = None) -&gt; GaussianDistribution\n</code></pre> <p>Compute the predictive distribution of the GP at the test inputs.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>The predictive distribution of the GP at the test inputs.</p> </li> </ul> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def predict(\n    self,\n    state: ComputationAwareGPState[_LinearSolverState],\n    test_inputs: Float[Array, \"N D\"] | None = None,\n) -&gt; GaussianDistribution:\n    \"\"\"Compute the predictive distribution of the GP at the test inputs.\n\n    Args:\n        state: State of the conditioned GP computed by [`init`][..init]\n        test_inputs: The test inputs at which to make predictions. If not provided,\n            predictions are made at the training inputs.\n\n    Returns:\n        GaussianDistribution: The predictive distribution of the GP at the\n            test inputs.\n    \"\"\"\n    train_data = state.train_data\n    assert train_data.X is not None  # help out pyright\n    x = train_data.X\n\n    # Predictions at test points\n    z = test_inputs if test_inputs is not None else x\n    prior = self.posterior.prior\n    mean_z = prior.mean_function(z).squeeze()\n    # Work around GPJax promoting dtype of mean to float64 (See JaxGaussianProcesses/GPJax#523)\n    if isinstance(prior.mean_function, Constant):\n        constant = prior.mean_function.constant[...]\n        mean_z = mean_z.astype(constant.dtype)\n    cov_zz = lazify(prior.kernel.gram(z))\n    cov_zx = cov_zz if test_inputs is None else prior.kernel.cross_covariance(z, x)\n    cov_zx_proj = cov_zx @ state.actions\n\n    # Posterior predictive distribution\n    mean_pred = jnp.atleast_1d(mean_z + cov_zx_proj @ state.repr_weights_proj)\n    cov_pred = cov_zz - self.solver.inv_congruence_transform(\n        state.cov_prior_proj_state, cov_zx_proj.T\n    )\n    cov_pred = cola.PSD(cov_pred)\n\n    return GaussianDistribution(mean_pred, cov_pred, solver=self.solver)\n</code></pre>"},{"location":"reference/cagpjax/models/#cagpjax.models.ComputationAwareGP.predict(state)","title":"<code>state</code>","text":"(<code>ComputationAwareGPState[_LinearSolverState]</code>)           \u2013            <p>State of the conditioned GP computed by <code>init</code></p>"},{"location":"reference/cagpjax/models/#cagpjax.models.ComputationAwareGP.predict(test_inputs)","title":"<code>test_inputs</code>","text":"(<code>Float[Array, 'N D'] | None</code>, default:                   <code>None</code> )           \u2013            <p>The test inputs at which to make predictions. If not provided, predictions are made at the training inputs.</p>"},{"location":"reference/cagpjax/models/#cagpjax.models.ComputationAwareGP.prior_kl","title":"prior_kl","text":"<pre><code>prior_kl(state: ComputationAwareGPState[_LinearSolverState]) -&gt; ScalarFloat\n</code></pre> <p>Compute KL divergence between CaGP posterior and GP prior..</p> <p>Calculates \\(\\mathrm{KL}[q(f) || p(f)]\\), where \\(q(f)\\) is the CaGP posterior approximation and \\(p(f)\\) is the GP prior.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>ScalarFloat</code>           \u2013            <p>KL divergence value (scalar).</p> </li> </ul> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def prior_kl(\n    self, state: ComputationAwareGPState[_LinearSolverState]\n) -&gt; ScalarFloat:\n    r\"\"\"Compute KL divergence between CaGP posterior and GP prior..\n\n    Calculates $\\mathrm{KL}[q(f) || p(f)]$, where $q(f)$ is the CaGP\n    posterior approximation and $p(f)$ is the GP prior.\n\n    Args:\n        state: State of the conditioned GP computed by [`init`][..init]\n\n    Returns:\n        KL divergence value (scalar).\n    \"\"\"\n    obs_cov_proj_solver_state = self.solver.init(state.obs_cov_proj)\n\n    kl = (\n        _kl_divergence_from_solvers(\n            self.solver,\n            state.residual_proj,\n            obs_cov_proj_solver_state,\n            jnp.zeros_like(state.residual_proj),\n            state.cov_prior_proj_state,\n        )\n        - 0.5\n        * congruence_transform(\n            state.repr_weights_proj.T, state.obs_cov_proj\n        ).squeeze()\n    )\n\n    return kl\n</code></pre>"},{"location":"reference/cagpjax/models/#cagpjax.models.ComputationAwareGP.prior_kl(state)","title":"<code>state</code>","text":"(<code>ComputationAwareGPState[_LinearSolverState]</code>)           \u2013            <p>State of the conditioned GP computed by <code>init</code></p>"},{"location":"reference/cagpjax/models/#cagpjax.models.ComputationAwareGP.variational_expectation","title":"variational_expectation","text":"<pre><code>variational_expectation(state: ComputationAwareGPState[_LinearSolverState]) -&gt; Float[Array, N]\n</code></pre> <p>Compute the variational expectation.</p> <p>Compute the pointwise expected log-likelihood under the variational distribution.</p> Note <p>This should be used instead of <code>gpjax.objectives.variational_expectation</code></p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>expectation</code> (              <code>Float[Array, N]</code> )          \u2013            <p>The pointwise expected log-likelihood under the variational distribution.</p> </li> </ul> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def variational_expectation(\n    self, state: ComputationAwareGPState[_LinearSolverState]\n) -&gt; Float[Array, \"N\"]:\n    \"\"\"Compute the variational expectation.\n\n    Compute the pointwise expected log-likelihood under the variational distribution.\n\n    Note:\n        This should be used instead of ``gpjax.objectives.variational_expectation``\n\n    Args:\n        state: State of the conditioned GP computed by [`init`][..init]\n\n    Returns:\n        expectation: The pointwise expected log-likelihood under the variational distribution.\n    \"\"\"\n\n    # Unpack data\n    y = state.train_data.y\n\n    # Predict and compute expectation\n    qpred = self.predict(state)\n    mean = qpred.mean\n    variance = qpred.variance\n    expectation = self.posterior.likelihood.expected_log_likelihood(\n        y, mean[:, None], variance[:, None]\n    )\n\n    return expectation\n</code></pre>"},{"location":"reference/cagpjax/models/#cagpjax.models.ComputationAwareGP.variational_expectation(state)","title":"<code>state</code>","text":"(<code>ComputationAwareGPState[_LinearSolverState]</code>)           \u2013            <p>State of the conditioned GP computed by <code>init</code></p>"},{"location":"reference/cagpjax/models/cagp/","title":"cagp","text":""},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp","title":"cagpjax.models.cagp","text":"<p>Computation-aware Gaussian Process models.</p> <p>Classes:</p> <ul> <li> <code>ComputationAwareGP</code>           \u2013            <p>Computation-aware Gaussian Process model.</p> </li> <li> <code>ComputationAwareGPState</code>           \u2013            <p>Projected quantities for computation-aware GP inference.</p> </li> </ul>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGP","title":"ComputationAwareGP","text":"<pre><code>ComputationAwareGP(posterior: ConjugatePosterior, policy: AbstractBatchLinearSolverPolicy, solver: AbstractLinearSolver[_LinearSolverState] = Cholesky(1e-06))\n</code></pre> <p>               Bases: <code>Module</code>, <code>Generic[_LinearSolverState]</code></p> <p>Computation-aware Gaussian Process model.</p> <p>This model implements scalable GP inference by using batch linear solver policies to project the kernel and data to a lower-dimensional subspace, while accounting for the extra uncertainty imposed by observing only this subspace.</p> <p>Attributes:</p> <ul> <li> <code>posterior</code>               (<code>ConjugatePosterior</code>)           \u2013            <p>The original (exact) posterior.</p> </li> <li> <code>policy</code>               (<code>AbstractBatchLinearSolverPolicy</code>)           \u2013            <p>The batch linear solver policy.</p> </li> <li> <code>solver</code>               (<code>AbstractLinearSolver[_LinearSolverState]</code>)           \u2013            <p>The linear solver method to use for solving linear systems with positive semi-definite operators.</p> </li> </ul> Notes <ul> <li>Only single-output models are currently supported.</li> </ul> <p>Initialize the Computation-Aware GP model.</p> <p>Parameters:</p> <ul> <li> </li> <li> </li> <li> </li> </ul> <p>Methods:</p> <ul> <li> <code>elbo</code>             \u2013              <p>Compute the evidence lower bound.</p> </li> <li> <code>init</code>             \u2013              <p>Compute the state of the conditioned GP posterior.</p> </li> <li> <code>predict</code>             \u2013              <p>Compute the predictive distribution of the GP at the test inputs.</p> </li> <li> <code>prior_kl</code>             \u2013              <p>Compute KL divergence between CaGP posterior and GP prior..</p> </li> <li> <code>variational_expectation</code>             \u2013              <p>Compute the variational expectation.</p> </li> </ul> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def __init__(\n    self,\n    posterior: ConjugatePosterior,\n    policy: AbstractBatchLinearSolverPolicy,\n    solver: AbstractLinearSolver[_LinearSolverState] = Cholesky(1e-6),\n):\n    \"\"\"Initialize the Computation-Aware GP model.\n\n    Args:\n        posterior: GPJax conjugate posterior.\n        policy: The batch linear solver policy that defines the subspace into\n            which the data is projected.\n        solver: The linear solver method to use for solving linear systems with\n            positive semi-definite operators.\n    \"\"\"\n    self.posterior = posterior\n    self.policy = policy\n    self.solver = solver\n</code></pre>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGP(posterior)","title":"<code>posterior</code>","text":"(<code>ConjugatePosterior</code>)           \u2013            <p>GPJax conjugate posterior.</p>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGP(policy)","title":"<code>policy</code>","text":"(<code>AbstractBatchLinearSolverPolicy</code>)           \u2013            <p>The batch linear solver policy that defines the subspace into which the data is projected.</p>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGP(solver)","title":"<code>solver</code>","text":"(<code>AbstractLinearSolver[_LinearSolverState]</code>, default:                   <code>Cholesky(1e-06)</code> )           \u2013            <p>The linear solver method to use for solving linear systems with positive semi-definite operators.</p>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGP.elbo","title":"elbo","text":"<pre><code>elbo(state: ComputationAwareGPState[_LinearSolverState]) -&gt; ScalarFloat\n</code></pre> <p>Compute the evidence lower bound.</p> <p>Computes the evidence lower bound (ELBO) under this model's variational distribution.</p> Note <p>This should be used instead of <code>gpjax.objectives.elbo</code></p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>ScalarFloat</code>           \u2013            <p>ELBO value (scalar).</p> </li> </ul> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def elbo(self, state: ComputationAwareGPState[_LinearSolverState]) -&gt; ScalarFloat:\n    \"\"\"Compute the evidence lower bound.\n\n    Computes the evidence lower bound (ELBO) under this model's variational distribution.\n\n    Note:\n        This should be used instead of ``gpjax.objectives.elbo``\n\n    Args:\n        state: State of the conditioned GP computed by [`init`][..init]\n\n    Returns:\n        ELBO value (scalar).\n    \"\"\"\n    var_exp = self.variational_expectation(state)\n    kl = self.prior_kl(state)\n    return jnp.sum(var_exp) - kl\n</code></pre>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGP.elbo(state)","title":"<code>state</code>","text":"(<code>ComputationAwareGPState[_LinearSolverState]</code>)           \u2013            <p>State of the conditioned GP computed by <code>init</code></p>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGP.init","title":"init","text":"<pre><code>init(train_data: Dataset) -&gt; ComputationAwareGPState[_LinearSolverState]\n</code></pre> <p>Compute the state of the conditioned GP posterior.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>state</code> (              <code>ComputationAwareGPState[_LinearSolverState]</code> )          \u2013            <p>State of the conditioned CaGP posterior, which stores any necessary intermediate values for prediction and computing objectives.</p> </li> </ul> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def init(self, train_data: Dataset) -&gt; ComputationAwareGPState[_LinearSolverState]:\n    \"\"\"Compute the state of the conditioned GP posterior.\n\n    Args:\n        train_data: The training data used to fit the GP.\n\n    Returns:\n        state: State of the conditioned CaGP posterior, which stores any necessary\n            intermediate values for prediction and computing objectives.\n    \"\"\"\n    # Ensure we have supervised training data\n    if train_data.X is None or train_data.y is None:\n        raise ValueError(\"Training data must be supervised.\")\n\n    # Unpack training data\n    x = jnp.atleast_2d(train_data.X)\n    y = jnp.atleast_1d(train_data.y).squeeze()\n\n    # Unpack prior and likelihood\n    prior = self.posterior.prior\n    likelihood = self.posterior.likelihood\n\n    # Mean and covariance of prior-predictive distribution\n    mean_prior = prior.mean_function(x).squeeze()\n    # Work around GPJax promoting dtype of mean to float64 (See JaxGaussianProcesses/GPJax#523)\n    if isinstance(prior.mean_function, Constant):\n        constant = prior.mean_function.constant[...]\n        mean_prior = mean_prior.astype(constant.dtype)\n    cov_xx = lazify(prior.kernel.gram(x))\n    obs_cov = diag_like(cov_xx, likelihood.obs_stddev[...] ** 2)\n    cov_prior = cov_xx + obs_cov\n\n    # Project quantities to subspace\n    actions = self.policy.to_actions(cov_prior)\n    obs_cov_proj = congruence_transform(actions, obs_cov)\n    cov_prior_proj = congruence_transform(actions, cov_prior)\n    cov_prior_proj_state = self.solver.init(cov_prior_proj)\n\n    residual_proj = actions.T @ (y - mean_prior)\n    repr_weights_proj = self.solver.solve(cov_prior_proj_state, residual_proj)\n\n    return ComputationAwareGPState(\n        train_data=train_data,\n        actions=actions,\n        obs_cov_proj=obs_cov_proj,\n        cov_prior_proj_state=cov_prior_proj_state,\n        residual_proj=residual_proj,\n        repr_weights_proj=repr_weights_proj,\n    )\n</code></pre>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGP.init(train_data)","title":"<code>train_data</code>","text":"(<code>Dataset</code>)           \u2013            <p>The training data used to fit the GP.</p>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGP.predict","title":"predict","text":"<pre><code>predict(state: ComputationAwareGPState[_LinearSolverState], test_inputs: Float[Array, 'N D'] | None = None) -&gt; GaussianDistribution\n</code></pre> <p>Compute the predictive distribution of the GP at the test inputs.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>GaussianDistribution</code> (              <code>GaussianDistribution</code> )          \u2013            <p>The predictive distribution of the GP at the test inputs.</p> </li> </ul> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def predict(\n    self,\n    state: ComputationAwareGPState[_LinearSolverState],\n    test_inputs: Float[Array, \"N D\"] | None = None,\n) -&gt; GaussianDistribution:\n    \"\"\"Compute the predictive distribution of the GP at the test inputs.\n\n    Args:\n        state: State of the conditioned GP computed by [`init`][..init]\n        test_inputs: The test inputs at which to make predictions. If not provided,\n            predictions are made at the training inputs.\n\n    Returns:\n        GaussianDistribution: The predictive distribution of the GP at the\n            test inputs.\n    \"\"\"\n    train_data = state.train_data\n    assert train_data.X is not None  # help out pyright\n    x = train_data.X\n\n    # Predictions at test points\n    z = test_inputs if test_inputs is not None else x\n    prior = self.posterior.prior\n    mean_z = prior.mean_function(z).squeeze()\n    # Work around GPJax promoting dtype of mean to float64 (See JaxGaussianProcesses/GPJax#523)\n    if isinstance(prior.mean_function, Constant):\n        constant = prior.mean_function.constant[...]\n        mean_z = mean_z.astype(constant.dtype)\n    cov_zz = lazify(prior.kernel.gram(z))\n    cov_zx = cov_zz if test_inputs is None else prior.kernel.cross_covariance(z, x)\n    cov_zx_proj = cov_zx @ state.actions\n\n    # Posterior predictive distribution\n    mean_pred = jnp.atleast_1d(mean_z + cov_zx_proj @ state.repr_weights_proj)\n    cov_pred = cov_zz - self.solver.inv_congruence_transform(\n        state.cov_prior_proj_state, cov_zx_proj.T\n    )\n    cov_pred = cola.PSD(cov_pred)\n\n    return GaussianDistribution(mean_pred, cov_pred, solver=self.solver)\n</code></pre>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGP.predict(state)","title":"<code>state</code>","text":"(<code>ComputationAwareGPState[_LinearSolverState]</code>)           \u2013            <p>State of the conditioned GP computed by <code>init</code></p>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGP.predict(test_inputs)","title":"<code>test_inputs</code>","text":"(<code>Float[Array, 'N D'] | None</code>, default:                   <code>None</code> )           \u2013            <p>The test inputs at which to make predictions. If not provided, predictions are made at the training inputs.</p>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGP.prior_kl","title":"prior_kl","text":"<pre><code>prior_kl(state: ComputationAwareGPState[_LinearSolverState]) -&gt; ScalarFloat\n</code></pre> <p>Compute KL divergence between CaGP posterior and GP prior..</p> <p>Calculates \\(\\mathrm{KL}[q(f) || p(f)]\\), where \\(q(f)\\) is the CaGP posterior approximation and \\(p(f)\\) is the GP prior.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>ScalarFloat</code>           \u2013            <p>KL divergence value (scalar).</p> </li> </ul> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def prior_kl(\n    self, state: ComputationAwareGPState[_LinearSolverState]\n) -&gt; ScalarFloat:\n    r\"\"\"Compute KL divergence between CaGP posterior and GP prior..\n\n    Calculates $\\mathrm{KL}[q(f) || p(f)]$, where $q(f)$ is the CaGP\n    posterior approximation and $p(f)$ is the GP prior.\n\n    Args:\n        state: State of the conditioned GP computed by [`init`][..init]\n\n    Returns:\n        KL divergence value (scalar).\n    \"\"\"\n    obs_cov_proj_solver_state = self.solver.init(state.obs_cov_proj)\n\n    kl = (\n        _kl_divergence_from_solvers(\n            self.solver,\n            state.residual_proj,\n            obs_cov_proj_solver_state,\n            jnp.zeros_like(state.residual_proj),\n            state.cov_prior_proj_state,\n        )\n        - 0.5\n        * congruence_transform(\n            state.repr_weights_proj.T, state.obs_cov_proj\n        ).squeeze()\n    )\n\n    return kl\n</code></pre>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGP.prior_kl(state)","title":"<code>state</code>","text":"(<code>ComputationAwareGPState[_LinearSolverState]</code>)           \u2013            <p>State of the conditioned GP computed by <code>init</code></p>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGP.variational_expectation","title":"variational_expectation","text":"<pre><code>variational_expectation(state: ComputationAwareGPState[_LinearSolverState]) -&gt; Float[Array, N]\n</code></pre> <p>Compute the variational expectation.</p> <p>Compute the pointwise expected log-likelihood under the variational distribution.</p> Note <p>This should be used instead of <code>gpjax.objectives.variational_expectation</code></p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>expectation</code> (              <code>Float[Array, N]</code> )          \u2013            <p>The pointwise expected log-likelihood under the variational distribution.</p> </li> </ul> Source code in <code>src/cagpjax/models/cagp.py</code> <pre><code>def variational_expectation(\n    self, state: ComputationAwareGPState[_LinearSolverState]\n) -&gt; Float[Array, \"N\"]:\n    \"\"\"Compute the variational expectation.\n\n    Compute the pointwise expected log-likelihood under the variational distribution.\n\n    Note:\n        This should be used instead of ``gpjax.objectives.variational_expectation``\n\n    Args:\n        state: State of the conditioned GP computed by [`init`][..init]\n\n    Returns:\n        expectation: The pointwise expected log-likelihood under the variational distribution.\n    \"\"\"\n\n    # Unpack data\n    y = state.train_data.y\n\n    # Predict and compute expectation\n    qpred = self.predict(state)\n    mean = qpred.mean\n    variance = qpred.variance\n    expectation = self.posterior.likelihood.expected_log_likelihood(\n        y, mean[:, None], variance[:, None]\n    )\n\n    return expectation\n</code></pre>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGP.variational_expectation(state)","title":"<code>state</code>","text":"(<code>ComputationAwareGPState[_LinearSolverState]</code>)           \u2013            <p>State of the conditioned GP computed by <code>init</code></p>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGPState","title":"ComputationAwareGPState  <code>dataclass</code>","text":"<pre><code>ComputationAwareGPState(train_data: Dataset, actions: LinearOperator, obs_cov_proj: LinearOperator, cov_prior_proj_state: _LinearSolverState, residual_proj: Float[Array, M], repr_weights_proj: Float[Array, M])\n</code></pre> <p>               Bases: <code>Generic[_LinearSolverState]</code></p> <p>Projected quantities for computation-aware GP inference.</p> <p>Parameters:</p>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGPState(train_data)","title":"<code>train_data</code>","text":"(<code>Dataset</code>)           \u2013            <p>Training data with N inputs with D dimensions.</p>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGPState(actions)","title":"<code>actions</code>","text":"(<code>LinearOperator</code>)           \u2013            <p>Actions operator; transpose of operator projecting from N-dimensional space to M-dimensional subspace.</p>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGPState(obs_cov_proj)","title":"<code>obs_cov_proj</code>","text":"(<code>LinearOperator</code>)           \u2013            <p>Projected covariance of likelihood.</p>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGPState(cov_prior_proj_state)","title":"<code>cov_prior_proj_state</code>","text":"(<code>_LinearSolverState</code>)           \u2013            <p>Linear solver state for <code>cov_prior_proj</code>.</p>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGPState(residual_proj)","title":"<code>residual_proj</code>","text":"(<code>Float[Array, M]</code>)           \u2013            <p>Projected residuals between observations and prior mean.</p>"},{"location":"reference/cagpjax/models/cagp/#cagpjax.models.cagp.ComputationAwareGPState(repr_weights_proj)","title":"<code>repr_weights_proj</code>","text":"(<code>Float[Array, M]</code>)           \u2013            <p>Projected representer weights.</p>"},{"location":"reference/cagpjax/operators/","title":"operators","text":""},{"location":"reference/cagpjax/operators/#cagpjax.operators","title":"cagpjax.operators","text":"<p>Custom linear operators.</p> <p>Modules:</p> <ul> <li> <code>annotations</code>           \u2013            <p>Annotations for operators.</p> </li> <li> <code>block_diagonal_sparse</code>           \u2013            <p>Block-diagonal sparse linear operator.</p> </li> <li> <code>diag_like</code>           \u2013            </li> <li> <code>lazy_kernel</code>           \u2013            <p>Lazy kernel operator</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>BlockDiagonalSparse</code>           \u2013            <p>Block-diagonal sparse linear operator.</p> </li> <li> <code>LazyKernel</code>           \u2013            <p>A lazy kernel operator that avoids materializing large kernel matrices.</p> </li> </ul>"},{"location":"reference/cagpjax/operators/#cagpjax.operators.BlockDiagonalSparse","title":"BlockDiagonalSparse","text":"<pre><code>BlockDiagonalSparse(nz_values: Float[Array, N], n_blocks: int)\n</code></pre> <p>               Bases: <code>LinearOperator</code></p> <p>Block-diagonal sparse linear operator.</p> <p>This operator represents a block-diagonal matrix structure where the blocks are contiguous, and each contains a column vector, so that exactly one value is non-zero in each row.</p> <p>Parameters:</p>"},{"location":"reference/cagpjax/operators/#cagpjax.operators.BlockDiagonalSparse(nz_values)","title":"<code>nz_values</code>","text":"(<code>Float[Array, N]</code>)           \u2013            <p>Non-zero values to be distributed across diagonal blocks.</p>"},{"location":"reference/cagpjax/operators/#cagpjax.operators.BlockDiagonalSparse(n_blocks)","title":"<code>n_blocks</code>","text":"(<code>int</code>)           \u2013            <p>Number of diagonal blocks in the matrix.</p>"},{"location":"reference/cagpjax/operators/#cagpjax.operators.BlockDiagonalSparse--examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; from cagpjax.operators import BlockDiagonalSparse\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create a 3x6 block-diagonal matrix with 3 blocks\n&gt;&gt;&gt; nz_values = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n&gt;&gt;&gt; op = BlockDiagonalSparse(nz_values, n_blocks=3)\n&gt;&gt;&gt; print(op.shape)\n(6, 3)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Apply to identity matrices\n&gt;&gt;&gt; op @ jnp.eye(3)\nArray([[1., 0., 0.],\n       [2., 0., 0.],\n       [0., 3., 0.],\n       [0., 4., 0.],\n       [0., 0., 5.],\n       [0., 0., 6.]], dtype=float32)\n</code></pre> Source code in <code>src/cagpjax/operators/block_diagonal_sparse.py</code> <pre><code>def __init__(self, nz_values: Float[Array, \"N\"], n_blocks: int):\n    n = nz_values.shape[0]\n    super().__init__(nz_values.dtype, (n, n_blocks), annotations={ScaledOrthogonal})\n    self.nz_values = nz_values\n</code></pre>"},{"location":"reference/cagpjax/operators/#cagpjax.operators.LazyKernel","title":"LazyKernel","text":"<pre><code>LazyKernel(kernel: AbstractKernel, x1: Float[Array, 'M D'], x2: Float[Array, 'N D'], /, *, max_memory_mb: int = 2 ** 10, batch_size: int | None = None, checkpoint: bool = False)\n</code></pre> <p>               Bases: <code>LinearOperator</code></p> <p>A lazy kernel operator that avoids materializing large kernel matrices.</p> <p>This class implements a lazy kernel operator that computes rows/cols of a kernel matrix in blocks, preventing memory issues with large datasets.</p> <p>Parameters:</p> <p>Attributes:</p> <ul> <li> <code>batch_size_col</code>               (<code>int</code>)           \u2013            <p>Maximum number of columns to materialize at once during left mat(-vec)muls.</p> </li> <li> <code>batch_size_row</code>               (<code>int</code>)           \u2013            <p>Maximum number of rows to materialize at once during right mat(-vec)muls.</p> </li> <li> <code>max_elements</code>               (<code>int</code>)           \u2013            <p>Maximum number of elements to store in memory during matmul operations.</p> </li> </ul> Source code in <code>src/cagpjax/operators/lazy_kernel.py</code> <pre><code>def __init__(\n    self,\n    kernel: AbstractKernel,\n    x1: Float[Array, \"M D\"],\n    x2: Float[Array, \"N D\"],\n    /,\n    *,\n    max_memory_mb: int = 2**10,  # 1GB\n    batch_size: int | None = None,\n    checkpoint: bool = False,\n):\n    shape = (x1.shape[0], x2.shape[0])\n    dtype = kernel(x1[0, ...], x2[0, ...]).dtype\n    super().__init__(dtype=dtype, shape=shape)\n    self.kernel = kernel\n    self.x1 = x1\n    self.x2 = x2\n    self._compute_engine = DenseKernelComputation()\n    self.max_memory_mb = max_memory_mb\n    self.batch_size = batch_size\n    self.checkpoint = checkpoint\n</code></pre>"},{"location":"reference/cagpjax/operators/#cagpjax.operators.LazyKernel(kernel)","title":"<code>kernel</code>","text":"(<code>AbstractKernel</code>)           \u2013            <p>The kernel function to use for computations.</p>"},{"location":"reference/cagpjax/operators/#cagpjax.operators.LazyKernel(x1)","title":"<code>x1</code>","text":"(<code>Float[Array, 'M D']</code>)           \u2013            <p>First set of input points for kernel evaluation.</p>"},{"location":"reference/cagpjax/operators/#cagpjax.operators.LazyKernel(x2)","title":"<code>x2</code>","text":"(<code>Float[Array, 'N D']</code>)           \u2013            <p>Second set of input points for kernel evaluation.</p>"},{"location":"reference/cagpjax/operators/#cagpjax.operators.LazyKernel(max_memory_mb)","title":"<code>max_memory_mb</code>","text":"(<code>int</code>, default:                   <code>2 ** 10</code> )           \u2013            <p>Maximum number of megabytes of memory to allocate for batching the kernel matrix. If <code>batch_size</code> is provided, this is ignored.</p>"},{"location":"reference/cagpjax/operators/#cagpjax.operators.LazyKernel(batch_size)","title":"<code>batch_size</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of rows/cols to materialize at once. If <code>None</code>, the batch size is determined based on <code>max_memory_mb</code>.</p>"},{"location":"reference/cagpjax/operators/#cagpjax.operators.LazyKernel(checkpoint)","title":"<code>checkpoint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to checkpoint the computation. This is usually necessary to prevent all materialized submatrices from being retained in memory for gradient computation.</p>"},{"location":"reference/cagpjax/operators/#cagpjax.operators.LazyKernel.batch_size_col","title":"batch_size_col  <code>property</code>","text":"<pre><code>batch_size_col: int\n</code></pre> <p>Maximum number of columns to materialize at once during left mat(-vec)muls.</p>"},{"location":"reference/cagpjax/operators/#cagpjax.operators.LazyKernel.batch_size_row","title":"batch_size_row  <code>property</code>","text":"<pre><code>batch_size_row: int\n</code></pre> <p>Maximum number of rows to materialize at once during right mat(-vec)muls.</p>"},{"location":"reference/cagpjax/operators/#cagpjax.operators.LazyKernel.max_elements","title":"max_elements  <code>property</code>","text":"<pre><code>max_elements: int\n</code></pre> <p>Maximum number of elements to store in memory during matmul operations.</p>"},{"location":"reference/cagpjax/operators/annotations/","title":"annotations","text":""},{"location":"reference/cagpjax/operators/annotations/#cagpjax.operators.annotations","title":"cagpjax.operators.annotations","text":"<p>Annotations for operators.</p> <p>Classes:</p> <ul> <li> <code>ScaledOrthogonal</code>           \u2013            <p>Annotation for an operator whose columns are orthogonal (but not necessarily orthonormal).</p> </li> </ul>"},{"location":"reference/cagpjax/operators/annotations/#cagpjax.operators.annotations.ScaledOrthogonal","title":"ScaledOrthogonal","text":"<p>               Bases: <code>Annotation</code></p> <p>Annotation for an operator whose columns are orthogonal (but not necessarily orthonormal).</p>"},{"location":"reference/cagpjax/operators/block_diagonal_sparse/","title":"block_diagonal_sparse","text":""},{"location":"reference/cagpjax/operators/block_diagonal_sparse/#cagpjax.operators.block_diagonal_sparse","title":"cagpjax.operators.block_diagonal_sparse","text":"<p>Block-diagonal sparse linear operator.</p> <p>Classes:</p> <ul> <li> <code>BlockDiagonalSparse</code>           \u2013            <p>Block-diagonal sparse linear operator.</p> </li> </ul>"},{"location":"reference/cagpjax/operators/block_diagonal_sparse/#cagpjax.operators.block_diagonal_sparse.BlockDiagonalSparse","title":"BlockDiagonalSparse","text":"<pre><code>BlockDiagonalSparse(nz_values: Float[Array, N], n_blocks: int)\n</code></pre> <p>               Bases: <code>LinearOperator</code></p> <p>Block-diagonal sparse linear operator.</p> <p>This operator represents a block-diagonal matrix structure where the blocks are contiguous, and each contains a column vector, so that exactly one value is non-zero in each row.</p> <p>Parameters:</p>"},{"location":"reference/cagpjax/operators/block_diagonal_sparse/#cagpjax.operators.block_diagonal_sparse.BlockDiagonalSparse(nz_values)","title":"<code>nz_values</code>","text":"(<code>Float[Array, N]</code>)           \u2013            <p>Non-zero values to be distributed across diagonal blocks.</p>"},{"location":"reference/cagpjax/operators/block_diagonal_sparse/#cagpjax.operators.block_diagonal_sparse.BlockDiagonalSparse(n_blocks)","title":"<code>n_blocks</code>","text":"(<code>int</code>)           \u2013            <p>Number of diagonal blocks in the matrix.</p>"},{"location":"reference/cagpjax/operators/block_diagonal_sparse/#cagpjax.operators.block_diagonal_sparse.BlockDiagonalSparse--examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; from cagpjax.operators import BlockDiagonalSparse\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create a 3x6 block-diagonal matrix with 3 blocks\n&gt;&gt;&gt; nz_values = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n&gt;&gt;&gt; op = BlockDiagonalSparse(nz_values, n_blocks=3)\n&gt;&gt;&gt; print(op.shape)\n(6, 3)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Apply to identity matrices\n&gt;&gt;&gt; op @ jnp.eye(3)\nArray([[1., 0., 0.],\n       [2., 0., 0.],\n       [0., 3., 0.],\n       [0., 4., 0.],\n       [0., 0., 5.],\n       [0., 0., 6.]], dtype=float32)\n</code></pre> Source code in <code>src/cagpjax/operators/block_diagonal_sparse.py</code> <pre><code>def __init__(self, nz_values: Float[Array, \"N\"], n_blocks: int):\n    n = nz_values.shape[0]\n    super().__init__(nz_values.dtype, (n, n_blocks), annotations={ScaledOrthogonal})\n    self.nz_values = nz_values\n</code></pre>"},{"location":"reference/cagpjax/operators/diag_like/","title":"diag_like","text":""},{"location":"reference/cagpjax/operators/diag_like/#cagpjax.operators.diag_like","title":"cagpjax.operators.diag_like","text":"<p>Functions:</p> <ul> <li> <code>diag_like</code>             \u2013              <p>Create a diagonal operator with the same shape, dtype, and device as the operator.</p> </li> </ul>"},{"location":"reference/cagpjax/operators/diag_like/#cagpjax.operators.diag_like.diag_like","title":"diag_like","text":"<pre><code>diag_like(operator: LinearOperator, values: ScalarFloat | Float[Array, N]) -&gt; Diagonal | ScalarMul\n</code></pre> <p>Create a diagonal operator with the same shape, dtype, and device as the operator.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Diagonal | ScalarMul</code>           \u2013            <p>Diagonal or scalar operator.</p> </li> </ul> Source code in <code>src/cagpjax/operators/diag_like.py</code> <pre><code>def diag_like(\n    operator: LinearOperator, values: ScalarFloat | Float[Array, \"N\"]\n) -&gt; Diagonal | ScalarMul:\n    \"\"\"Create a diagonal operator with the same shape, dtype, and device as the operator.\n\n    Args:\n        operator: Linear operator.\n        values: Scalar for a scalar matrix or array of diagonal values for a diagonal matrix.\n\n    Returns:\n            Diagonal or scalar operator.\n    \"\"\"\n    device = operator.device\n    dtype = operator.dtype\n    if jnp.isscalar(values):\n        return ScalarMul(values, operator.shape, dtype=dtype, device=device)\n    else:\n        values = values.astype(dtype).to_device(device)\n        return Diagonal(values)\n</code></pre>"},{"location":"reference/cagpjax/operators/diag_like/#cagpjax.operators.diag_like.diag_like(operator)","title":"<code>operator</code>","text":"(<code>LinearOperator</code>)           \u2013            <p>Linear operator.</p>"},{"location":"reference/cagpjax/operators/diag_like/#cagpjax.operators.diag_like.diag_like(values)","title":"<code>values</code>","text":"(<code>ScalarFloat | Float[Array, N]</code>)           \u2013            <p>Scalar for a scalar matrix or array of diagonal values for a diagonal matrix.</p>"},{"location":"reference/cagpjax/operators/lazy_kernel/","title":"lazy_kernel","text":""},{"location":"reference/cagpjax/operators/lazy_kernel/#cagpjax.operators.lazy_kernel","title":"cagpjax.operators.lazy_kernel","text":"<p>Lazy kernel operator</p> <p>Classes:</p> <ul> <li> <code>LazyKernel</code>           \u2013            <p>A lazy kernel operator that avoids materializing large kernel matrices.</p> </li> </ul>"},{"location":"reference/cagpjax/operators/lazy_kernel/#cagpjax.operators.lazy_kernel.LazyKernel","title":"LazyKernel","text":"<pre><code>LazyKernel(kernel: AbstractKernel, x1: Float[Array, 'M D'], x2: Float[Array, 'N D'], /, *, max_memory_mb: int = 2 ** 10, batch_size: int | None = None, checkpoint: bool = False)\n</code></pre> <p>               Bases: <code>LinearOperator</code></p> <p>A lazy kernel operator that avoids materializing large kernel matrices.</p> <p>This class implements a lazy kernel operator that computes rows/cols of a kernel matrix in blocks, preventing memory issues with large datasets.</p> <p>Parameters:</p> <p>Attributes:</p> <ul> <li> <code>batch_size_col</code>               (<code>int</code>)           \u2013            <p>Maximum number of columns to materialize at once during left mat(-vec)muls.</p> </li> <li> <code>batch_size_row</code>               (<code>int</code>)           \u2013            <p>Maximum number of rows to materialize at once during right mat(-vec)muls.</p> </li> <li> <code>max_elements</code>               (<code>int</code>)           \u2013            <p>Maximum number of elements to store in memory during matmul operations.</p> </li> </ul> Source code in <code>src/cagpjax/operators/lazy_kernel.py</code> <pre><code>def __init__(\n    self,\n    kernel: AbstractKernel,\n    x1: Float[Array, \"M D\"],\n    x2: Float[Array, \"N D\"],\n    /,\n    *,\n    max_memory_mb: int = 2**10,  # 1GB\n    batch_size: int | None = None,\n    checkpoint: bool = False,\n):\n    shape = (x1.shape[0], x2.shape[0])\n    dtype = kernel(x1[0, ...], x2[0, ...]).dtype\n    super().__init__(dtype=dtype, shape=shape)\n    self.kernel = kernel\n    self.x1 = x1\n    self.x2 = x2\n    self._compute_engine = DenseKernelComputation()\n    self.max_memory_mb = max_memory_mb\n    self.batch_size = batch_size\n    self.checkpoint = checkpoint\n</code></pre>"},{"location":"reference/cagpjax/operators/lazy_kernel/#cagpjax.operators.lazy_kernel.LazyKernel(kernel)","title":"<code>kernel</code>","text":"(<code>AbstractKernel</code>)           \u2013            <p>The kernel function to use for computations.</p>"},{"location":"reference/cagpjax/operators/lazy_kernel/#cagpjax.operators.lazy_kernel.LazyKernel(x1)","title":"<code>x1</code>","text":"(<code>Float[Array, 'M D']</code>)           \u2013            <p>First set of input points for kernel evaluation.</p>"},{"location":"reference/cagpjax/operators/lazy_kernel/#cagpjax.operators.lazy_kernel.LazyKernel(x2)","title":"<code>x2</code>","text":"(<code>Float[Array, 'N D']</code>)           \u2013            <p>Second set of input points for kernel evaluation.</p>"},{"location":"reference/cagpjax/operators/lazy_kernel/#cagpjax.operators.lazy_kernel.LazyKernel(max_memory_mb)","title":"<code>max_memory_mb</code>","text":"(<code>int</code>, default:                   <code>2 ** 10</code> )           \u2013            <p>Maximum number of megabytes of memory to allocate for batching the kernel matrix. If <code>batch_size</code> is provided, this is ignored.</p>"},{"location":"reference/cagpjax/operators/lazy_kernel/#cagpjax.operators.lazy_kernel.LazyKernel(batch_size)","title":"<code>batch_size</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of rows/cols to materialize at once. If <code>None</code>, the batch size is determined based on <code>max_memory_mb</code>.</p>"},{"location":"reference/cagpjax/operators/lazy_kernel/#cagpjax.operators.lazy_kernel.LazyKernel(checkpoint)","title":"<code>checkpoint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to checkpoint the computation. This is usually necessary to prevent all materialized submatrices from being retained in memory for gradient computation.</p>"},{"location":"reference/cagpjax/operators/lazy_kernel/#cagpjax.operators.lazy_kernel.LazyKernel.batch_size_col","title":"batch_size_col  <code>property</code>","text":"<pre><code>batch_size_col: int\n</code></pre> <p>Maximum number of columns to materialize at once during left mat(-vec)muls.</p>"},{"location":"reference/cagpjax/operators/lazy_kernel/#cagpjax.operators.lazy_kernel.LazyKernel.batch_size_row","title":"batch_size_row  <code>property</code>","text":"<pre><code>batch_size_row: int\n</code></pre> <p>Maximum number of rows to materialize at once during right mat(-vec)muls.</p>"},{"location":"reference/cagpjax/operators/lazy_kernel/#cagpjax.operators.lazy_kernel.LazyKernel.max_elements","title":"max_elements  <code>property</code>","text":"<pre><code>max_elements: int\n</code></pre> <p>Maximum number of elements to store in memory during matmul operations.</p>"},{"location":"reference/cagpjax/operators/utils/","title":"utils","text":""},{"location":"reference/cagpjax/operators/utils/#cagpjax.operators.utils","title":"cagpjax.operators.utils","text":""},{"location":"reference/cagpjax/policies/","title":"policies","text":""},{"location":"reference/cagpjax/policies/#cagpjax.policies","title":"cagpjax.policies","text":"<p>Modules:</p> <ul> <li> <code>base</code>           \u2013            </li> <li> <code>block_sparse</code>           \u2013            <p>Block-sparse policy.</p> </li> <li> <code>lanczos</code>           \u2013            <p>Lanczos-based policies.</p> </li> <li> <code>orthogonalization</code>           \u2013            </li> <li> <code>pseudoinput</code>           \u2013            <p>Pseodo-input linear solver policy.</p> </li> </ul> <p>Classes:</p> <ul> <li> <code>AbstractBatchLinearSolverPolicy</code>           \u2013            <p>Abstract base class for policies that product action matrices.</p> </li> <li> <code>AbstractLinearSolverPolicy</code>           \u2013            <p>Abstract base class for all linear solver policies.</p> </li> <li> <code>BlockSparsePolicy</code>           \u2013            <p>Block-sparse linear solver policy.</p> </li> <li> <code>LanczosPolicy</code>           \u2013            <p>Lanczos-based policy for eigenvalue decomposition approximation.</p> </li> <li> <code>OrthogonalizationPolicy</code>           \u2013            <p>Orthogonalization policy.</p> </li> <li> <code>PseudoInputPolicy</code>           \u2013            <p>Pseudo-input linear solver policy.</p> </li> </ul>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.AbstractBatchLinearSolverPolicy","title":"AbstractBatchLinearSolverPolicy","text":"<p>               Bases: <code>AbstractLinearSolverPolicy</code>, <code>ABC</code></p> <p>Abstract base class for policies that product action matrices.</p> <p>Methods:</p> <ul> <li> <code>to_actions</code>             \u2013              <p>Compute all actions used to solve the linear system \\(Ax=b\\).</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>n_actions</code>               (<code>int</code>)           \u2013            <p>Number of actions in this policy.</p> </li> </ul>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.AbstractBatchLinearSolverPolicy.n_actions","title":"n_actions  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>n_actions: int\n</code></pre> <p>Number of actions in this policy.</p>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.AbstractBatchLinearSolverPolicy.to_actions","title":"to_actions  <code>abstractmethod</code>","text":"<pre><code>to_actions(A: LinearOperator) -&gt; LinearOperator\n</code></pre> <p>Compute all actions used to solve the linear system \\(Ax=b\\).</p> <p>For a matrix \\(A\\) with shape <code>(n, n)</code>, the action matrix has shape <code>(n, n_actions)</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LinearOperator</code>           \u2013            <p>Linear operator representing the action matrix.</p> </li> </ul> Source code in <code>src/cagpjax/policies/base.py</code> <pre><code>@abc.abstractmethod\ndef to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n    r\"\"\"Compute all actions used to solve the linear system $Ax=b$.\n\n    For a matrix $A$ with shape ``(n, n)``, the action matrix has shape\n    ``(n, n_actions)``.\n\n    Args:\n        A: Linear operator representing the linear system.\n\n    Returns:\n        Linear operator representing the action matrix.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.AbstractBatchLinearSolverPolicy.to_actions(A)","title":"<code>A</code>","text":"(<code>LinearOperator</code>)           \u2013            <p>Linear operator representing the linear system.</p>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.AbstractLinearSolverPolicy","title":"AbstractLinearSolverPolicy","text":"<p>               Bases: <code>Module</code></p> <p>Abstract base class for all linear solver policies.</p> <p>Policies define actions used to solve a linear system \\(A x = b\\), where \\(A\\) is a square linear operator.</p>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.BlockSparsePolicy","title":"BlockSparsePolicy","text":"<pre><code>BlockSparsePolicy(n_actions: int, n: int | None = None, nz_values: Float[Array, N] | Variable[Float[Array, N]] | None = None, key: PRNGKeyArray | None = None, **kwargs)\n</code></pre> <p>               Bases: <code>AbstractBatchLinearSolverPolicy</code></p> <p>Block-sparse linear solver policy.</p> <p>This policy uses a fixed block-diagonal sparse structure to define independent learnable actions. The matrix has the following structure:</p> \\[ S = \\begin{bmatrix}     s_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\     0 &amp; s_2 &amp; \\cdots &amp; 0 \\\\     \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\     0 &amp; 0 &amp; \\cdots &amp; s_{\\text{n_actions}} \\end{bmatrix} \\] <p>These are stacked and stored as a single trainable parameter <code>nz_values</code>.</p> <p>Initialize the block sparse policy.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>to_actions</code>             \u2013              <p>Convert to block diagonal sparse action operators.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>n_actions</code>               (<code>int</code>)           \u2013            <p>Number of actions to be used.</p> </li> </ul> Source code in <code>src/cagpjax/policies/block_sparse.py</code> <pre><code>def __init__(\n    self,\n    n_actions: int,\n    n: int | None = None,\n    nz_values: Float[Array, \"N\"] | nnx.Variable[Float[Array, \"N\"]] | None = None,\n    key: PRNGKeyArray | None = None,\n    **kwargs,\n):\n    \"\"\"Initialize the block sparse policy.\n\n    Args:\n        n_actions: Number of actions to use.\n        n: Number of rows and columns of the full operator. Must be provided if ``nz_values`` is\n            not provided.\n        nz_values: Non-zero values of the block-diagonal sparse matrix (shape ``(n,)``). If not\n            provided, random actions are sampled using the key if provided.\n        key: Random key for sampling actions if ``nz_values`` is not provided.\n        **kwargs: Additional keyword arguments for ``jax.random.normal`` (e.g. ``dtype``)\n    \"\"\"\n    if nz_values is None:\n        if n is None:\n            raise ValueError(\"n must be provided if nz_values is not provided\")\n        if key is None:\n            key = jax.random.PRNGKey(0)\n        block_size = n // n_actions\n        nz_values = jax.random.normal(key, (n,), **kwargs)\n        nz_values /= jnp.sqrt(block_size)\n    elif n is not None:\n        warnings.warn(\"n is ignored because nz_values is provided\")\n\n    if not isinstance(nz_values, nnx.Variable):\n        nz_values = Real(nz_values)\n\n    self.nz_values: nnx.Variable[Float[Array, \"N\"]] = nz_values\n    self._n_actions: int = n_actions\n</code></pre>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.BlockSparsePolicy(n_actions)","title":"<code>n_actions</code>","text":"(<code>int</code>)           \u2013            <p>Number of actions to use.</p>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.BlockSparsePolicy(n)","title":"<code>n</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of rows and columns of the full operator. Must be provided if <code>nz_values</code> is not provided.</p>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.BlockSparsePolicy(nz_values)","title":"<code>nz_values</code>","text":"(<code>Float[Array, N] | Variable[Float[Array, N]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Non-zero values of the block-diagonal sparse matrix (shape <code>(n,)</code>). If not provided, random actions are sampled using the key if provided.</p>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.BlockSparsePolicy(key)","title":"<code>key</code>","text":"(<code>PRNGKeyArray | None</code>, default:                   <code>None</code> )           \u2013            <p>Random key for sampling actions if <code>nz_values</code> is not provided.</p>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.BlockSparsePolicy(**kwargs)","title":"<code>**kwargs</code>","text":"\u2013            <p>Additional keyword arguments for <code>jax.random.normal</code> (e.g. <code>dtype</code>)</p>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.BlockSparsePolicy.n_actions","title":"n_actions  <code>property</code>","text":"<pre><code>n_actions: int\n</code></pre> <p>Number of actions to be used.</p>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.BlockSparsePolicy.to_actions","title":"to_actions","text":"<pre><code>to_actions(A: LinearOperator) -&gt; LinearOperator\n</code></pre> <p>Convert to block diagonal sparse action operators.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>BlockDiagonalSparse</code> (              <code>LinearOperator</code> )          \u2013            <p>Sparse action structure representing the blocks.</p> </li> </ul> Source code in <code>src/cagpjax/policies/block_sparse.py</code> <pre><code>@override\ndef to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n    \"\"\"Convert to block diagonal sparse action operators.\n\n    Args:\n        A: Linear operator (unused).\n\n    Returns:\n        BlockDiagonalSparse: Sparse action structure representing the blocks.\n    \"\"\"\n    return BlockDiagonalSparse(self.nz_values[...], self.n_actions)\n</code></pre>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.BlockSparsePolicy.to_actions(A)","title":"<code>A</code>","text":"(<code>LinearOperator</code>)           \u2013            <p>Linear operator (unused).</p>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.LanczosPolicy","title":"LanczosPolicy","text":"<pre><code>LanczosPolicy(n_actions: int | None, key: PRNGKeyArray | None = None, grad_rtol: float | None = 0.0)\n</code></pre> <p>               Bases: <code>AbstractBatchLinearSolverPolicy</code></p> <p>Lanczos-based policy for eigenvalue decomposition approximation.</p> <p>This policy uses the Lanczos algorithm to compute the top <code>n_actions</code> eigenvectors of the linear operator \\(A\\).</p> <p>Attributes:</p> <ul> <li> <code>n_actions</code>               (<code>int</code>)           \u2013            <p>Number of Lanczos vectors/actions to compute.</p> </li> <li> <code>key</code>               (<code>PRNGKeyArray | None</code>)           \u2013            <p>Random key for reproducible Lanczos iterations.</p> </li> </ul> <p>Initialize the Lanczos policy.</p> <p>Parameters:</p> <ul> <li> </li> <li> </li> <li> </li> </ul> <p>Methods:</p> <ul> <li> <code>to_actions</code>             \u2013              <p>Compute action matrix.</p> </li> </ul> Source code in <code>src/cagpjax/policies/lanczos.py</code> <pre><code>def __init__(\n    self,\n    n_actions: int | None,\n    key: PRNGKeyArray | None = None,\n    grad_rtol: float | None = 0.0,\n):\n    \"\"\"Initialize the Lanczos policy.\n\n    Args:\n        n_actions: Number of Lanczos vectors to compute.\n        key: Random key for initialization.\n        grad_rtol: Specifies the cutoff for similar eigenvalues, used to improve\n            gradient computation for (almost-)degenerate matrices.\n            If not provided, the default is 0.0.\n            If None or negative, all eigenvalues are treated as distinct.\n            (see [`cagpjax.linalg.eigh`][] for more details)\n    \"\"\"\n    self._n_actions: int = n_actions\n    self.key = key\n    self.grad_rtol = grad_rtol\n</code></pre>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.LanczosPolicy(n_actions)","title":"<code>n_actions</code>","text":"(<code>int | None</code>)           \u2013            <p>Number of Lanczos vectors to compute.</p>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.LanczosPolicy(key)","title":"<code>key</code>","text":"(<code>PRNGKeyArray | None</code>, default:                   <code>None</code> )           \u2013            <p>Random key for initialization.</p>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.LanczosPolicy(grad_rtol)","title":"<code>grad_rtol</code>","text":"(<code>float | None</code>, default:                   <code>0.0</code> )           \u2013            <p>Specifies the cutoff for similar eigenvalues, used to improve gradient computation for (almost-)degenerate matrices. If not provided, the default is 0.0. If None or negative, all eigenvalues are treated as distinct. (see <code>cagpjax.linalg.eigh</code> for more details)</p>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.LanczosPolicy.to_actions","title":"to_actions","text":"<pre><code>to_actions(A: LinearOperator) -&gt; LinearOperator\n</code></pre> <p>Compute action matrix.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LinearOperator</code>           \u2013            <p>Linear operator containing the Lanczos vectors as columns.</p> </li> </ul> Source code in <code>src/cagpjax/policies/lanczos.py</code> <pre><code>@override\ndef to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n    \"\"\"Compute action matrix.\n\n    Args:\n        A: Symmetric linear operator representing the linear system.\n\n    Returns:\n        Linear operator containing the Lanczos vectors as columns.\n    \"\"\"\n    vecs = eigh(\n        A, alg=Lanczos(self.n_actions, key=self.key), grad_rtol=self.grad_rtol\n    ).eigenvectors\n    return vecs\n</code></pre>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.LanczosPolicy.to_actions(A)","title":"<code>A</code>","text":"(<code>LinearOperator</code>)           \u2013            <p>Symmetric linear operator representing the linear system.</p>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.OrthogonalizationPolicy","title":"OrthogonalizationPolicy","text":"<pre><code>OrthogonalizationPolicy(base_policy: AbstractBatchLinearSolverPolicy, method: OrthogonalizationMethod = OrthogonalizationMethod.QR, n_reortho: int = 0)\n</code></pre> <p>               Bases: <code>AbstractBatchLinearSolverPolicy</code></p> <p>Orthogonalization policy.</p> <p>This policy orthogonalizes (if necessary) the action operator produced by the base policy.</p> <p>Parameters:</p> Source code in <code>src/cagpjax/policies/orthogonalization.py</code> <pre><code>def __init__(\n    self,\n    base_policy: AbstractBatchLinearSolverPolicy,\n    method: OrthogonalizationMethod = OrthogonalizationMethod.QR,\n    n_reortho: int = 0,\n):\n    self.base_policy = base_policy\n    self.method = method\n    self.n_reortho = n_reortho\n</code></pre>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.OrthogonalizationPolicy(base_policy)","title":"<code>base_policy</code>","text":"(<code>AbstractBatchLinearSolverPolicy</code>)           \u2013            <p>The base policy that produces the action operator to be orthogonalized.</p>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.OrthogonalizationPolicy(method)","title":"<code>method</code>","text":"(<code>OrthogonalizationMethod</code>, default:                   <code>QR</code> )           \u2013            <p>The method to use for orthogonalization.</p>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.OrthogonalizationPolicy(n_reortho)","title":"<code>n_reortho</code>","text":"(<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The number of times to re-orthogonalize each column. Reorthogonalizing once is generally sufficient to improve orthogonality for Gram-Schmidt variants (see e.g. 10.1007/s00211-005-0615-4).</p>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.PseudoInputPolicy","title":"PseudoInputPolicy","text":"<pre><code>PseudoInputPolicy(pseudo_inputs: Float[Array, 'M D'] | Parameter[Float[Array, 'M D']], train_inputs_or_dataset: Float[Array, 'N D'] | Dataset, kernel: AbstractKernel)\n</code></pre> <p>               Bases: <code>AbstractBatchLinearSolverPolicy</code></p> <p>Pseudo-input linear solver policy.</p> <p>This policy constructs actions from the cross-covariance between the training inputs and pseudo-inputs in the same input space. These pseudo-inputs are conceptually similar to inducing points and can be marked as trainable.</p> <p>Parameters:</p> Note <p>When training with many pseudo-inputs, it is common for the cross-covariance matrix to become poorly conditioned. Performance can be significantly improved by orthogonalizing the actions using an <code>OrthogonalizationPolicy</code>.</p> Source code in <code>src/cagpjax/policies/pseudoinput.py</code> <pre><code>def __init__(\n    self,\n    pseudo_inputs: Float[Array, \"M D\"] | Parameter[Float[Array, \"M D\"]],\n    train_inputs_or_dataset: Float[Array, \"N D\"] | gpjax.dataset.Dataset,\n    kernel: gpjax.kernels.AbstractKernel,\n):\n    if isinstance(train_inputs_or_dataset, gpjax.dataset.Dataset):\n        train_data = train_inputs_or_dataset\n        if train_data.X is None:\n            raise ValueError(\"Dataset must contain training inputs.\")\n        train_inputs = train_data.X\n    else:\n        train_inputs = train_inputs_or_dataset\n    self.pseudo_inputs = pseudo_inputs\n    self.train_inputs = jnp.atleast_2d(train_inputs)\n    self.kernel = kernel\n</code></pre>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.PseudoInputPolicy(pseudo_inputs)","title":"<code>pseudo_inputs</code>","text":"(<code>Float[Array, 'M D'] | Parameter[Float[Array, 'M D']]</code>)           \u2013            <p>Pseudo-inputs for the kernel. If wrapped as a <code>gpjax.parameters.Parameter</code>, they will be treated as trainable.</p>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.PseudoInputPolicy(train_inputs)","title":"<code>train_inputs</code>","text":"\u2013            <p>Training inputs or a dataset containing training inputs. These must be the same inputs in the same order as the training data used to condition the CaGP model.</p>"},{"location":"reference/cagpjax/policies/#cagpjax.policies.PseudoInputPolicy(kernel)","title":"<code>kernel</code>","text":"(<code>AbstractKernel</code>)           \u2013            <p>Kernel for the GP prior. It must be able to take <code>train_inputs</code> and <code>pseudo_inputs</code> as arguments to its <code>cross_covariance</code> method.</p>"},{"location":"reference/cagpjax/policies/base/","title":"base","text":""},{"location":"reference/cagpjax/policies/base/#cagpjax.policies.base","title":"cagpjax.policies.base","text":"<p>Classes:</p> <ul> <li> <code>AbstractBatchLinearSolverPolicy</code>           \u2013            <p>Abstract base class for policies that product action matrices.</p> </li> <li> <code>AbstractLinearSolverPolicy</code>           \u2013            <p>Abstract base class for all linear solver policies.</p> </li> </ul>"},{"location":"reference/cagpjax/policies/base/#cagpjax.policies.base.AbstractBatchLinearSolverPolicy","title":"AbstractBatchLinearSolverPolicy","text":"<p>               Bases: <code>AbstractLinearSolverPolicy</code>, <code>ABC</code></p> <p>Abstract base class for policies that product action matrices.</p> <p>Methods:</p> <ul> <li> <code>to_actions</code>             \u2013              <p>Compute all actions used to solve the linear system \\(Ax=b\\).</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>n_actions</code>               (<code>int</code>)           \u2013            <p>Number of actions in this policy.</p> </li> </ul>"},{"location":"reference/cagpjax/policies/base/#cagpjax.policies.base.AbstractBatchLinearSolverPolicy.n_actions","title":"n_actions  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>n_actions: int\n</code></pre> <p>Number of actions in this policy.</p>"},{"location":"reference/cagpjax/policies/base/#cagpjax.policies.base.AbstractBatchLinearSolverPolicy.to_actions","title":"to_actions  <code>abstractmethod</code>","text":"<pre><code>to_actions(A: LinearOperator) -&gt; LinearOperator\n</code></pre> <p>Compute all actions used to solve the linear system \\(Ax=b\\).</p> <p>For a matrix \\(A\\) with shape <code>(n, n)</code>, the action matrix has shape <code>(n, n_actions)</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LinearOperator</code>           \u2013            <p>Linear operator representing the action matrix.</p> </li> </ul> Source code in <code>src/cagpjax/policies/base.py</code> <pre><code>@abc.abstractmethod\ndef to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n    r\"\"\"Compute all actions used to solve the linear system $Ax=b$.\n\n    For a matrix $A$ with shape ``(n, n)``, the action matrix has shape\n    ``(n, n_actions)``.\n\n    Args:\n        A: Linear operator representing the linear system.\n\n    Returns:\n        Linear operator representing the action matrix.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/cagpjax/policies/base/#cagpjax.policies.base.AbstractBatchLinearSolverPolicy.to_actions(A)","title":"<code>A</code>","text":"(<code>LinearOperator</code>)           \u2013            <p>Linear operator representing the linear system.</p>"},{"location":"reference/cagpjax/policies/base/#cagpjax.policies.base.AbstractLinearSolverPolicy","title":"AbstractLinearSolverPolicy","text":"<p>               Bases: <code>Module</code></p> <p>Abstract base class for all linear solver policies.</p> <p>Policies define actions used to solve a linear system \\(A x = b\\), where \\(A\\) is a square linear operator.</p>"},{"location":"reference/cagpjax/policies/block_sparse/","title":"block_sparse","text":""},{"location":"reference/cagpjax/policies/block_sparse/#cagpjax.policies.block_sparse","title":"cagpjax.policies.block_sparse","text":"<p>Block-sparse policy.</p> <p>Classes:</p> <ul> <li> <code>BlockSparsePolicy</code>           \u2013            <p>Block-sparse linear solver policy.</p> </li> </ul>"},{"location":"reference/cagpjax/policies/block_sparse/#cagpjax.policies.block_sparse.BlockSparsePolicy","title":"BlockSparsePolicy","text":"<pre><code>BlockSparsePolicy(n_actions: int, n: int | None = None, nz_values: Float[Array, N] | Variable[Float[Array, N]] | None = None, key: PRNGKeyArray | None = None, **kwargs)\n</code></pre> <p>               Bases: <code>AbstractBatchLinearSolverPolicy</code></p> <p>Block-sparse linear solver policy.</p> <p>This policy uses a fixed block-diagonal sparse structure to define independent learnable actions. The matrix has the following structure:</p> \\[ S = \\begin{bmatrix}     s_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\     0 &amp; s_2 &amp; \\cdots &amp; 0 \\\\     \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\     0 &amp; 0 &amp; \\cdots &amp; s_{\\text{n_actions}} \\end{bmatrix} \\] <p>These are stacked and stored as a single trainable parameter <code>nz_values</code>.</p> <p>Initialize the block sparse policy.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>to_actions</code>             \u2013              <p>Convert to block diagonal sparse action operators.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>n_actions</code>               (<code>int</code>)           \u2013            <p>Number of actions to be used.</p> </li> </ul> Source code in <code>src/cagpjax/policies/block_sparse.py</code> <pre><code>def __init__(\n    self,\n    n_actions: int,\n    n: int | None = None,\n    nz_values: Float[Array, \"N\"] | nnx.Variable[Float[Array, \"N\"]] | None = None,\n    key: PRNGKeyArray | None = None,\n    **kwargs,\n):\n    \"\"\"Initialize the block sparse policy.\n\n    Args:\n        n_actions: Number of actions to use.\n        n: Number of rows and columns of the full operator. Must be provided if ``nz_values`` is\n            not provided.\n        nz_values: Non-zero values of the block-diagonal sparse matrix (shape ``(n,)``). If not\n            provided, random actions are sampled using the key if provided.\n        key: Random key for sampling actions if ``nz_values`` is not provided.\n        **kwargs: Additional keyword arguments for ``jax.random.normal`` (e.g. ``dtype``)\n    \"\"\"\n    if nz_values is None:\n        if n is None:\n            raise ValueError(\"n must be provided if nz_values is not provided\")\n        if key is None:\n            key = jax.random.PRNGKey(0)\n        block_size = n // n_actions\n        nz_values = jax.random.normal(key, (n,), **kwargs)\n        nz_values /= jnp.sqrt(block_size)\n    elif n is not None:\n        warnings.warn(\"n is ignored because nz_values is provided\")\n\n    if not isinstance(nz_values, nnx.Variable):\n        nz_values = Real(nz_values)\n\n    self.nz_values: nnx.Variable[Float[Array, \"N\"]] = nz_values\n    self._n_actions: int = n_actions\n</code></pre>"},{"location":"reference/cagpjax/policies/block_sparse/#cagpjax.policies.block_sparse.BlockSparsePolicy(n_actions)","title":"<code>n_actions</code>","text":"(<code>int</code>)           \u2013            <p>Number of actions to use.</p>"},{"location":"reference/cagpjax/policies/block_sparse/#cagpjax.policies.block_sparse.BlockSparsePolicy(n)","title":"<code>n</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of rows and columns of the full operator. Must be provided if <code>nz_values</code> is not provided.</p>"},{"location":"reference/cagpjax/policies/block_sparse/#cagpjax.policies.block_sparse.BlockSparsePolicy(nz_values)","title":"<code>nz_values</code>","text":"(<code>Float[Array, N] | Variable[Float[Array, N]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Non-zero values of the block-diagonal sparse matrix (shape <code>(n,)</code>). If not provided, random actions are sampled using the key if provided.</p>"},{"location":"reference/cagpjax/policies/block_sparse/#cagpjax.policies.block_sparse.BlockSparsePolicy(key)","title":"<code>key</code>","text":"(<code>PRNGKeyArray | None</code>, default:                   <code>None</code> )           \u2013            <p>Random key for sampling actions if <code>nz_values</code> is not provided.</p>"},{"location":"reference/cagpjax/policies/block_sparse/#cagpjax.policies.block_sparse.BlockSparsePolicy(**kwargs)","title":"<code>**kwargs</code>","text":"\u2013            <p>Additional keyword arguments for <code>jax.random.normal</code> (e.g. <code>dtype</code>)</p>"},{"location":"reference/cagpjax/policies/block_sparse/#cagpjax.policies.block_sparse.BlockSparsePolicy.n_actions","title":"n_actions  <code>property</code>","text":"<pre><code>n_actions: int\n</code></pre> <p>Number of actions to be used.</p>"},{"location":"reference/cagpjax/policies/block_sparse/#cagpjax.policies.block_sparse.BlockSparsePolicy.to_actions","title":"to_actions","text":"<pre><code>to_actions(A: LinearOperator) -&gt; LinearOperator\n</code></pre> <p>Convert to block diagonal sparse action operators.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>BlockDiagonalSparse</code> (              <code>LinearOperator</code> )          \u2013            <p>Sparse action structure representing the blocks.</p> </li> </ul> Source code in <code>src/cagpjax/policies/block_sparse.py</code> <pre><code>@override\ndef to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n    \"\"\"Convert to block diagonal sparse action operators.\n\n    Args:\n        A: Linear operator (unused).\n\n    Returns:\n        BlockDiagonalSparse: Sparse action structure representing the blocks.\n    \"\"\"\n    return BlockDiagonalSparse(self.nz_values[...], self.n_actions)\n</code></pre>"},{"location":"reference/cagpjax/policies/block_sparse/#cagpjax.policies.block_sparse.BlockSparsePolicy.to_actions(A)","title":"<code>A</code>","text":"(<code>LinearOperator</code>)           \u2013            <p>Linear operator (unused).</p>"},{"location":"reference/cagpjax/policies/lanczos/","title":"lanczos","text":""},{"location":"reference/cagpjax/policies/lanczos/#cagpjax.policies.lanczos","title":"cagpjax.policies.lanczos","text":"<p>Lanczos-based policies.</p> <p>Classes:</p> <ul> <li> <code>LanczosPolicy</code>           \u2013            <p>Lanczos-based policy for eigenvalue decomposition approximation.</p> </li> </ul>"},{"location":"reference/cagpjax/policies/lanczos/#cagpjax.policies.lanczos.LanczosPolicy","title":"LanczosPolicy","text":"<pre><code>LanczosPolicy(n_actions: int | None, key: PRNGKeyArray | None = None, grad_rtol: float | None = 0.0)\n</code></pre> <p>               Bases: <code>AbstractBatchLinearSolverPolicy</code></p> <p>Lanczos-based policy for eigenvalue decomposition approximation.</p> <p>This policy uses the Lanczos algorithm to compute the top <code>n_actions</code> eigenvectors of the linear operator \\(A\\).</p> <p>Attributes:</p> <ul> <li> <code>n_actions</code>               (<code>int</code>)           \u2013            <p>Number of Lanczos vectors/actions to compute.</p> </li> <li> <code>key</code>               (<code>PRNGKeyArray | None</code>)           \u2013            <p>Random key for reproducible Lanczos iterations.</p> </li> </ul> <p>Initialize the Lanczos policy.</p> <p>Parameters:</p> <ul> <li> </li> <li> </li> <li> </li> </ul> <p>Methods:</p> <ul> <li> <code>to_actions</code>             \u2013              <p>Compute action matrix.</p> </li> </ul> Source code in <code>src/cagpjax/policies/lanczos.py</code> <pre><code>def __init__(\n    self,\n    n_actions: int | None,\n    key: PRNGKeyArray | None = None,\n    grad_rtol: float | None = 0.0,\n):\n    \"\"\"Initialize the Lanczos policy.\n\n    Args:\n        n_actions: Number of Lanczos vectors to compute.\n        key: Random key for initialization.\n        grad_rtol: Specifies the cutoff for similar eigenvalues, used to improve\n            gradient computation for (almost-)degenerate matrices.\n            If not provided, the default is 0.0.\n            If None or negative, all eigenvalues are treated as distinct.\n            (see [`cagpjax.linalg.eigh`][] for more details)\n    \"\"\"\n    self._n_actions: int = n_actions\n    self.key = key\n    self.grad_rtol = grad_rtol\n</code></pre>"},{"location":"reference/cagpjax/policies/lanczos/#cagpjax.policies.lanczos.LanczosPolicy(n_actions)","title":"<code>n_actions</code>","text":"(<code>int | None</code>)           \u2013            <p>Number of Lanczos vectors to compute.</p>"},{"location":"reference/cagpjax/policies/lanczos/#cagpjax.policies.lanczos.LanczosPolicy(key)","title":"<code>key</code>","text":"(<code>PRNGKeyArray | None</code>, default:                   <code>None</code> )           \u2013            <p>Random key for initialization.</p>"},{"location":"reference/cagpjax/policies/lanczos/#cagpjax.policies.lanczos.LanczosPolicy(grad_rtol)","title":"<code>grad_rtol</code>","text":"(<code>float | None</code>, default:                   <code>0.0</code> )           \u2013            <p>Specifies the cutoff for similar eigenvalues, used to improve gradient computation for (almost-)degenerate matrices. If not provided, the default is 0.0. If None or negative, all eigenvalues are treated as distinct. (see <code>cagpjax.linalg.eigh</code> for more details)</p>"},{"location":"reference/cagpjax/policies/lanczos/#cagpjax.policies.lanczos.LanczosPolicy.to_actions","title":"to_actions","text":"<pre><code>to_actions(A: LinearOperator) -&gt; LinearOperator\n</code></pre> <p>Compute action matrix.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LinearOperator</code>           \u2013            <p>Linear operator containing the Lanczos vectors as columns.</p> </li> </ul> Source code in <code>src/cagpjax/policies/lanczos.py</code> <pre><code>@override\ndef to_actions(self, A: LinearOperator) -&gt; LinearOperator:\n    \"\"\"Compute action matrix.\n\n    Args:\n        A: Symmetric linear operator representing the linear system.\n\n    Returns:\n        Linear operator containing the Lanczos vectors as columns.\n    \"\"\"\n    vecs = eigh(\n        A, alg=Lanczos(self.n_actions, key=self.key), grad_rtol=self.grad_rtol\n    ).eigenvectors\n    return vecs\n</code></pre>"},{"location":"reference/cagpjax/policies/lanczos/#cagpjax.policies.lanczos.LanczosPolicy.to_actions(A)","title":"<code>A</code>","text":"(<code>LinearOperator</code>)           \u2013            <p>Symmetric linear operator representing the linear system.</p>"},{"location":"reference/cagpjax/policies/orthogonalization/","title":"orthogonalization","text":""},{"location":"reference/cagpjax/policies/orthogonalization/#cagpjax.policies.orthogonalization","title":"cagpjax.policies.orthogonalization","text":"<p>Classes:</p> <ul> <li> <code>OrthogonalizationPolicy</code>           \u2013            <p>Orthogonalization policy.</p> </li> </ul>"},{"location":"reference/cagpjax/policies/orthogonalization/#cagpjax.policies.orthogonalization.OrthogonalizationPolicy","title":"OrthogonalizationPolicy","text":"<pre><code>OrthogonalizationPolicy(base_policy: AbstractBatchLinearSolverPolicy, method: OrthogonalizationMethod = OrthogonalizationMethod.QR, n_reortho: int = 0)\n</code></pre> <p>               Bases: <code>AbstractBatchLinearSolverPolicy</code></p> <p>Orthogonalization policy.</p> <p>This policy orthogonalizes (if necessary) the action operator produced by the base policy.</p> <p>Parameters:</p> Source code in <code>src/cagpjax/policies/orthogonalization.py</code> <pre><code>def __init__(\n    self,\n    base_policy: AbstractBatchLinearSolverPolicy,\n    method: OrthogonalizationMethod = OrthogonalizationMethod.QR,\n    n_reortho: int = 0,\n):\n    self.base_policy = base_policy\n    self.method = method\n    self.n_reortho = n_reortho\n</code></pre>"},{"location":"reference/cagpjax/policies/orthogonalization/#cagpjax.policies.orthogonalization.OrthogonalizationPolicy(base_policy)","title":"<code>base_policy</code>","text":"(<code>AbstractBatchLinearSolverPolicy</code>)           \u2013            <p>The base policy that produces the action operator to be orthogonalized.</p>"},{"location":"reference/cagpjax/policies/orthogonalization/#cagpjax.policies.orthogonalization.OrthogonalizationPolicy(method)","title":"<code>method</code>","text":"(<code>OrthogonalizationMethod</code>, default:                   <code>QR</code> )           \u2013            <p>The method to use for orthogonalization.</p>"},{"location":"reference/cagpjax/policies/orthogonalization/#cagpjax.policies.orthogonalization.OrthogonalizationPolicy(n_reortho)","title":"<code>n_reortho</code>","text":"(<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The number of times to re-orthogonalize each column. Reorthogonalizing once is generally sufficient to improve orthogonality for Gram-Schmidt variants (see e.g. 10.1007/s00211-005-0615-4).</p>"},{"location":"reference/cagpjax/policies/pseudoinput/","title":"pseudoinput","text":""},{"location":"reference/cagpjax/policies/pseudoinput/#cagpjax.policies.pseudoinput","title":"cagpjax.policies.pseudoinput","text":"<p>Pseodo-input linear solver policy.</p> <p>Classes:</p> <ul> <li> <code>PseudoInputPolicy</code>           \u2013            <p>Pseudo-input linear solver policy.</p> </li> </ul>"},{"location":"reference/cagpjax/policies/pseudoinput/#cagpjax.policies.pseudoinput.PseudoInputPolicy","title":"PseudoInputPolicy","text":"<pre><code>PseudoInputPolicy(pseudo_inputs: Float[Array, 'M D'] | Parameter[Float[Array, 'M D']], train_inputs_or_dataset: Float[Array, 'N D'] | Dataset, kernel: AbstractKernel)\n</code></pre> <p>               Bases: <code>AbstractBatchLinearSolverPolicy</code></p> <p>Pseudo-input linear solver policy.</p> <p>This policy constructs actions from the cross-covariance between the training inputs and pseudo-inputs in the same input space. These pseudo-inputs are conceptually similar to inducing points and can be marked as trainable.</p> <p>Parameters:</p> Note <p>When training with many pseudo-inputs, it is common for the cross-covariance matrix to become poorly conditioned. Performance can be significantly improved by orthogonalizing the actions using an <code>OrthogonalizationPolicy</code>.</p> Source code in <code>src/cagpjax/policies/pseudoinput.py</code> <pre><code>def __init__(\n    self,\n    pseudo_inputs: Float[Array, \"M D\"] | Parameter[Float[Array, \"M D\"]],\n    train_inputs_or_dataset: Float[Array, \"N D\"] | gpjax.dataset.Dataset,\n    kernel: gpjax.kernels.AbstractKernel,\n):\n    if isinstance(train_inputs_or_dataset, gpjax.dataset.Dataset):\n        train_data = train_inputs_or_dataset\n        if train_data.X is None:\n            raise ValueError(\"Dataset must contain training inputs.\")\n        train_inputs = train_data.X\n    else:\n        train_inputs = train_inputs_or_dataset\n    self.pseudo_inputs = pseudo_inputs\n    self.train_inputs = jnp.atleast_2d(train_inputs)\n    self.kernel = kernel\n</code></pre>"},{"location":"reference/cagpjax/policies/pseudoinput/#cagpjax.policies.pseudoinput.PseudoInputPolicy(pseudo_inputs)","title":"<code>pseudo_inputs</code>","text":"(<code>Float[Array, 'M D'] | Parameter[Float[Array, 'M D']]</code>)           \u2013            <p>Pseudo-inputs for the kernel. If wrapped as a <code>gpjax.parameters.Parameter</code>, they will be treated as trainable.</p>"},{"location":"reference/cagpjax/policies/pseudoinput/#cagpjax.policies.pseudoinput.PseudoInputPolicy(train_inputs)","title":"<code>train_inputs</code>","text":"\u2013            <p>Training inputs or a dataset containing training inputs. These must be the same inputs in the same order as the training data used to condition the CaGP model.</p>"},{"location":"reference/cagpjax/policies/pseudoinput/#cagpjax.policies.pseudoinput.PseudoInputPolicy(kernel)","title":"<code>kernel</code>","text":"(<code>AbstractKernel</code>)           \u2013            <p>Kernel for the GP prior. It must be able to take <code>train_inputs</code> and <code>pseudo_inputs</code> as arguments to its <code>cross_covariance</code> method.</p>"},{"location":"reference/cagpjax/solvers/","title":"solvers","text":""},{"location":"reference/cagpjax/solvers/#cagpjax.solvers","title":"cagpjax.solvers","text":"<p>Modules:</p> <ul> <li> <code>base</code>           \u2013            <p>Base classes for linear solvers and methods.</p> </li> <li> <code>cholesky</code>           \u2013            <p>Linear solvers based on Cholesky decomposition.</p> </li> <li> <code>pseudoinverse</code>           \u2013            </li> </ul> <p>Classes:</p> <ul> <li> <code>AbstractLinearSolver</code>           \u2013            <p>Base class for linear solvers.</p> </li> <li> <code>Cholesky</code>           \u2013            <p>Solve a linear system using the Cholesky decomposition.</p> </li> <li> <code>PseudoInverse</code>           \u2013            <p>Solve a linear system using the Moore-Penrose pseudoinverse.</p> </li> </ul>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver","title":"AbstractLinearSolver","text":"<p>               Bases: <code>Module</code>, <code>Generic[_LinearSolverState]</code></p> <p>Base class for linear solvers.</p> <p>These solvers are used to exactly or approximately solve the linear system \\(Ax = b\\) for \\(x\\), where \\(A\\) is a positive (semi-)definite (PSD) linear operator.</p> <p>Methods:</p> <ul> <li> <code>init</code>             \u2013              <p>Construct a solver state.</p> </li> <li> <code>inv_congruence_transform</code>             \u2013              <p>Compute the inverse congruence transform \\(B^T x\\) for \\(x\\) in \\(Ax = B\\).</p> </li> <li> <code>inv_quad</code>             \u2013              <p>Compute the inverse quadratic form \\(b^T x\\), for \\(x\\) in \\(Ax = b\\).</p> </li> <li> <code>logdet</code>             \u2013              <p>Compute the logarithm of the (pseudo-)determinant of \\(A\\).</p> </li> <li> <code>solve</code>             \u2013              <p>Compute a solution to the linear system \\(Ax = b\\).</p> </li> <li> <code>trace_solve</code>             \u2013              <p>Compute \\(\\mathrm{trace}(X)\\) in \\(AX=B\\) for PSD \\(A\\) and \\(B\\).</p> </li> <li> <code>unwhiten</code>             \u2013              <p>Given an IID standard normal vector \\(z\\), return \\(x\\) with covariance \\(A\\).</p> </li> </ul>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.init","title":"init  <code>abstractmethod</code>","text":"<pre><code>init(A: LinearOperator) -&gt; _LinearSolverState\n</code></pre> <p>Construct a solver state.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>_LinearSolverState</code>           \u2013            <p>State of the linear solver, which stores any necessary intermediate values.</p> </li> </ul> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>@abstractmethod\ndef init(self, A: LinearOperator) -&gt; _LinearSolverState:\n    \"\"\"Construct a solver state.\n\n    Arguments:\n        A: Positive (semi-)definite linear operator.\n\n    Returns:\n        State of the linear solver, which stores any necessary intermediate values.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.init(A)","title":"<code>A</code>","text":"(<code>LinearOperator</code>)           \u2013            <p>Positive (semi-)definite linear operator.</p>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.inv_congruence_transform","title":"inv_congruence_transform  <code>abstractmethod</code>","text":"<pre><code>inv_congruence_transform(state: _LinearSolverState, B: LinearOperator | Float[Array, 'N K']) -&gt; LinearOperator | Float[Array, 'K K']\n</code></pre> <p>Compute the inverse congruence transform \\(B^T x\\) for \\(x\\) in \\(Ax = B\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LinearOperator | Float[Array, 'K K']</code>           \u2013            <p>Linear operator or array resulting from the congruence transform.</p> </li> </ul> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>@abstractmethod\ndef inv_congruence_transform(\n    self, state: _LinearSolverState, B: LinearOperator | Float[Array, \"N K\"]\n) -&gt; LinearOperator | Float[Array, \"K K\"]:\n    \"\"\"Compute the inverse congruence transform $B^T x$ for $x$ in $Ax = B$.\n\n    Arguments:\n        state: State of the linear solver returned by [`init`][..init].\n        B: Linear operator or array to be applied.\n\n    Returns:\n        Linear operator or array resulting from the congruence transform.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.inv_congruence_transform(state)","title":"<code>state</code>","text":"(<code>_LinearSolverState</code>)           \u2013            <p>State of the linear solver returned by <code>init</code>.</p>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.inv_congruence_transform(B)","title":"<code>B</code>","text":"(<code>LinearOperator | Float[Array, 'N K']</code>)           \u2013            <p>Linear operator or array to be applied.</p>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.inv_quad","title":"inv_quad","text":"<pre><code>inv_quad(state: _LinearSolverState, b: Float[Array, N]) -&gt; ScalarFloat\n</code></pre> <p>Compute the inverse quadratic form \\(b^T x\\), for \\(x\\) in \\(Ax = b\\).</p> <p>Parameters:</p> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>def inv_quad(\n    self, state: _LinearSolverState, b: Float[Array, \"N #1\"]\n) -&gt; ScalarFloat:\n    \"\"\"Compute the inverse quadratic form $b^T x$, for $x$ in $Ax = b$.\n\n    Arguments:\n        state: State of the linear solver returned by [`init`][..init].\n        b: Right-hand side of the linear system.\n    \"\"\"\n    return self.inv_congruence_transform(state, b[:, None]).squeeze()\n</code></pre>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.inv_quad(state)","title":"<code>state</code>","text":"(<code>_LinearSolverState</code>)           \u2013            <p>State of the linear solver returned by <code>init</code>.</p>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.inv_quad(b)","title":"<code>b</code>","text":"(<code>Float[Array, N]</code>)           \u2013            <p>Right-hand side of the linear system.</p>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.logdet","title":"logdet  <code>abstractmethod</code>","text":"<pre><code>logdet(state: _LinearSolverState) -&gt; ScalarFloat\n</code></pre> <p>Compute the logarithm of the (pseudo-)determinant of \\(A\\).</p> <p>Parameters:</p> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>@abstractmethod\ndef logdet(self, state: _LinearSolverState) -&gt; ScalarFloat:\n    \"\"\"Compute the logarithm of the (pseudo-)determinant of $A$.\n\n    Arguments:\n        state: State of the linear solver returned by [`init`][..init].\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.logdet(state)","title":"<code>state</code>","text":"(<code>_LinearSolverState</code>)           \u2013            <p>State of the linear solver returned by <code>init</code>.</p>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.solve","title":"solve  <code>abstractmethod</code>","text":"<pre><code>solve(state: _LinearSolverState, b: Float[Array, N]) -&gt; Float[Array, N]\n</code></pre> <p>Compute a solution to the linear system \\(Ax = b\\).</p> <p>Parameters:</p> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>@abstractmethod\ndef solve(\n    self, state: _LinearSolverState, b: Float[Array, \"N #K\"]\n) -&gt; Float[Array, \"N #K\"]:\n    \"\"\"Compute a solution to the linear system $Ax = b$.\n\n    Arguments:\n        state: State of the linear solver returned by [`init`][..init].\n        b: Right-hand side of the linear system.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.solve(state)","title":"<code>state</code>","text":"(<code>_LinearSolverState</code>)           \u2013            <p>State of the linear solver returned by <code>init</code>.</p>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.solve(b)","title":"<code>b</code>","text":"(<code>Float[Array, N]</code>)           \u2013            <p>Right-hand side of the linear system.</p>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.trace_solve","title":"trace_solve  <code>abstractmethod</code>","text":"<pre><code>trace_solve(state: _LinearSolverState, state_other: _LinearSolverState) -&gt; ScalarFloat\n</code></pre> <p>Compute \\(\\mathrm{trace}(X)\\) in \\(AX=B\\) for PSD \\(A\\) and \\(B\\).</p> <p>Parameters:</p> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>@abstractmethod\ndef trace_solve(\n    self, state: _LinearSolverState, state_other: _LinearSolverState\n) -&gt; ScalarFloat:\n    r\"\"\"Compute $\\mathrm{trace}(X)$ in $AX=B$ for PSD $A$ and $B$.\n\n    Arguments:\n        state: State of the linear solver obtained by applying [`init`][..init] to an operator\n            representing $A$\n        state_other: Another state obtained by applying `init` to an operator representing $B$.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.trace_solve(state)","title":"<code>state</code>","text":"(<code>_LinearSolverState</code>)           \u2013            <p>State of the linear solver obtained by applying <code>init</code> to an operator representing \\(A\\)</p>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.trace_solve(state_other)","title":"<code>state_other</code>","text":"(<code>_LinearSolverState</code>)           \u2013            <p>Another state obtained by applying <code>init</code> to an operator representing \\(B\\).</p>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.unwhiten","title":"unwhiten  <code>abstractmethod</code>","text":"<pre><code>unwhiten(state: _LinearSolverState, z: Float[Array, N]) -&gt; Float[Array, N]\n</code></pre> <p>Given an IID standard normal vector \\(z\\), return \\(x\\) with covariance \\(A\\).</p> <p>Parameters:</p> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>@abstractmethod\ndef unwhiten(\n    self, state: _LinearSolverState, z: Float[Array, \"N #K\"]\n) -&gt; Float[Array, \"N #K\"]:\n    \"\"\"Given an IID standard normal vector $z$, return $x$ with covariance $A$.\n\n    Arguments:\n        state: State of the linear solver returned by [`init`][..init].\n        z: IID standard normal vector.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.unwhiten(state)","title":"<code>state</code>","text":"(<code>_LinearSolverState</code>)           \u2013            <p>State of the linear solver returned by <code>init</code>.</p>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.AbstractLinearSolver.unwhiten(z)","title":"<code>z</code>","text":"(<code>Float[Array, N]</code>)           \u2013            <p>IID standard normal vector.</p>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.Cholesky","title":"Cholesky","text":"<pre><code>Cholesky(jitter: ScalarFloat | None = None)\n</code></pre> <p>               Bases: <code>AbstractLinearSolver[CholeskyState]</code></p> <p>Solve a linear system using the Cholesky decomposition.</p> <p>Due to numerical imprecision, Cholesky factorization may fail even for positive-definite \\(A\\). Optionally, a small amount of <code>jitter</code> (\\(\\epsilon\\)) can be added to \\(A\\) to ensure positive-definiteness. Note that the resulting system solved is slightly different from the original system.</p> <p>Attributes:</p> <ul> <li> <code>jitter</code>               (<code>ScalarFloat | None</code>)           \u2013            <p>Small amount of jitter to add to \\(A\\) to ensure positive-definiteness.</p> </li> </ul> Source code in <code>src/cagpjax/solvers/cholesky.py</code> <pre><code>def __init__(self, jitter: ScalarFloat | None = None):\n    self.jitter = jitter\n</code></pre>"},{"location":"reference/cagpjax/solvers/#cagpjax.solvers.PseudoInverse","title":"PseudoInverse","text":"<pre><code>PseudoInverse(rtol: ScalarFloat | None = None, grad_rtol: float | None = None, alg: Algorithm = Eigh())\n</code></pre> <p>               Bases: <code>AbstractLinearSolver[PseudoInverseState]</code></p> <p>Solve a linear system using the Moore-Penrose pseudoinverse.</p> <p>This solver computes the least-squares solution \\(x = A^+ b\\) for any \\(A\\), where \\(A^+\\) is the Moore-Penrose pseudoinverse. This is equivalent to the exact solution for non-singular \\(A\\) but generalizes to singular \\(A\\) and improves stability for almost-singular \\(A\\); note, however, that if the rank of \\(A\\) is dependent on hyperparameters being optimized, because the pseudoinverse is discontinuous, the optimization problem may be ill-posed.</p> <p>Note that if \\(A\\) is (almost-)degenerate (some eigenvalues repeat), then the gradient of its solves in JAX may be non-computable or numerically unstable (see jax#669). For degenerate operators, it may be necessary to increase <code>grad_rtol</code> to improve stability of gradients. See <code>cagpjax.linalg.eigh</code> for more details.</p> <p>Attributes:</p> <ul> <li> <code>rtol</code>               (<code>ScalarFloat | None</code>)           \u2013            <p>Specifies the cutoff for small eigenvalues.   Eigenvalues smaller than <code>rtol * largest_nonzero_eigenvalue</code> are treated as zero.   The default is determined based on the floating point precision of the dtype   of the operator (see <code>jax.numpy.linalg.pinv</code>).</p> </li> <li> <code>grad_rtol</code>               (<code>float | None</code>)           \u2013            <p>Specifies the cutoff for similar eigenvalues, used to improve gradient computation for (almost-)degenerate matrices. If not provided, the default is 0.0. If None or negative, all eigenvalues are treated as distinct.</p> </li> <li> <code>alg</code>               (<code>Algorithm</code>)           \u2013            <p>Algorithm for eigenvalue decomposition passed to <code>cagpjax.linalg.eigh</code>.</p> </li> </ul> Source code in <code>src/cagpjax/solvers/pseudoinverse.py</code> <pre><code>def __init__(\n    self,\n    rtol: ScalarFloat | None = None,\n    grad_rtol: float | None = None,\n    alg: cola.linalg.Algorithm = Eigh(),\n):\n    self.rtol = rtol\n    self.grad_rtol = grad_rtol\n    self.alg = alg\n</code></pre>"},{"location":"reference/cagpjax/solvers/base/","title":"base","text":""},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base","title":"cagpjax.solvers.base","text":"<p>Base classes for linear solvers and methods.</p> <p>Classes:</p> <ul> <li> <code>AbstractLinearSolver</code>           \u2013            <p>Base class for linear solvers.</p> </li> </ul>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver","title":"AbstractLinearSolver","text":"<p>               Bases: <code>Module</code>, <code>Generic[_LinearSolverState]</code></p> <p>Base class for linear solvers.</p> <p>These solvers are used to exactly or approximately solve the linear system \\(Ax = b\\) for \\(x\\), where \\(A\\) is a positive (semi-)definite (PSD) linear operator.</p> <p>Methods:</p> <ul> <li> <code>init</code>             \u2013              <p>Construct a solver state.</p> </li> <li> <code>inv_congruence_transform</code>             \u2013              <p>Compute the inverse congruence transform \\(B^T x\\) for \\(x\\) in \\(Ax = B\\).</p> </li> <li> <code>inv_quad</code>             \u2013              <p>Compute the inverse quadratic form \\(b^T x\\), for \\(x\\) in \\(Ax = b\\).</p> </li> <li> <code>logdet</code>             \u2013              <p>Compute the logarithm of the (pseudo-)determinant of \\(A\\).</p> </li> <li> <code>solve</code>             \u2013              <p>Compute a solution to the linear system \\(Ax = b\\).</p> </li> <li> <code>trace_solve</code>             \u2013              <p>Compute \\(\\mathrm{trace}(X)\\) in \\(AX=B\\) for PSD \\(A\\) and \\(B\\).</p> </li> <li> <code>unwhiten</code>             \u2013              <p>Given an IID standard normal vector \\(z\\), return \\(x\\) with covariance \\(A\\).</p> </li> </ul>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.init","title":"init  <code>abstractmethod</code>","text":"<pre><code>init(A: LinearOperator) -&gt; _LinearSolverState\n</code></pre> <p>Construct a solver state.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>_LinearSolverState</code>           \u2013            <p>State of the linear solver, which stores any necessary intermediate values.</p> </li> </ul> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>@abstractmethod\ndef init(self, A: LinearOperator) -&gt; _LinearSolverState:\n    \"\"\"Construct a solver state.\n\n    Arguments:\n        A: Positive (semi-)definite linear operator.\n\n    Returns:\n        State of the linear solver, which stores any necessary intermediate values.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.init(A)","title":"<code>A</code>","text":"(<code>LinearOperator</code>)           \u2013            <p>Positive (semi-)definite linear operator.</p>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.inv_congruence_transform","title":"inv_congruence_transform  <code>abstractmethod</code>","text":"<pre><code>inv_congruence_transform(state: _LinearSolverState, B: LinearOperator | Float[Array, 'N K']) -&gt; LinearOperator | Float[Array, 'K K']\n</code></pre> <p>Compute the inverse congruence transform \\(B^T x\\) for \\(x\\) in \\(Ax = B\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LinearOperator | Float[Array, 'K K']</code>           \u2013            <p>Linear operator or array resulting from the congruence transform.</p> </li> </ul> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>@abstractmethod\ndef inv_congruence_transform(\n    self, state: _LinearSolverState, B: LinearOperator | Float[Array, \"N K\"]\n) -&gt; LinearOperator | Float[Array, \"K K\"]:\n    \"\"\"Compute the inverse congruence transform $B^T x$ for $x$ in $Ax = B$.\n\n    Arguments:\n        state: State of the linear solver returned by [`init`][..init].\n        B: Linear operator or array to be applied.\n\n    Returns:\n        Linear operator or array resulting from the congruence transform.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.inv_congruence_transform(state)","title":"<code>state</code>","text":"(<code>_LinearSolverState</code>)           \u2013            <p>State of the linear solver returned by <code>init</code>.</p>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.inv_congruence_transform(B)","title":"<code>B</code>","text":"(<code>LinearOperator | Float[Array, 'N K']</code>)           \u2013            <p>Linear operator or array to be applied.</p>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.inv_quad","title":"inv_quad","text":"<pre><code>inv_quad(state: _LinearSolverState, b: Float[Array, N]) -&gt; ScalarFloat\n</code></pre> <p>Compute the inverse quadratic form \\(b^T x\\), for \\(x\\) in \\(Ax = b\\).</p> <p>Parameters:</p> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>def inv_quad(\n    self, state: _LinearSolverState, b: Float[Array, \"N #1\"]\n) -&gt; ScalarFloat:\n    \"\"\"Compute the inverse quadratic form $b^T x$, for $x$ in $Ax = b$.\n\n    Arguments:\n        state: State of the linear solver returned by [`init`][..init].\n        b: Right-hand side of the linear system.\n    \"\"\"\n    return self.inv_congruence_transform(state, b[:, None]).squeeze()\n</code></pre>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.inv_quad(state)","title":"<code>state</code>","text":"(<code>_LinearSolverState</code>)           \u2013            <p>State of the linear solver returned by <code>init</code>.</p>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.inv_quad(b)","title":"<code>b</code>","text":"(<code>Float[Array, N]</code>)           \u2013            <p>Right-hand side of the linear system.</p>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.logdet","title":"logdet  <code>abstractmethod</code>","text":"<pre><code>logdet(state: _LinearSolverState) -&gt; ScalarFloat\n</code></pre> <p>Compute the logarithm of the (pseudo-)determinant of \\(A\\).</p> <p>Parameters:</p> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>@abstractmethod\ndef logdet(self, state: _LinearSolverState) -&gt; ScalarFloat:\n    \"\"\"Compute the logarithm of the (pseudo-)determinant of $A$.\n\n    Arguments:\n        state: State of the linear solver returned by [`init`][..init].\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.logdet(state)","title":"<code>state</code>","text":"(<code>_LinearSolverState</code>)           \u2013            <p>State of the linear solver returned by <code>init</code>.</p>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.solve","title":"solve  <code>abstractmethod</code>","text":"<pre><code>solve(state: _LinearSolverState, b: Float[Array, N]) -&gt; Float[Array, N]\n</code></pre> <p>Compute a solution to the linear system \\(Ax = b\\).</p> <p>Parameters:</p> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>@abstractmethod\ndef solve(\n    self, state: _LinearSolverState, b: Float[Array, \"N #K\"]\n) -&gt; Float[Array, \"N #K\"]:\n    \"\"\"Compute a solution to the linear system $Ax = b$.\n\n    Arguments:\n        state: State of the linear solver returned by [`init`][..init].\n        b: Right-hand side of the linear system.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.solve(state)","title":"<code>state</code>","text":"(<code>_LinearSolverState</code>)           \u2013            <p>State of the linear solver returned by <code>init</code>.</p>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.solve(b)","title":"<code>b</code>","text":"(<code>Float[Array, N]</code>)           \u2013            <p>Right-hand side of the linear system.</p>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.trace_solve","title":"trace_solve  <code>abstractmethod</code>","text":"<pre><code>trace_solve(state: _LinearSolverState, state_other: _LinearSolverState) -&gt; ScalarFloat\n</code></pre> <p>Compute \\(\\mathrm{trace}(X)\\) in \\(AX=B\\) for PSD \\(A\\) and \\(B\\).</p> <p>Parameters:</p> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>@abstractmethod\ndef trace_solve(\n    self, state: _LinearSolverState, state_other: _LinearSolverState\n) -&gt; ScalarFloat:\n    r\"\"\"Compute $\\mathrm{trace}(X)$ in $AX=B$ for PSD $A$ and $B$.\n\n    Arguments:\n        state: State of the linear solver obtained by applying [`init`][..init] to an operator\n            representing $A$\n        state_other: Another state obtained by applying `init` to an operator representing $B$.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.trace_solve(state)","title":"<code>state</code>","text":"(<code>_LinearSolverState</code>)           \u2013            <p>State of the linear solver obtained by applying <code>init</code> to an operator representing \\(A\\)</p>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.trace_solve(state_other)","title":"<code>state_other</code>","text":"(<code>_LinearSolverState</code>)           \u2013            <p>Another state obtained by applying <code>init</code> to an operator representing \\(B\\).</p>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.unwhiten","title":"unwhiten  <code>abstractmethod</code>","text":"<pre><code>unwhiten(state: _LinearSolverState, z: Float[Array, N]) -&gt; Float[Array, N]\n</code></pre> <p>Given an IID standard normal vector \\(z\\), return \\(x\\) with covariance \\(A\\).</p> <p>Parameters:</p> Source code in <code>src/cagpjax/solvers/base.py</code> <pre><code>@abstractmethod\ndef unwhiten(\n    self, state: _LinearSolverState, z: Float[Array, \"N #K\"]\n) -&gt; Float[Array, \"N #K\"]:\n    \"\"\"Given an IID standard normal vector $z$, return $x$ with covariance $A$.\n\n    Arguments:\n        state: State of the linear solver returned by [`init`][..init].\n        z: IID standard normal vector.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.unwhiten(state)","title":"<code>state</code>","text":"(<code>_LinearSolverState</code>)           \u2013            <p>State of the linear solver returned by <code>init</code>.</p>"},{"location":"reference/cagpjax/solvers/base/#cagpjax.solvers.base.AbstractLinearSolver.unwhiten(z)","title":"<code>z</code>","text":"(<code>Float[Array, N]</code>)           \u2013            <p>IID standard normal vector.</p>"},{"location":"reference/cagpjax/solvers/cholesky/","title":"cholesky","text":""},{"location":"reference/cagpjax/solvers/cholesky/#cagpjax.solvers.cholesky","title":"cagpjax.solvers.cholesky","text":"<p>Linear solvers based on Cholesky decomposition.</p> <p>Classes:</p> <ul> <li> <code>Cholesky</code>           \u2013            <p>Solve a linear system using the Cholesky decomposition.</p> </li> </ul>"},{"location":"reference/cagpjax/solvers/cholesky/#cagpjax.solvers.cholesky.Cholesky","title":"Cholesky","text":"<pre><code>Cholesky(jitter: ScalarFloat | None = None)\n</code></pre> <p>               Bases: <code>AbstractLinearSolver[CholeskyState]</code></p> <p>Solve a linear system using the Cholesky decomposition.</p> <p>Due to numerical imprecision, Cholesky factorization may fail even for positive-definite \\(A\\). Optionally, a small amount of <code>jitter</code> (\\(\\epsilon\\)) can be added to \\(A\\) to ensure positive-definiteness. Note that the resulting system solved is slightly different from the original system.</p> <p>Attributes:</p> <ul> <li> <code>jitter</code>               (<code>ScalarFloat | None</code>)           \u2013            <p>Small amount of jitter to add to \\(A\\) to ensure positive-definiteness.</p> </li> </ul> Source code in <code>src/cagpjax/solvers/cholesky.py</code> <pre><code>def __init__(self, jitter: ScalarFloat | None = None):\n    self.jitter = jitter\n</code></pre>"},{"location":"reference/cagpjax/solvers/pseudoinverse/","title":"pseudoinverse","text":""},{"location":"reference/cagpjax/solvers/pseudoinverse/#cagpjax.solvers.pseudoinverse","title":"cagpjax.solvers.pseudoinverse","text":"<p>Classes:</p> <ul> <li> <code>PseudoInverse</code>           \u2013            <p>Solve a linear system using the Moore-Penrose pseudoinverse.</p> </li> </ul>"},{"location":"reference/cagpjax/solvers/pseudoinverse/#cagpjax.solvers.pseudoinverse.PseudoInverse","title":"PseudoInverse","text":"<pre><code>PseudoInverse(rtol: ScalarFloat | None = None, grad_rtol: float | None = None, alg: Algorithm = Eigh())\n</code></pre> <p>               Bases: <code>AbstractLinearSolver[PseudoInverseState]</code></p> <p>Solve a linear system using the Moore-Penrose pseudoinverse.</p> <p>This solver computes the least-squares solution \\(x = A^+ b\\) for any \\(A\\), where \\(A^+\\) is the Moore-Penrose pseudoinverse. This is equivalent to the exact solution for non-singular \\(A\\) but generalizes to singular \\(A\\) and improves stability for almost-singular \\(A\\); note, however, that if the rank of \\(A\\) is dependent on hyperparameters being optimized, because the pseudoinverse is discontinuous, the optimization problem may be ill-posed.</p> <p>Note that if \\(A\\) is (almost-)degenerate (some eigenvalues repeat), then the gradient of its solves in JAX may be non-computable or numerically unstable (see jax#669). For degenerate operators, it may be necessary to increase <code>grad_rtol</code> to improve stability of gradients. See <code>cagpjax.linalg.eigh</code> for more details.</p> <p>Attributes:</p> <ul> <li> <code>rtol</code>               (<code>ScalarFloat | None</code>)           \u2013            <p>Specifies the cutoff for small eigenvalues.   Eigenvalues smaller than <code>rtol * largest_nonzero_eigenvalue</code> are treated as zero.   The default is determined based on the floating point precision of the dtype   of the operator (see <code>jax.numpy.linalg.pinv</code>).</p> </li> <li> <code>grad_rtol</code>               (<code>float | None</code>)           \u2013            <p>Specifies the cutoff for similar eigenvalues, used to improve gradient computation for (almost-)degenerate matrices. If not provided, the default is 0.0. If None or negative, all eigenvalues are treated as distinct.</p> </li> <li> <code>alg</code>               (<code>Algorithm</code>)           \u2013            <p>Algorithm for eigenvalue decomposition passed to <code>cagpjax.linalg.eigh</code>.</p> </li> </ul> Source code in <code>src/cagpjax/solvers/pseudoinverse.py</code> <pre><code>def __init__(\n    self,\n    rtol: ScalarFloat | None = None,\n    grad_rtol: float | None = None,\n    alg: cola.linalg.Algorithm = Eigh(),\n):\n    self.rtol = rtol\n    self.grad_rtol = grad_rtol\n    self.alg = alg\n</code></pre>"},{"location":"reference/cagpjax/typing/","title":"typing","text":""},{"location":"reference/cagpjax/typing/#cagpjax.typing","title":"cagpjax.typing","text":""}]}